{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n",
       "        21.8000, 48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: x: torch.Size([]), y: torch.Size([3, 1])\n",
      "        z: torch.Size([1, 3]), a: torch.Size([2, 1, 1])\n",
      "x * y: torch.Size([3, 1])\n",
      "y * z: torch.Size([3, 3])\n",
      "y * z * a: torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(())\n",
    "y = torch.ones(3,1)\n",
    "z = torch.ones(1,3)\n",
    "a = torch.ones(2, 1, 1)\n",
    "print(f\"shapes: x: {x.shape}, y: {y.shape}\")\n",
    "print(f\"        z: {z.shape}, a: {a.shape}\")\n",
    "print(\"x * y:\", (x * y).shape)\n",
    "print(\"y * z:\", (y * z).shape)\n",
    "print(\"y * z * a:\", (y * z * a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "loss_rate_of_change_w = \\\n",
    "    (loss_fn(model(t_u, w + delta, b), t_c) - \n",
    "     loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "w = w - learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "    (loss_fn(model(t_u, w, b + delta), t_c) - \n",
    "     loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "\n",
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2 * (t_p - t_c) / t_p.size(0)  # <1>\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])  # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        print('------')\n",
    "        print('Epoch %d, Loss %f, ' % (epoch, float(loss))) # <3>\n",
    "        print('Params: ', params)\n",
    "        print('Grad: ', grad)\n",
    "        print('------')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n",
    "                  print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            if print_params:\n",
    "                print('    Params:', params)\n",
    "                print('    Grad:  ', grad)\n",
    "        if epoch in {4, 12, 101}:\n",
    "            print('...')\n",
    "\n",
    "        if not torch.isfinite(loss).all():\n",
    "            break  # <3>\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Epoch 1, Loss 1763.884766, \n",
      "Params:  tensor([-44.1730,  -0.8260])\n",
      "Grad:  tensor([4517.2964,   82.6000])\n",
      "------\n",
      "------\n",
      "Epoch 2, Loss 5802484.500000, \n",
      "Params:  tensor([2568.4011,   45.1637])\n",
      "Grad:  tensor([-261257.4062,   -4598.9702])\n",
      "------\n",
      "------\n",
      "Epoch 3, Loss 19408029696.000000, \n",
      "Params:  tensor([-148527.7344,   -2616.3931])\n",
      "Grad:  tensor([15109614.0000,   266155.6875])\n",
      "------\n",
      "------\n",
      "Epoch 4, Loss 64915905708032.000000, \n",
      "Params:  tensor([8589999.0000,  151310.8906])\n",
      "Grad:  tensor([-8.7385e+08, -1.5393e+07])\n",
      "------\n",
      "------\n",
      "Epoch 5, Loss 217130525461053440.000000, \n",
      "Params:  tensor([-4.9680e+08, -8.7510e+06])\n",
      "Grad:  tensor([5.0539e+10, 8.9023e+08])\n",
      "------\n",
      "------\n",
      "Epoch 6, Loss 726257583152928129024.000000, \n",
      "Params:  tensor([2.8732e+10, 5.0610e+08])\n",
      "Grad:  tensor([-2.9229e+12, -5.1486e+10])\n",
      "------\n",
      "------\n",
      "Epoch 7, Loss 2429183416467662896627712.000000, \n",
      "Params:  tensor([-1.6617e+12, -2.9270e+10])\n",
      "Grad:  tensor([1.6904e+14, 2.9776e+12])\n",
      "------\n",
      "------\n",
      "Epoch 8, Loss 8125122549611731432050262016.000000, \n",
      "Params:  tensor([9.6102e+13, 1.6928e+12])\n",
      "Grad:  tensor([-9.7764e+15, -1.7221e+14])\n",
      "------\n",
      "------\n",
      "Epoch 9, Loss 27176882120842590626938030653440.000000, \n",
      "Params:  tensor([-5.5580e+15, -9.7903e+13])\n",
      "Grad:  tensor([5.6541e+17, 9.9596e+15])\n",
      "------\n",
      "------\n",
      "Epoch 10, Loss 90901105189019073810297959556841472.000000, \n",
      "Params:  tensor([3.2144e+17, 5.6621e+15])\n",
      "Grad:  tensor([-3.2700e+19, -5.7600e+17])\n",
      "------\n",
      "------\n",
      "Epoch 11, Loss inf, \n",
      "Params:  tensor([-1.8590e+19, -3.2746e+17])\n",
      "Grad:  tensor([1.8912e+21, 3.3313e+19])\n",
      "------\n",
      "------\n",
      "Epoch 12, Loss inf, \n",
      "Params:  tensor([1.0752e+21, 1.8939e+19])\n",
      "Grad:  tensor([-1.0937e+23, -1.9266e+21])\n",
      "------\n",
      "------\n",
      "Epoch 13, Loss inf, \n",
      "Params:  tensor([-6.2181e+22, -1.0953e+21])\n",
      "Grad:  tensor([6.3256e+24, 1.1142e+23])\n",
      "------\n",
      "------\n",
      "Epoch 14, Loss inf, \n",
      "Params:  tensor([3.5962e+24, 6.3346e+22])\n",
      "Grad:  tensor([-3.6584e+26, -6.4441e+24])\n",
      "------\n",
      "------\n",
      "Epoch 15, Loss inf, \n",
      "Params:  tensor([-2.0798e+26, -3.6636e+24])\n",
      "Grad:  tensor([2.1158e+28, 3.7269e+26])\n",
      "------\n",
      "------\n",
      "Epoch 16, Loss inf, \n",
      "Params:  tensor([1.2028e+28, 2.1188e+26])\n",
      "Grad:  tensor([-1.2236e+30, -2.1554e+28])\n",
      "------\n",
      "------\n",
      "Epoch 17, Loss inf, \n",
      "Params:  tensor([-6.9566e+29, -1.2254e+28])\n",
      "Grad:  tensor([7.0769e+31, 1.2466e+30])\n",
      "------\n",
      "------\n",
      "Epoch 18, Loss inf, \n",
      "Params:  tensor([4.0233e+31, 7.0869e+29])\n",
      "Grad:  tensor([-4.0929e+33, -7.2095e+31])\n",
      "------\n",
      "------\n",
      "Epoch 19, Loss inf, \n",
      "Params:  tensor([-2.3268e+33, -4.0987e+31])\n",
      "Grad:  tensor([2.3671e+35, 4.1695e+33])\n",
      "------\n",
      "------\n",
      "Epoch 20, Loss inf, \n",
      "Params:  tensor([1.3457e+35, 2.3704e+33])\n",
      "Grad:  tensor([-1.3690e+37, -2.4114e+35])\n",
      "------\n",
      "------\n",
      "Epoch 21, Loss inf, \n",
      "Params:  tensor([       -inf, -1.3709e+35])\n",
      "Grad:  tensor([       inf, 1.3946e+37])\n",
      "------\n",
      "------\n",
      "Epoch 22, Loss inf, \n",
      "Params:  tensor([nan, inf])\n",
      "Grad:  tensor([-inf, -inf])\n",
      "------\n",
      "------\n",
      "Epoch 23, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 24, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 25, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 26, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 27, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 28, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 29, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 30, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 31, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 32, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 33, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 34, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 35, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 36, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 37, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 38, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 39, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 40, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 41, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 42, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 43, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 44, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 45, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 46, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 47, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 48, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 49, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 50, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 51, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 52, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 53, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 54, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 55, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 56, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 57, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 58, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 59, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 60, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 61, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 62, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 63, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 64, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 65, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 66, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 67, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 68, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 69, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 70, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 71, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 72, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 73, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 74, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 75, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 76, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 77, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 78, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 79, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 80, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 81, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 82, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 83, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 84, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 85, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 86, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 87, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 88, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 89, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 90, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 91, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 92, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 93, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 94, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 95, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 96, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 97, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 98, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 99, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n",
      "------\n",
      "Epoch 100, Loss nan, \n",
      "Params:  tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_u, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Epoch 1, Loss 1763.884766, \n",
      "Params:  tensor([ 0.5483, -0.0083])\n",
      "Grad:  tensor([4517.2964,   82.6000])\n",
      "------\n",
      "------\n",
      "Epoch 2, Loss 323.090515, \n",
      "Params:  tensor([ 0.3623, -0.0118])\n",
      "Grad:  tensor([1859.5493,   35.7843])\n",
      "------\n",
      "------\n",
      "Epoch 3, Loss 78.929634, \n",
      "Params:  tensor([ 0.2858, -0.0135])\n",
      "Grad:  tensor([765.4666,  16.5122])\n",
      "------\n",
      "------\n",
      "Epoch 4, Loss 37.552845, \n",
      "Params:  tensor([ 0.2543, -0.0143])\n",
      "Grad:  tensor([315.0790,   8.5787])\n",
      "------\n",
      "------\n",
      "Epoch 5, Loss 30.540283, \n",
      "Params:  tensor([ 0.2413, -0.0149])\n",
      "Grad:  tensor([129.6733,   5.3127])\n",
      "------\n",
      "------\n",
      "Epoch 6, Loss 29.351154, \n",
      "Params:  tensor([ 0.2360, -0.0153])\n",
      "Grad:  tensor([53.3495,  3.9682])\n",
      "------\n",
      "------\n",
      "Epoch 7, Loss 29.148884, \n",
      "Params:  tensor([ 0.2338, -0.0156])\n",
      "Grad:  tensor([21.9304,  3.4148])\n",
      "------\n",
      "------\n",
      "Epoch 8, Loss 29.113848, \n",
      "Params:  tensor([ 0.2329, -0.0159])\n",
      "Grad:  tensor([8.9964, 3.1869])\n",
      "------\n",
      "------\n",
      "Epoch 9, Loss 29.107145, \n",
      "Params:  tensor([ 0.2325, -0.0162])\n",
      "Grad:  tensor([3.6721, 3.0930])\n",
      "------\n",
      "------\n",
      "Epoch 10, Loss 29.105247, \n",
      "Params:  tensor([ 0.2324, -0.0166])\n",
      "Grad:  tensor([1.4803, 3.0544])\n",
      "------\n",
      "------\n",
      "Epoch 11, Loss 29.104168, \n",
      "Params:  tensor([ 0.2323, -0.0169])\n",
      "Grad:  tensor([0.5781, 3.0384])\n",
      "------\n",
      "------\n",
      "Epoch 12, Loss 29.103222, \n",
      "Params:  tensor([ 0.2323, -0.0172])\n",
      "Grad:  tensor([0.2066, 3.0318])\n",
      "------\n",
      "------\n",
      "Epoch 13, Loss 29.102295, \n",
      "Params:  tensor([ 0.2323, -0.0175])\n",
      "Grad:  tensor([0.0537, 3.0291])\n",
      "------\n",
      "------\n",
      "Epoch 14, Loss 29.101379, \n",
      "Params:  tensor([ 0.2323, -0.0178])\n",
      "Grad:  tensor([-0.0093,  3.0279])\n",
      "------\n",
      "------\n",
      "Epoch 15, Loss 29.100466, \n",
      "Params:  tensor([ 0.2323, -0.0181])\n",
      "Grad:  tensor([-0.0353,  3.0274])\n",
      "------\n",
      "------\n",
      "Epoch 16, Loss 29.099548, \n",
      "Params:  tensor([ 0.2323, -0.0184])\n",
      "Grad:  tensor([-0.0459,  3.0272])\n",
      "------\n",
      "------\n",
      "Epoch 17, Loss 29.098631, \n",
      "Params:  tensor([ 0.2323, -0.0187])\n",
      "Grad:  tensor([-0.0502,  3.0270])\n",
      "------\n",
      "------\n",
      "Epoch 18, Loss 29.097717, \n",
      "Params:  tensor([ 0.2323, -0.0190])\n",
      "Grad:  tensor([-0.0520,  3.0270])\n",
      "------\n",
      "------\n",
      "Epoch 19, Loss 29.096796, \n",
      "Params:  tensor([ 0.2323, -0.0193])\n",
      "Grad:  tensor([-0.0528,  3.0269])\n",
      "------\n",
      "------\n",
      "Epoch 20, Loss 29.095881, \n",
      "Params:  tensor([ 0.2323, -0.0196])\n",
      "Grad:  tensor([-0.0531,  3.0268])\n",
      "------\n",
      "------\n",
      "Epoch 21, Loss 29.094959, \n",
      "Params:  tensor([ 0.2323, -0.0199])\n",
      "Grad:  tensor([-0.0533,  3.0268])\n",
      "------\n",
      "------\n",
      "Epoch 22, Loss 29.094049, \n",
      "Params:  tensor([ 0.2323, -0.0202])\n",
      "Grad:  tensor([-0.0533,  3.0267])\n",
      "------\n",
      "------\n",
      "Epoch 23, Loss 29.093134, \n",
      "Params:  tensor([ 0.2323, -0.0205])\n",
      "Grad:  tensor([-0.0533,  3.0267])\n",
      "------\n",
      "------\n",
      "Epoch 24, Loss 29.092216, \n",
      "Params:  tensor([ 0.2323, -0.0208])\n",
      "Grad:  tensor([-0.0533,  3.0266])\n",
      "------\n",
      "------\n",
      "Epoch 25, Loss 29.091301, \n",
      "Params:  tensor([ 0.2323, -0.0211])\n",
      "Grad:  tensor([-0.0533,  3.0266])\n",
      "------\n",
      "------\n",
      "Epoch 26, Loss 29.090385, \n",
      "Params:  tensor([ 0.2323, -0.0214])\n",
      "Grad:  tensor([-0.0533,  3.0265])\n",
      "------\n",
      "------\n",
      "Epoch 27, Loss 29.089464, \n",
      "Params:  tensor([ 0.2323, -0.0217])\n",
      "Grad:  tensor([-0.0533,  3.0265])\n",
      "------\n",
      "------\n",
      "Epoch 28, Loss 29.088551, \n",
      "Params:  tensor([ 0.2323, -0.0220])\n",
      "Grad:  tensor([-0.0532,  3.0264])\n",
      "------\n",
      "------\n",
      "Epoch 29, Loss 29.087635, \n",
      "Params:  tensor([ 0.2323, -0.0223])\n",
      "Grad:  tensor([-0.0533,  3.0264])\n",
      "------\n",
      "------\n",
      "Epoch 30, Loss 29.086714, \n",
      "Params:  tensor([ 0.2323, -0.0226])\n",
      "Grad:  tensor([-0.0533,  3.0263])\n",
      "------\n",
      "------\n",
      "Epoch 31, Loss 29.085804, \n",
      "Params:  tensor([ 0.2324, -0.0229])\n",
      "Grad:  tensor([-0.0532,  3.0262])\n",
      "------\n",
      "------\n",
      "Epoch 32, Loss 29.084888, \n",
      "Params:  tensor([ 0.2324, -0.0232])\n",
      "Grad:  tensor([-0.0533,  3.0262])\n",
      "------\n",
      "------\n",
      "Epoch 33, Loss 29.083967, \n",
      "Params:  tensor([ 0.2324, -0.0235])\n",
      "Grad:  tensor([-0.0533,  3.0261])\n",
      "------\n",
      "------\n",
      "Epoch 34, Loss 29.083057, \n",
      "Params:  tensor([ 0.2324, -0.0238])\n",
      "Grad:  tensor([-0.0533,  3.0261])\n",
      "------\n",
      "------\n",
      "Epoch 35, Loss 29.082142, \n",
      "Params:  tensor([ 0.2324, -0.0241])\n",
      "Grad:  tensor([-0.0532,  3.0260])\n",
      "------\n",
      "------\n",
      "Epoch 36, Loss 29.081221, \n",
      "Params:  tensor([ 0.2324, -0.0244])\n",
      "Grad:  tensor([-0.0533,  3.0260])\n",
      "------\n",
      "------\n",
      "Epoch 37, Loss 29.080309, \n",
      "Params:  tensor([ 0.2324, -0.0247])\n",
      "Grad:  tensor([-0.0533,  3.0259])\n",
      "------\n",
      "------\n",
      "Epoch 38, Loss 29.079390, \n",
      "Params:  tensor([ 0.2324, -0.0250])\n",
      "Grad:  tensor([-0.0532,  3.0259])\n",
      "------\n",
      "------\n",
      "Epoch 39, Loss 29.078474, \n",
      "Params:  tensor([ 0.2324, -0.0253])\n",
      "Grad:  tensor([-0.0533,  3.0258])\n",
      "------\n",
      "------\n",
      "Epoch 40, Loss 29.077562, \n",
      "Params:  tensor([ 0.2324, -0.0256])\n",
      "Grad:  tensor([-0.0533,  3.0258])\n",
      "------\n",
      "------\n",
      "Epoch 41, Loss 29.076649, \n",
      "Params:  tensor([ 0.2324, -0.0259])\n",
      "Grad:  tensor([-0.0533,  3.0257])\n",
      "------\n",
      "------\n",
      "Epoch 42, Loss 29.075731, \n",
      "Params:  tensor([ 0.2324, -0.0262])\n",
      "Grad:  tensor([-0.0532,  3.0257])\n",
      "------\n",
      "------\n",
      "Epoch 43, Loss 29.074812, \n",
      "Params:  tensor([ 0.2324, -0.0265])\n",
      "Grad:  tensor([-0.0533,  3.0256])\n",
      "------\n",
      "------\n",
      "Epoch 44, Loss 29.073895, \n",
      "Params:  tensor([ 0.2324, -0.0268])\n",
      "Grad:  tensor([-0.0533,  3.0256])\n",
      "------\n",
      "------\n",
      "Epoch 45, Loss 29.072981, \n",
      "Params:  tensor([ 0.2324, -0.0271])\n",
      "Grad:  tensor([-0.0533,  3.0255])\n",
      "------\n",
      "------\n",
      "Epoch 46, Loss 29.072069, \n",
      "Params:  tensor([ 0.2324, -0.0274])\n",
      "Grad:  tensor([-0.0533,  3.0254])\n",
      "------\n",
      "------\n",
      "Epoch 47, Loss 29.071148, \n",
      "Params:  tensor([ 0.2324, -0.0277])\n",
      "Grad:  tensor([-0.0533,  3.0254])\n",
      "------\n",
      "------\n",
      "Epoch 48, Loss 29.070234, \n",
      "Params:  tensor([ 0.2324, -0.0281])\n",
      "Grad:  tensor([-0.0533,  3.0253])\n",
      "------\n",
      "------\n",
      "Epoch 49, Loss 29.069323, \n",
      "Params:  tensor([ 0.2325, -0.0284])\n",
      "Grad:  tensor([-0.0533,  3.0253])\n",
      "------\n",
      "------\n",
      "Epoch 50, Loss 29.068401, \n",
      "Params:  tensor([ 0.2325, -0.0287])\n",
      "Grad:  tensor([-0.0532,  3.0252])\n",
      "------\n",
      "------\n",
      "Epoch 51, Loss 29.067486, \n",
      "Params:  tensor([ 0.2325, -0.0290])\n",
      "Grad:  tensor([-0.0533,  3.0252])\n",
      "------\n",
      "------\n",
      "Epoch 52, Loss 29.066566, \n",
      "Params:  tensor([ 0.2325, -0.0293])\n",
      "Grad:  tensor([-0.0533,  3.0251])\n",
      "------\n",
      "------\n",
      "Epoch 53, Loss 29.065657, \n",
      "Params:  tensor([ 0.2325, -0.0296])\n",
      "Grad:  tensor([-0.0533,  3.0251])\n",
      "------\n",
      "------\n",
      "Epoch 54, Loss 29.064741, \n",
      "Params:  tensor([ 0.2325, -0.0299])\n",
      "Grad:  tensor([-0.0533,  3.0250])\n",
      "------\n",
      "------\n",
      "Epoch 55, Loss 29.063826, \n",
      "Params:  tensor([ 0.2325, -0.0302])\n",
      "Grad:  tensor([-0.0532,  3.0250])\n",
      "------\n",
      "------\n",
      "Epoch 56, Loss 29.062910, \n",
      "Params:  tensor([ 0.2325, -0.0305])\n",
      "Grad:  tensor([-0.0533,  3.0249])\n",
      "------\n",
      "------\n",
      "Epoch 57, Loss 29.061995, \n",
      "Params:  tensor([ 0.2325, -0.0308])\n",
      "Grad:  tensor([-0.0532,  3.0249])\n",
      "------\n",
      "------\n",
      "Epoch 58, Loss 29.061079, \n",
      "Params:  tensor([ 0.2325, -0.0311])\n",
      "Grad:  tensor([-0.0533,  3.0248])\n",
      "------\n",
      "------\n",
      "Epoch 59, Loss 29.060169, \n",
      "Params:  tensor([ 0.2325, -0.0314])\n",
      "Grad:  tensor([-0.0533,  3.0248])\n",
      "------\n",
      "------\n",
      "Epoch 60, Loss 29.059248, \n",
      "Params:  tensor([ 0.2325, -0.0317])\n",
      "Grad:  tensor([-0.0533,  3.0247])\n",
      "------\n",
      "------\n",
      "Epoch 61, Loss 29.058336, \n",
      "Params:  tensor([ 0.2325, -0.0320])\n",
      "Grad:  tensor([-0.0533,  3.0247])\n",
      "------\n",
      "------\n",
      "Epoch 62, Loss 29.057415, \n",
      "Params:  tensor([ 0.2325, -0.0323])\n",
      "Grad:  tensor([-0.0534,  3.0246])\n",
      "------\n",
      "------\n",
      "Epoch 63, Loss 29.056507, \n",
      "Params:  tensor([ 0.2325, -0.0326])\n",
      "Grad:  tensor([-0.0533,  3.0245])\n",
      "------\n",
      "------\n",
      "Epoch 64, Loss 29.055586, \n",
      "Params:  tensor([ 0.2325, -0.0329])\n",
      "Grad:  tensor([-0.0532,  3.0245])\n",
      "------\n",
      "------\n",
      "Epoch 65, Loss 29.054674, \n",
      "Params:  tensor([ 0.2325, -0.0332])\n",
      "Grad:  tensor([-0.0533,  3.0244])\n",
      "------\n",
      "------\n",
      "Epoch 66, Loss 29.053761, \n",
      "Params:  tensor([ 0.2325, -0.0335])\n",
      "Grad:  tensor([-0.0533,  3.0244])\n",
      "------\n",
      "------\n",
      "Epoch 67, Loss 29.052843, \n",
      "Params:  tensor([ 0.2325, -0.0338])\n",
      "Grad:  tensor([-0.0533,  3.0243])\n",
      "------\n",
      "------\n",
      "Epoch 68, Loss 29.051929, \n",
      "Params:  tensor([ 0.2326, -0.0341])\n",
      "Grad:  tensor([-0.0532,  3.0243])\n",
      "------\n",
      "------\n",
      "Epoch 69, Loss 29.051012, \n",
      "Params:  tensor([ 0.2326, -0.0344])\n",
      "Grad:  tensor([-0.0533,  3.0242])\n",
      "------\n",
      "------\n",
      "Epoch 70, Loss 29.050098, \n",
      "Params:  tensor([ 0.2326, -0.0347])\n",
      "Grad:  tensor([-0.0532,  3.0242])\n",
      "------\n",
      "------\n",
      "Epoch 71, Loss 29.049183, \n",
      "Params:  tensor([ 0.2326, -0.0350])\n",
      "Grad:  tensor([-0.0533,  3.0241])\n",
      "------\n",
      "------\n",
      "Epoch 72, Loss 29.048273, \n",
      "Params:  tensor([ 0.2326, -0.0353])\n",
      "Grad:  tensor([-0.0533,  3.0241])\n",
      "------\n",
      "------\n",
      "Epoch 73, Loss 29.047350, \n",
      "Params:  tensor([ 0.2326, -0.0356])\n",
      "Grad:  tensor([-0.0532,  3.0240])\n",
      "------\n",
      "------\n",
      "Epoch 74, Loss 29.046442, \n",
      "Params:  tensor([ 0.2326, -0.0359])\n",
      "Grad:  tensor([-0.0533,  3.0240])\n",
      "------\n",
      "------\n",
      "Epoch 75, Loss 29.045530, \n",
      "Params:  tensor([ 0.2326, -0.0362])\n",
      "Grad:  tensor([-0.0532,  3.0239])\n",
      "------\n",
      "------\n",
      "Epoch 76, Loss 29.044611, \n",
      "Params:  tensor([ 0.2326, -0.0365])\n",
      "Grad:  tensor([-0.0533,  3.0239])\n",
      "------\n",
      "------\n",
      "Epoch 77, Loss 29.043699, \n",
      "Params:  tensor([ 0.2326, -0.0368])\n",
      "Grad:  tensor([-0.0533,  3.0238])\n",
      "------\n",
      "------\n",
      "Epoch 78, Loss 29.042784, \n",
      "Params:  tensor([ 0.2326, -0.0371])\n",
      "Grad:  tensor([-0.0533,  3.0238])\n",
      "------\n",
      "------\n",
      "Epoch 79, Loss 29.041870, \n",
      "Params:  tensor([ 0.2326, -0.0374])\n",
      "Grad:  tensor([-0.0533,  3.0237])\n",
      "------\n",
      "------\n",
      "Epoch 80, Loss 29.040955, \n",
      "Params:  tensor([ 0.2326, -0.0377])\n",
      "Grad:  tensor([-0.0532,  3.0236])\n",
      "------\n",
      "------\n",
      "Epoch 81, Loss 29.040039, \n",
      "Params:  tensor([ 0.2326, -0.0380])\n",
      "Grad:  tensor([-0.0534,  3.0236])\n",
      "------\n",
      "------\n",
      "Epoch 82, Loss 29.039122, \n",
      "Params:  tensor([ 0.2326, -0.0383])\n",
      "Grad:  tensor([-0.0533,  3.0235])\n",
      "------\n",
      "------\n",
      "Epoch 83, Loss 29.038210, \n",
      "Params:  tensor([ 0.2326, -0.0386])\n",
      "Grad:  tensor([-0.0532,  3.0235])\n",
      "------\n",
      "------\n",
      "Epoch 84, Loss 29.037294, \n",
      "Params:  tensor([ 0.2326, -0.0389])\n",
      "Grad:  tensor([-0.0533,  3.0234])\n",
      "------\n",
      "------\n",
      "Epoch 85, Loss 29.036379, \n",
      "Params:  tensor([ 0.2326, -0.0392])\n",
      "Grad:  tensor([-0.0533,  3.0234])\n",
      "------\n",
      "------\n",
      "Epoch 86, Loss 29.035463, \n",
      "Params:  tensor([ 0.2326, -0.0395])\n",
      "Grad:  tensor([-0.0532,  3.0233])\n",
      "------\n",
      "------\n",
      "Epoch 87, Loss 29.034554, \n",
      "Params:  tensor([ 0.2327, -0.0398])\n",
      "Grad:  tensor([-0.0533,  3.0233])\n",
      "------\n",
      "------\n",
      "Epoch 88, Loss 29.033636, \n",
      "Params:  tensor([ 0.2327, -0.0401])\n",
      "Grad:  tensor([-0.0532,  3.0232])\n",
      "------\n",
      "------\n",
      "Epoch 89, Loss 29.032722, \n",
      "Params:  tensor([ 0.2327, -0.0405])\n",
      "Grad:  tensor([-0.0533,  3.0232])\n",
      "------\n",
      "------\n",
      "Epoch 90, Loss 29.031811, \n",
      "Params:  tensor([ 0.2327, -0.0408])\n",
      "Grad:  tensor([-0.0533,  3.0231])\n",
      "------\n",
      "------\n",
      "Epoch 91, Loss 29.030895, \n",
      "Params:  tensor([ 0.2327, -0.0411])\n",
      "Grad:  tensor([-0.0532,  3.0231])\n",
      "------\n",
      "------\n",
      "Epoch 92, Loss 29.029976, \n",
      "Params:  tensor([ 0.2327, -0.0414])\n",
      "Grad:  tensor([-0.0532,  3.0230])\n",
      "------\n",
      "------\n",
      "Epoch 93, Loss 29.029066, \n",
      "Params:  tensor([ 0.2327, -0.0417])\n",
      "Grad:  tensor([-0.0533,  3.0230])\n",
      "------\n",
      "------\n",
      "Epoch 94, Loss 29.028151, \n",
      "Params:  tensor([ 0.2327, -0.0420])\n",
      "Grad:  tensor([-0.0532,  3.0229])\n",
      "------\n",
      "------\n",
      "Epoch 95, Loss 29.027235, \n",
      "Params:  tensor([ 0.2327, -0.0423])\n",
      "Grad:  tensor([-0.0533,  3.0229])\n",
      "------\n",
      "------\n",
      "Epoch 96, Loss 29.026323, \n",
      "Params:  tensor([ 0.2327, -0.0426])\n",
      "Grad:  tensor([-0.0533,  3.0228])\n",
      "------\n",
      "------\n",
      "Epoch 97, Loss 29.025410, \n",
      "Params:  tensor([ 0.2327, -0.0429])\n",
      "Grad:  tensor([-0.0532,  3.0227])\n",
      "------\n",
      "------\n",
      "Epoch 98, Loss 29.024492, \n",
      "Params:  tensor([ 0.2327, -0.0432])\n",
      "Grad:  tensor([-0.0532,  3.0227])\n",
      "------\n",
      "------\n",
      "Epoch 99, Loss 29.023582, \n",
      "Params:  tensor([ 0.2327, -0.0435])\n",
      "Grad:  tensor([-0.0533,  3.0226])\n",
      "------\n",
      "------\n",
      "Epoch 100, Loss 29.022667, \n",
      "Params:  tensor([ 0.2327, -0.0438])\n",
      "Grad:  tensor([-0.0532,  3.0226])\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-4, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_u, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5700, 5.5900, 5.8200, 8.1900, 5.6300, 4.8900, 3.3900, 2.1800,\n",
       "        4.8400, 6.0400, 6.8400])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_un = 0.1 * t_u\n",
    "t_un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Epoch 1, Loss 80.364342, \n",
      "Params:  tensor([1.7761, 0.1064])\n",
      "Grad:  tensor([-77.6140, -10.6400])\n",
      "------\n",
      "------\n",
      "Epoch 2, Loss 37.574913, \n",
      "Params:  tensor([2.0848, 0.1303])\n",
      "Grad:  tensor([-30.8623,  -2.3864])\n",
      "------\n",
      "------\n",
      "Epoch 3, Loss 30.871077, \n",
      "Params:  tensor([2.2094, 0.1217])\n",
      "Grad:  tensor([-12.4631,   0.8587])\n",
      "------\n",
      "------\n",
      "Epoch 4, Loss 29.756193, \n",
      "Params:  tensor([2.2616, 0.1004])\n",
      "Grad:  tensor([-5.2218,  2.1327])\n",
      "------\n",
      "------\n",
      "Epoch 5, Loss 29.507153, \n",
      "Params:  tensor([2.2853, 0.0740])\n",
      "Grad:  tensor([-2.3715,  2.6310])\n",
      "------\n",
      "------\n",
      "Epoch 6, Loss 29.392456, \n",
      "Params:  tensor([2.2978, 0.0458])\n",
      "Grad:  tensor([-1.2492,  2.8241])\n",
      "------\n",
      "------\n",
      "Epoch 7, Loss 29.298828, \n",
      "Params:  tensor([2.3059, 0.0168])\n",
      "Grad:  tensor([-0.8071,  2.8970])\n",
      "------\n",
      "------\n",
      "Epoch 8, Loss 29.208717, \n",
      "Params:  tensor([ 2.3122, -0.0124])\n",
      "Grad:  tensor([-0.6325,  2.9227])\n",
      "------\n",
      "------\n",
      "Epoch 9, Loss 29.119415, \n",
      "Params:  tensor([ 2.3178, -0.0417])\n",
      "Grad:  tensor([-0.5633,  2.9298])\n",
      "------\n",
      "------\n",
      "Epoch 10, Loss 29.030489, \n",
      "Params:  tensor([ 2.3232, -0.0710])\n",
      "Grad:  tensor([-0.5355,  2.9295])\n",
      "------\n",
      "------\n",
      "Epoch 11, Loss 28.941877, \n",
      "Params:  tensor([ 2.3284, -0.1003])\n",
      "Grad:  tensor([-0.5240,  2.9264])\n",
      "------\n",
      "------\n",
      "Epoch 12, Loss 28.853565, \n",
      "Params:  tensor([ 2.3336, -0.1295])\n",
      "Grad:  tensor([-0.5190,  2.9222])\n",
      "------\n",
      "------\n",
      "Epoch 13, Loss 28.765553, \n",
      "Params:  tensor([ 2.3388, -0.1587])\n",
      "Grad:  tensor([-0.5165,  2.9175])\n",
      "------\n",
      "------\n",
      "Epoch 14, Loss 28.677851, \n",
      "Params:  tensor([ 2.3439, -0.1878])\n",
      "Grad:  tensor([-0.5150,  2.9126])\n",
      "------\n",
      "------\n",
      "Epoch 15, Loss 28.590431, \n",
      "Params:  tensor([ 2.3491, -0.2169])\n",
      "Grad:  tensor([-0.5138,  2.9077])\n",
      "------\n",
      "------\n",
      "Epoch 16, Loss 28.503319, \n",
      "Params:  tensor([ 2.3542, -0.2459])\n",
      "Grad:  tensor([-0.5129,  2.9028])\n",
      "------\n",
      "------\n",
      "Epoch 17, Loss 28.416498, \n",
      "Params:  tensor([ 2.3593, -0.2749])\n",
      "Grad:  tensor([-0.5120,  2.8979])\n",
      "------\n",
      "------\n",
      "Epoch 18, Loss 28.329973, \n",
      "Params:  tensor([ 2.3644, -0.3038])\n",
      "Grad:  tensor([-0.5111,  2.8930])\n",
      "------\n",
      "------\n",
      "Epoch 19, Loss 28.243742, \n",
      "Params:  tensor([ 2.3695, -0.3327])\n",
      "Grad:  tensor([-0.5102,  2.8881])\n",
      "------\n",
      "------\n",
      "Epoch 20, Loss 28.157804, \n",
      "Params:  tensor([ 2.3746, -0.3615])\n",
      "Grad:  tensor([-0.5093,  2.8832])\n",
      "------\n",
      "------\n",
      "Epoch 21, Loss 28.072151, \n",
      "Params:  tensor([ 2.3797, -0.3903])\n",
      "Grad:  tensor([-0.5084,  2.8783])\n",
      "------\n",
      "------\n",
      "Epoch 22, Loss 27.986797, \n",
      "Params:  tensor([ 2.3848, -0.4190])\n",
      "Grad:  tensor([-0.5076,  2.8734])\n",
      "------\n",
      "------\n",
      "Epoch 23, Loss 27.901728, \n",
      "Params:  tensor([ 2.3899, -0.4477])\n",
      "Grad:  tensor([-0.5067,  2.8685])\n",
      "------\n",
      "------\n",
      "Epoch 24, Loss 27.816950, \n",
      "Params:  tensor([ 2.3949, -0.4763])\n",
      "Grad:  tensor([-0.5059,  2.8636])\n",
      "------\n",
      "------\n",
      "Epoch 25, Loss 27.732464, \n",
      "Params:  tensor([ 2.4000, -0.5049])\n",
      "Grad:  tensor([-0.5050,  2.8588])\n",
      "------\n",
      "------\n",
      "Epoch 26, Loss 27.648256, \n",
      "Params:  tensor([ 2.4050, -0.5335])\n",
      "Grad:  tensor([-0.5042,  2.8539])\n",
      "------\n",
      "------\n",
      "Epoch 27, Loss 27.564344, \n",
      "Params:  tensor([ 2.4101, -0.5620])\n",
      "Grad:  tensor([-0.5033,  2.8490])\n",
      "------\n",
      "------\n",
      "Epoch 28, Loss 27.480707, \n",
      "Params:  tensor([ 2.4151, -0.5904])\n",
      "Grad:  tensor([-0.5024,  2.8442])\n",
      "------\n",
      "------\n",
      "Epoch 29, Loss 27.397362, \n",
      "Params:  tensor([ 2.4201, -0.6188])\n",
      "Grad:  tensor([-0.5016,  2.8394])\n",
      "------\n",
      "------\n",
      "Epoch 30, Loss 27.314295, \n",
      "Params:  tensor([ 2.4251, -0.6471])\n",
      "Grad:  tensor([-0.5007,  2.8346])\n",
      "------\n",
      "------\n",
      "Epoch 31, Loss 27.231512, \n",
      "Params:  tensor([ 2.4301, -0.6754])\n",
      "Grad:  tensor([-0.4999,  2.8297])\n",
      "------\n",
      "------\n",
      "Epoch 32, Loss 27.149010, \n",
      "Params:  tensor([ 2.4351, -0.7037])\n",
      "Grad:  tensor([-0.4990,  2.8249])\n",
      "------\n",
      "------\n",
      "Epoch 33, Loss 27.066790, \n",
      "Params:  tensor([ 2.4401, -0.7319])\n",
      "Grad:  tensor([-0.4982,  2.8201])\n",
      "------\n",
      "------\n",
      "Epoch 34, Loss 26.984844, \n",
      "Params:  tensor([ 2.4450, -0.7600])\n",
      "Grad:  tensor([-0.4973,  2.8153])\n",
      "------\n",
      "------\n",
      "Epoch 35, Loss 26.903175, \n",
      "Params:  tensor([ 2.4500, -0.7881])\n",
      "Grad:  tensor([-0.4965,  2.8106])\n",
      "------\n",
      "------\n",
      "Epoch 36, Loss 26.821791, \n",
      "Params:  tensor([ 2.4550, -0.8162])\n",
      "Grad:  tensor([-0.4957,  2.8058])\n",
      "------\n",
      "------\n",
      "Epoch 37, Loss 26.740679, \n",
      "Params:  tensor([ 2.4599, -0.8442])\n",
      "Grad:  tensor([-0.4948,  2.8010])\n",
      "------\n",
      "------\n",
      "Epoch 38, Loss 26.659838, \n",
      "Params:  tensor([ 2.4649, -0.8722])\n",
      "Grad:  tensor([-0.4940,  2.7963])\n",
      "------\n",
      "------\n",
      "Epoch 39, Loss 26.579279, \n",
      "Params:  tensor([ 2.4698, -0.9001])\n",
      "Grad:  tensor([-0.4931,  2.7915])\n",
      "------\n",
      "------\n",
      "Epoch 40, Loss 26.498987, \n",
      "Params:  tensor([ 2.4747, -0.9280])\n",
      "Grad:  tensor([-0.4923,  2.7868])\n",
      "------\n",
      "------\n",
      "Epoch 41, Loss 26.418974, \n",
      "Params:  tensor([ 2.4796, -0.9558])\n",
      "Grad:  tensor([-0.4915,  2.7820])\n",
      "------\n",
      "------\n",
      "Epoch 42, Loss 26.339228, \n",
      "Params:  tensor([ 2.4845, -0.9836])\n",
      "Grad:  tensor([-0.4906,  2.7773])\n",
      "------\n",
      "------\n",
      "Epoch 43, Loss 26.259754, \n",
      "Params:  tensor([ 2.4894, -1.0113])\n",
      "Grad:  tensor([-0.4898,  2.7726])\n",
      "------\n",
      "------\n",
      "Epoch 44, Loss 26.180548, \n",
      "Params:  tensor([ 2.4943, -1.0390])\n",
      "Grad:  tensor([-0.4890,  2.7679])\n",
      "------\n",
      "------\n",
      "Epoch 45, Loss 26.101616, \n",
      "Params:  tensor([ 2.4992, -1.0666])\n",
      "Grad:  tensor([-0.4881,  2.7632])\n",
      "------\n",
      "------\n",
      "Epoch 46, Loss 26.022947, \n",
      "Params:  tensor([ 2.5041, -1.0942])\n",
      "Grad:  tensor([-0.4873,  2.7585])\n",
      "------\n",
      "------\n",
      "Epoch 47, Loss 25.944544, \n",
      "Params:  tensor([ 2.5089, -1.1217])\n",
      "Grad:  tensor([-0.4865,  2.7538])\n",
      "------\n",
      "------\n",
      "Epoch 48, Loss 25.866417, \n",
      "Params:  tensor([ 2.5138, -1.1492])\n",
      "Grad:  tensor([-0.4856,  2.7491])\n",
      "------\n",
      "------\n",
      "Epoch 49, Loss 25.788549, \n",
      "Params:  tensor([ 2.5186, -1.1766])\n",
      "Grad:  tensor([-0.4848,  2.7444])\n",
      "------\n",
      "------\n",
      "Epoch 50, Loss 25.710938, \n",
      "Params:  tensor([ 2.5235, -1.2040])\n",
      "Grad:  tensor([-0.4840,  2.7398])\n",
      "------\n",
      "------\n",
      "Epoch 51, Loss 25.633600, \n",
      "Params:  tensor([ 2.5283, -1.2314])\n",
      "Grad:  tensor([-0.4832,  2.7351])\n",
      "------\n",
      "------\n",
      "Epoch 52, Loss 25.556524, \n",
      "Params:  tensor([ 2.5331, -1.2587])\n",
      "Grad:  tensor([-0.4823,  2.7305])\n",
      "------\n",
      "------\n",
      "Epoch 53, Loss 25.479700, \n",
      "Params:  tensor([ 2.5379, -1.2860])\n",
      "Grad:  tensor([-0.4815,  2.7258])\n",
      "------\n",
      "------\n",
      "Epoch 54, Loss 25.403149, \n",
      "Params:  tensor([ 2.5428, -1.3132])\n",
      "Grad:  tensor([-0.4807,  2.7212])\n",
      "------\n",
      "------\n",
      "Epoch 55, Loss 25.326851, \n",
      "Params:  tensor([ 2.5476, -1.3403])\n",
      "Grad:  tensor([-0.4799,  2.7166])\n",
      "------\n",
      "------\n",
      "Epoch 56, Loss 25.250811, \n",
      "Params:  tensor([ 2.5523, -1.3675])\n",
      "Grad:  tensor([-0.4791,  2.7120])\n",
      "------\n",
      "------\n",
      "Epoch 57, Loss 25.175035, \n",
      "Params:  tensor([ 2.5571, -1.3945])\n",
      "Grad:  tensor([-0.4783,  2.7074])\n",
      "------\n",
      "------\n",
      "Epoch 58, Loss 25.099512, \n",
      "Params:  tensor([ 2.5619, -1.4216])\n",
      "Grad:  tensor([-0.4775,  2.7028])\n",
      "------\n",
      "------\n",
      "Epoch 59, Loss 25.024248, \n",
      "Params:  tensor([ 2.5667, -1.4485])\n",
      "Grad:  tensor([-0.4766,  2.6982])\n",
      "------\n",
      "------\n",
      "Epoch 60, Loss 24.949236, \n",
      "Params:  tensor([ 2.5714, -1.4755])\n",
      "Grad:  tensor([-0.4758,  2.6936])\n",
      "------\n",
      "------\n",
      "Epoch 61, Loss 24.874483, \n",
      "Params:  tensor([ 2.5762, -1.5024])\n",
      "Grad:  tensor([-0.4750,  2.6890])\n",
      "------\n",
      "------\n",
      "Epoch 62, Loss 24.799976, \n",
      "Params:  tensor([ 2.5809, -1.5292])\n",
      "Grad:  tensor([-0.4742,  2.6845])\n",
      "------\n",
      "------\n",
      "Epoch 63, Loss 24.725737, \n",
      "Params:  tensor([ 2.5857, -1.5560])\n",
      "Grad:  tensor([-0.4734,  2.6799])\n",
      "------\n",
      "------\n",
      "Epoch 64, Loss 24.651739, \n",
      "Params:  tensor([ 2.5904, -1.5828])\n",
      "Grad:  tensor([-0.4726,  2.6753])\n",
      "------\n",
      "------\n",
      "Epoch 65, Loss 24.577986, \n",
      "Params:  tensor([ 2.5951, -1.6095])\n",
      "Grad:  tensor([-0.4718,  2.6708])\n",
      "------\n",
      "------\n",
      "Epoch 66, Loss 24.504494, \n",
      "Params:  tensor([ 2.5998, -1.6361])\n",
      "Grad:  tensor([-0.4710,  2.6663])\n",
      "------\n",
      "------\n",
      "Epoch 67, Loss 24.431252, \n",
      "Params:  tensor([ 2.6045, -1.6628])\n",
      "Grad:  tensor([-0.4702,  2.6617])\n",
      "------\n",
      "------\n",
      "Epoch 68, Loss 24.358257, \n",
      "Params:  tensor([ 2.6092, -1.6893])\n",
      "Grad:  tensor([-0.4694,  2.6572])\n",
      "------\n",
      "------\n",
      "Epoch 69, Loss 24.285505, \n",
      "Params:  tensor([ 2.6139, -1.7159])\n",
      "Grad:  tensor([-0.4686,  2.6527])\n",
      "------\n",
      "------\n",
      "Epoch 70, Loss 24.212999, \n",
      "Params:  tensor([ 2.6186, -1.7423])\n",
      "Grad:  tensor([-0.4678,  2.6482])\n",
      "------\n",
      "------\n",
      "Epoch 71, Loss 24.140741, \n",
      "Params:  tensor([ 2.6232, -1.7688])\n",
      "Grad:  tensor([-0.4670,  2.6437])\n",
      "------\n",
      "------\n",
      "Epoch 72, Loss 24.068733, \n",
      "Params:  tensor([ 2.6279, -1.7952])\n",
      "Grad:  tensor([-0.4662,  2.6392])\n",
      "------\n",
      "------\n",
      "Epoch 73, Loss 23.996971, \n",
      "Params:  tensor([ 2.6326, -1.8215])\n",
      "Grad:  tensor([-0.4654,  2.6347])\n",
      "------\n",
      "------\n",
      "Epoch 74, Loss 23.925446, \n",
      "Params:  tensor([ 2.6372, -1.8478])\n",
      "Grad:  tensor([-0.4646,  2.6302])\n",
      "------\n",
      "------\n",
      "Epoch 75, Loss 23.854168, \n",
      "Params:  tensor([ 2.6418, -1.8741])\n",
      "Grad:  tensor([-0.4638,  2.6258])\n",
      "------\n",
      "------\n",
      "Epoch 76, Loss 23.783125, \n",
      "Params:  tensor([ 2.6465, -1.9003])\n",
      "Grad:  tensor([-0.4631,  2.6213])\n",
      "------\n",
      "------\n",
      "Epoch 77, Loss 23.712328, \n",
      "Params:  tensor([ 2.6511, -1.9265])\n",
      "Grad:  tensor([-0.4623,  2.6169])\n",
      "------\n",
      "------\n",
      "Epoch 78, Loss 23.641773, \n",
      "Params:  tensor([ 2.6557, -1.9526])\n",
      "Grad:  tensor([-0.4615,  2.6124])\n",
      "------\n",
      "------\n",
      "Epoch 79, Loss 23.571455, \n",
      "Params:  tensor([ 2.6603, -1.9787])\n",
      "Grad:  tensor([-0.4607,  2.6080])\n",
      "------\n",
      "------\n",
      "Epoch 80, Loss 23.501379, \n",
      "Params:  tensor([ 2.6649, -2.0047])\n",
      "Grad:  tensor([-0.4599,  2.6035])\n",
      "------\n",
      "------\n",
      "Epoch 81, Loss 23.431538, \n",
      "Params:  tensor([ 2.6695, -2.0307])\n",
      "Grad:  tensor([-0.4591,  2.5991])\n",
      "------\n",
      "------\n",
      "Epoch 82, Loss 23.361937, \n",
      "Params:  tensor([ 2.6741, -2.0566])\n",
      "Grad:  tensor([-0.4584,  2.5947])\n",
      "------\n",
      "------\n",
      "Epoch 83, Loss 23.292570, \n",
      "Params:  tensor([ 2.6787, -2.0825])\n",
      "Grad:  tensor([-0.4576,  2.5903])\n",
      "------\n",
      "------\n",
      "Epoch 84, Loss 23.223436, \n",
      "Params:  tensor([ 2.6832, -2.1084])\n",
      "Grad:  tensor([-0.4568,  2.5859])\n",
      "------\n",
      "------\n",
      "Epoch 85, Loss 23.154541, \n",
      "Params:  tensor([ 2.6878, -2.1342])\n",
      "Grad:  tensor([-0.4560,  2.5815])\n",
      "------\n",
      "------\n",
      "Epoch 86, Loss 23.085882, \n",
      "Params:  tensor([ 2.6923, -2.1600])\n",
      "Grad:  tensor([-0.4553,  2.5771])\n",
      "------\n",
      "------\n",
      "Epoch 87, Loss 23.017447, \n",
      "Params:  tensor([ 2.6969, -2.1857])\n",
      "Grad:  tensor([-0.4545,  2.5727])\n",
      "------\n",
      "------\n",
      "Epoch 88, Loss 22.949251, \n",
      "Params:  tensor([ 2.7014, -2.2114])\n",
      "Grad:  tensor([-0.4537,  2.5684])\n",
      "------\n",
      "------\n",
      "Epoch 89, Loss 22.881283, \n",
      "Params:  tensor([ 2.7060, -2.2370])\n",
      "Grad:  tensor([-0.4529,  2.5640])\n",
      "------\n",
      "------\n",
      "Epoch 90, Loss 22.813549, \n",
      "Params:  tensor([ 2.7105, -2.2626])\n",
      "Grad:  tensor([-0.4522,  2.5597])\n",
      "------\n",
      "------\n",
      "Epoch 91, Loss 22.746044, \n",
      "Params:  tensor([ 2.7150, -2.2882])\n",
      "Grad:  tensor([-0.4514,  2.5553])\n",
      "------\n",
      "------\n",
      "Epoch 92, Loss 22.678766, \n",
      "Params:  tensor([ 2.7195, -2.3137])\n",
      "Grad:  tensor([-0.4506,  2.5510])\n",
      "------\n",
      "------\n",
      "Epoch 93, Loss 22.611717, \n",
      "Params:  tensor([ 2.7240, -2.3392])\n",
      "Grad:  tensor([-0.4499,  2.5466])\n",
      "------\n",
      "------\n",
      "Epoch 94, Loss 22.544899, \n",
      "Params:  tensor([ 2.7285, -2.3646])\n",
      "Grad:  tensor([-0.4491,  2.5423])\n",
      "------\n",
      "------\n",
      "Epoch 95, Loss 22.478306, \n",
      "Params:  tensor([ 2.7330, -2.3900])\n",
      "Grad:  tensor([-0.4483,  2.5380])\n",
      "------\n",
      "------\n",
      "Epoch 96, Loss 22.411934, \n",
      "Params:  tensor([ 2.7374, -2.4153])\n",
      "Grad:  tensor([-0.4476,  2.5337])\n",
      "------\n",
      "------\n",
      "Epoch 97, Loss 22.345793, \n",
      "Params:  tensor([ 2.7419, -2.4406])\n",
      "Grad:  tensor([-0.4468,  2.5294])\n",
      "------\n",
      "------\n",
      "Epoch 98, Loss 22.279875, \n",
      "Params:  tensor([ 2.7464, -2.4658])\n",
      "Grad:  tensor([-0.4461,  2.5251])\n",
      "------\n",
      "------\n",
      "Epoch 99, Loss 22.214186, \n",
      "Params:  tensor([ 2.7508, -2.4910])\n",
      "Grad:  tensor([-0.4453,  2.5208])\n",
      "------\n",
      "------\n",
      "Epoch 100, Loss 22.148710, \n",
      "Params:  tensor([ 2.7553, -2.5162])\n",
      "Grad:  tensor([-0.4446,  2.5165])\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_un, # <1>\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Epoch 1, Loss 80.364342, \n",
      "Params:  tensor([1.7761, 0.1064])\n",
      "Grad:  tensor([-77.6140, -10.6400])\n",
      "------\n",
      "------\n",
      "Epoch 2, Loss 37.574913, \n",
      "Params:  tensor([2.0848, 0.1303])\n",
      "Grad:  tensor([-30.8623,  -2.3864])\n",
      "------\n",
      "------\n",
      "Epoch 3, Loss 30.871077, \n",
      "Params:  tensor([2.2094, 0.1217])\n",
      "Grad:  tensor([-12.4631,   0.8587])\n",
      "------\n",
      "------\n",
      "Epoch 4, Loss 29.756193, \n",
      "Params:  tensor([2.2616, 0.1004])\n",
      "Grad:  tensor([-5.2218,  2.1327])\n",
      "------\n",
      "------\n",
      "Epoch 5, Loss 29.507153, \n",
      "Params:  tensor([2.2853, 0.0740])\n",
      "Grad:  tensor([-2.3715,  2.6310])\n",
      "------\n",
      "------\n",
      "Epoch 6, Loss 29.392456, \n",
      "Params:  tensor([2.2978, 0.0458])\n",
      "Grad:  tensor([-1.2492,  2.8241])\n",
      "------\n",
      "------\n",
      "Epoch 7, Loss 29.298828, \n",
      "Params:  tensor([2.3059, 0.0168])\n",
      "Grad:  tensor([-0.8071,  2.8970])\n",
      "------\n",
      "------\n",
      "Epoch 8, Loss 29.208717, \n",
      "Params:  tensor([ 2.3122, -0.0124])\n",
      "Grad:  tensor([-0.6325,  2.9227])\n",
      "------\n",
      "------\n",
      "Epoch 9, Loss 29.119415, \n",
      "Params:  tensor([ 2.3178, -0.0417])\n",
      "Grad:  tensor([-0.5633,  2.9298])\n",
      "------\n",
      "------\n",
      "Epoch 10, Loss 29.030489, \n",
      "Params:  tensor([ 2.3232, -0.0710])\n",
      "Grad:  tensor([-0.5355,  2.9295])\n",
      "------\n",
      "------\n",
      "Epoch 11, Loss 28.941877, \n",
      "Params:  tensor([ 2.3284, -0.1003])\n",
      "Grad:  tensor([-0.5240,  2.9264])\n",
      "------\n",
      "------\n",
      "Epoch 12, Loss 28.853565, \n",
      "Params:  tensor([ 2.3336, -0.1295])\n",
      "Grad:  tensor([-0.5190,  2.9222])\n",
      "------\n",
      "------\n",
      "Epoch 13, Loss 28.765553, \n",
      "Params:  tensor([ 2.3388, -0.1587])\n",
      "Grad:  tensor([-0.5165,  2.9175])\n",
      "------\n",
      "------\n",
      "Epoch 14, Loss 28.677851, \n",
      "Params:  tensor([ 2.3439, -0.1878])\n",
      "Grad:  tensor([-0.5150,  2.9126])\n",
      "------\n",
      "------\n",
      "Epoch 15, Loss 28.590431, \n",
      "Params:  tensor([ 2.3491, -0.2169])\n",
      "Grad:  tensor([-0.5138,  2.9077])\n",
      "------\n",
      "------\n",
      "Epoch 16, Loss 28.503319, \n",
      "Params:  tensor([ 2.3542, -0.2459])\n",
      "Grad:  tensor([-0.5129,  2.9028])\n",
      "------\n",
      "------\n",
      "Epoch 17, Loss 28.416498, \n",
      "Params:  tensor([ 2.3593, -0.2749])\n",
      "Grad:  tensor([-0.5120,  2.8979])\n",
      "------\n",
      "------\n",
      "Epoch 18, Loss 28.329973, \n",
      "Params:  tensor([ 2.3644, -0.3038])\n",
      "Grad:  tensor([-0.5111,  2.8930])\n",
      "------\n",
      "------\n",
      "Epoch 19, Loss 28.243742, \n",
      "Params:  tensor([ 2.3695, -0.3327])\n",
      "Grad:  tensor([-0.5102,  2.8881])\n",
      "------\n",
      "------\n",
      "Epoch 20, Loss 28.157804, \n",
      "Params:  tensor([ 2.3746, -0.3615])\n",
      "Grad:  tensor([-0.5093,  2.8832])\n",
      "------\n",
      "------\n",
      "Epoch 21, Loss 28.072151, \n",
      "Params:  tensor([ 2.3797, -0.3903])\n",
      "Grad:  tensor([-0.5084,  2.8783])\n",
      "------\n",
      "------\n",
      "Epoch 22, Loss 27.986797, \n",
      "Params:  tensor([ 2.3848, -0.4190])\n",
      "Grad:  tensor([-0.5076,  2.8734])\n",
      "------\n",
      "------\n",
      "Epoch 23, Loss 27.901728, \n",
      "Params:  tensor([ 2.3899, -0.4477])\n",
      "Grad:  tensor([-0.5067,  2.8685])\n",
      "------\n",
      "------\n",
      "Epoch 24, Loss 27.816950, \n",
      "Params:  tensor([ 2.3949, -0.4763])\n",
      "Grad:  tensor([-0.5059,  2.8636])\n",
      "------\n",
      "------\n",
      "Epoch 25, Loss 27.732464, \n",
      "Params:  tensor([ 2.4000, -0.5049])\n",
      "Grad:  tensor([-0.5050,  2.8588])\n",
      "------\n",
      "------\n",
      "Epoch 26, Loss 27.648256, \n",
      "Params:  tensor([ 2.4050, -0.5335])\n",
      "Grad:  tensor([-0.5042,  2.8539])\n",
      "------\n",
      "------\n",
      "Epoch 27, Loss 27.564344, \n",
      "Params:  tensor([ 2.4101, -0.5620])\n",
      "Grad:  tensor([-0.5033,  2.8490])\n",
      "------\n",
      "------\n",
      "Epoch 28, Loss 27.480707, \n",
      "Params:  tensor([ 2.4151, -0.5904])\n",
      "Grad:  tensor([-0.5024,  2.8442])\n",
      "------\n",
      "------\n",
      "Epoch 29, Loss 27.397362, \n",
      "Params:  tensor([ 2.4201, -0.6188])\n",
      "Grad:  tensor([-0.5016,  2.8394])\n",
      "------\n",
      "------\n",
      "Epoch 30, Loss 27.314295, \n",
      "Params:  tensor([ 2.4251, -0.6471])\n",
      "Grad:  tensor([-0.5007,  2.8346])\n",
      "------\n",
      "------\n",
      "Epoch 31, Loss 27.231512, \n",
      "Params:  tensor([ 2.4301, -0.6754])\n",
      "Grad:  tensor([-0.4999,  2.8297])\n",
      "------\n",
      "------\n",
      "Epoch 32, Loss 27.149010, \n",
      "Params:  tensor([ 2.4351, -0.7037])\n",
      "Grad:  tensor([-0.4990,  2.8249])\n",
      "------\n",
      "------\n",
      "Epoch 33, Loss 27.066790, \n",
      "Params:  tensor([ 2.4401, -0.7319])\n",
      "Grad:  tensor([-0.4982,  2.8201])\n",
      "------\n",
      "------\n",
      "Epoch 34, Loss 26.984844, \n",
      "Params:  tensor([ 2.4450, -0.7600])\n",
      "Grad:  tensor([-0.4973,  2.8153])\n",
      "------\n",
      "------\n",
      "Epoch 35, Loss 26.903175, \n",
      "Params:  tensor([ 2.4500, -0.7881])\n",
      "Grad:  tensor([-0.4965,  2.8106])\n",
      "------\n",
      "------\n",
      "Epoch 36, Loss 26.821791, \n",
      "Params:  tensor([ 2.4550, -0.8162])\n",
      "Grad:  tensor([-0.4957,  2.8058])\n",
      "------\n",
      "------\n",
      "Epoch 37, Loss 26.740679, \n",
      "Params:  tensor([ 2.4599, -0.8442])\n",
      "Grad:  tensor([-0.4948,  2.8010])\n",
      "------\n",
      "------\n",
      "Epoch 38, Loss 26.659838, \n",
      "Params:  tensor([ 2.4649, -0.8722])\n",
      "Grad:  tensor([-0.4940,  2.7963])\n",
      "------\n",
      "------\n",
      "Epoch 39, Loss 26.579279, \n",
      "Params:  tensor([ 2.4698, -0.9001])\n",
      "Grad:  tensor([-0.4931,  2.7915])\n",
      "------\n",
      "------\n",
      "Epoch 40, Loss 26.498987, \n",
      "Params:  tensor([ 2.4747, -0.9280])\n",
      "Grad:  tensor([-0.4923,  2.7868])\n",
      "------\n",
      "------\n",
      "Epoch 41, Loss 26.418974, \n",
      "Params:  tensor([ 2.4796, -0.9558])\n",
      "Grad:  tensor([-0.4915,  2.7820])\n",
      "------\n",
      "------\n",
      "Epoch 42, Loss 26.339228, \n",
      "Params:  tensor([ 2.4845, -0.9836])\n",
      "Grad:  tensor([-0.4906,  2.7773])\n",
      "------\n",
      "------\n",
      "Epoch 43, Loss 26.259754, \n",
      "Params:  tensor([ 2.4894, -1.0113])\n",
      "Grad:  tensor([-0.4898,  2.7726])\n",
      "------\n",
      "------\n",
      "Epoch 44, Loss 26.180548, \n",
      "Params:  tensor([ 2.4943, -1.0390])\n",
      "Grad:  tensor([-0.4890,  2.7679])\n",
      "------\n",
      "------\n",
      "Epoch 45, Loss 26.101616, \n",
      "Params:  tensor([ 2.4992, -1.0666])\n",
      "Grad:  tensor([-0.4881,  2.7632])\n",
      "------\n",
      "------\n",
      "Epoch 46, Loss 26.022947, \n",
      "Params:  tensor([ 2.5041, -1.0942])\n",
      "Grad:  tensor([-0.4873,  2.7585])\n",
      "------\n",
      "------\n",
      "Epoch 47, Loss 25.944544, \n",
      "Params:  tensor([ 2.5089, -1.1217])\n",
      "Grad:  tensor([-0.4865,  2.7538])\n",
      "------\n",
      "------\n",
      "Epoch 48, Loss 25.866417, \n",
      "Params:  tensor([ 2.5138, -1.1492])\n",
      "Grad:  tensor([-0.4856,  2.7491])\n",
      "------\n",
      "------\n",
      "Epoch 49, Loss 25.788549, \n",
      "Params:  tensor([ 2.5186, -1.1766])\n",
      "Grad:  tensor([-0.4848,  2.7444])\n",
      "------\n",
      "------\n",
      "Epoch 50, Loss 25.710938, \n",
      "Params:  tensor([ 2.5235, -1.2040])\n",
      "Grad:  tensor([-0.4840,  2.7398])\n",
      "------\n",
      "------\n",
      "Epoch 51, Loss 25.633600, \n",
      "Params:  tensor([ 2.5283, -1.2314])\n",
      "Grad:  tensor([-0.4832,  2.7351])\n",
      "------\n",
      "------\n",
      "Epoch 52, Loss 25.556524, \n",
      "Params:  tensor([ 2.5331, -1.2587])\n",
      "Grad:  tensor([-0.4823,  2.7305])\n",
      "------\n",
      "------\n",
      "Epoch 53, Loss 25.479700, \n",
      "Params:  tensor([ 2.5379, -1.2860])\n",
      "Grad:  tensor([-0.4815,  2.7258])\n",
      "------\n",
      "------\n",
      "Epoch 54, Loss 25.403149, \n",
      "Params:  tensor([ 2.5428, -1.3132])\n",
      "Grad:  tensor([-0.4807,  2.7212])\n",
      "------\n",
      "------\n",
      "Epoch 55, Loss 25.326851, \n",
      "Params:  tensor([ 2.5476, -1.3403])\n",
      "Grad:  tensor([-0.4799,  2.7166])\n",
      "------\n",
      "------\n",
      "Epoch 56, Loss 25.250811, \n",
      "Params:  tensor([ 2.5523, -1.3675])\n",
      "Grad:  tensor([-0.4791,  2.7120])\n",
      "------\n",
      "------\n",
      "Epoch 57, Loss 25.175035, \n",
      "Params:  tensor([ 2.5571, -1.3945])\n",
      "Grad:  tensor([-0.4783,  2.7074])\n",
      "------\n",
      "------\n",
      "Epoch 58, Loss 25.099512, \n",
      "Params:  tensor([ 2.5619, -1.4216])\n",
      "Grad:  tensor([-0.4775,  2.7028])\n",
      "------\n",
      "------\n",
      "Epoch 59, Loss 25.024248, \n",
      "Params:  tensor([ 2.5667, -1.4485])\n",
      "Grad:  tensor([-0.4766,  2.6982])\n",
      "------\n",
      "------\n",
      "Epoch 60, Loss 24.949236, \n",
      "Params:  tensor([ 2.5714, -1.4755])\n",
      "Grad:  tensor([-0.4758,  2.6936])\n",
      "------\n",
      "------\n",
      "Epoch 61, Loss 24.874483, \n",
      "Params:  tensor([ 2.5762, -1.5024])\n",
      "Grad:  tensor([-0.4750,  2.6890])\n",
      "------\n",
      "------\n",
      "Epoch 62, Loss 24.799976, \n",
      "Params:  tensor([ 2.5809, -1.5292])\n",
      "Grad:  tensor([-0.4742,  2.6845])\n",
      "------\n",
      "------\n",
      "Epoch 63, Loss 24.725737, \n",
      "Params:  tensor([ 2.5857, -1.5560])\n",
      "Grad:  tensor([-0.4734,  2.6799])\n",
      "------\n",
      "------\n",
      "Epoch 64, Loss 24.651739, \n",
      "Params:  tensor([ 2.5904, -1.5828])\n",
      "Grad:  tensor([-0.4726,  2.6753])\n",
      "------\n",
      "------\n",
      "Epoch 65, Loss 24.577986, \n",
      "Params:  tensor([ 2.5951, -1.6095])\n",
      "Grad:  tensor([-0.4718,  2.6708])\n",
      "------\n",
      "------\n",
      "Epoch 66, Loss 24.504494, \n",
      "Params:  tensor([ 2.5998, -1.6361])\n",
      "Grad:  tensor([-0.4710,  2.6663])\n",
      "------\n",
      "------\n",
      "Epoch 67, Loss 24.431252, \n",
      "Params:  tensor([ 2.6045, -1.6628])\n",
      "Grad:  tensor([-0.4702,  2.6617])\n",
      "------\n",
      "------\n",
      "Epoch 68, Loss 24.358257, \n",
      "Params:  tensor([ 2.6092, -1.6893])\n",
      "Grad:  tensor([-0.4694,  2.6572])\n",
      "------\n",
      "------\n",
      "Epoch 69, Loss 24.285505, \n",
      "Params:  tensor([ 2.6139, -1.7159])\n",
      "Grad:  tensor([-0.4686,  2.6527])\n",
      "------\n",
      "------\n",
      "Epoch 70, Loss 24.212999, \n",
      "Params:  tensor([ 2.6186, -1.7423])\n",
      "Grad:  tensor([-0.4678,  2.6482])\n",
      "------\n",
      "------\n",
      "Epoch 71, Loss 24.140741, \n",
      "Params:  tensor([ 2.6232, -1.7688])\n",
      "Grad:  tensor([-0.4670,  2.6437])\n",
      "------\n",
      "------\n",
      "Epoch 72, Loss 24.068733, \n",
      "Params:  tensor([ 2.6279, -1.7952])\n",
      "Grad:  tensor([-0.4662,  2.6392])\n",
      "------\n",
      "------\n",
      "Epoch 73, Loss 23.996971, \n",
      "Params:  tensor([ 2.6326, -1.8215])\n",
      "Grad:  tensor([-0.4654,  2.6347])\n",
      "------\n",
      "------\n",
      "Epoch 74, Loss 23.925446, \n",
      "Params:  tensor([ 2.6372, -1.8478])\n",
      "Grad:  tensor([-0.4646,  2.6302])\n",
      "------\n",
      "------\n",
      "Epoch 75, Loss 23.854168, \n",
      "Params:  tensor([ 2.6418, -1.8741])\n",
      "Grad:  tensor([-0.4638,  2.6258])\n",
      "------\n",
      "------\n",
      "Epoch 76, Loss 23.783125, \n",
      "Params:  tensor([ 2.6465, -1.9003])\n",
      "Grad:  tensor([-0.4631,  2.6213])\n",
      "------\n",
      "------\n",
      "Epoch 77, Loss 23.712328, \n",
      "Params:  tensor([ 2.6511, -1.9265])\n",
      "Grad:  tensor([-0.4623,  2.6169])\n",
      "------\n",
      "------\n",
      "Epoch 78, Loss 23.641773, \n",
      "Params:  tensor([ 2.6557, -1.9526])\n",
      "Grad:  tensor([-0.4615,  2.6124])\n",
      "------\n",
      "------\n",
      "Epoch 79, Loss 23.571455, \n",
      "Params:  tensor([ 2.6603, -1.9787])\n",
      "Grad:  tensor([-0.4607,  2.6080])\n",
      "------\n",
      "------\n",
      "Epoch 80, Loss 23.501379, \n",
      "Params:  tensor([ 2.6649, -2.0047])\n",
      "Grad:  tensor([-0.4599,  2.6035])\n",
      "------\n",
      "------\n",
      "Epoch 81, Loss 23.431538, \n",
      "Params:  tensor([ 2.6695, -2.0307])\n",
      "Grad:  tensor([-0.4591,  2.5991])\n",
      "------\n",
      "------\n",
      "Epoch 82, Loss 23.361937, \n",
      "Params:  tensor([ 2.6741, -2.0566])\n",
      "Grad:  tensor([-0.4584,  2.5947])\n",
      "------\n",
      "------\n",
      "Epoch 83, Loss 23.292570, \n",
      "Params:  tensor([ 2.6787, -2.0825])\n",
      "Grad:  tensor([-0.4576,  2.5903])\n",
      "------\n",
      "------\n",
      "Epoch 84, Loss 23.223436, \n",
      "Params:  tensor([ 2.6832, -2.1084])\n",
      "Grad:  tensor([-0.4568,  2.5859])\n",
      "------\n",
      "------\n",
      "Epoch 85, Loss 23.154541, \n",
      "Params:  tensor([ 2.6878, -2.1342])\n",
      "Grad:  tensor([-0.4560,  2.5815])\n",
      "------\n",
      "------\n",
      "Epoch 86, Loss 23.085882, \n",
      "Params:  tensor([ 2.6923, -2.1600])\n",
      "Grad:  tensor([-0.4553,  2.5771])\n",
      "------\n",
      "------\n",
      "Epoch 87, Loss 23.017447, \n",
      "Params:  tensor([ 2.6969, -2.1857])\n",
      "Grad:  tensor([-0.4545,  2.5727])\n",
      "------\n",
      "------\n",
      "Epoch 88, Loss 22.949251, \n",
      "Params:  tensor([ 2.7014, -2.2114])\n",
      "Grad:  tensor([-0.4537,  2.5684])\n",
      "------\n",
      "------\n",
      "Epoch 89, Loss 22.881283, \n",
      "Params:  tensor([ 2.7060, -2.2370])\n",
      "Grad:  tensor([-0.4529,  2.5640])\n",
      "------\n",
      "------\n",
      "Epoch 90, Loss 22.813549, \n",
      "Params:  tensor([ 2.7105, -2.2626])\n",
      "Grad:  tensor([-0.4522,  2.5597])\n",
      "------\n",
      "------\n",
      "Epoch 91, Loss 22.746044, \n",
      "Params:  tensor([ 2.7150, -2.2882])\n",
      "Grad:  tensor([-0.4514,  2.5553])\n",
      "------\n",
      "------\n",
      "Epoch 92, Loss 22.678766, \n",
      "Params:  tensor([ 2.7195, -2.3137])\n",
      "Grad:  tensor([-0.4506,  2.5510])\n",
      "------\n",
      "------\n",
      "Epoch 93, Loss 22.611717, \n",
      "Params:  tensor([ 2.7240, -2.3392])\n",
      "Grad:  tensor([-0.4499,  2.5466])\n",
      "------\n",
      "------\n",
      "Epoch 94, Loss 22.544899, \n",
      "Params:  tensor([ 2.7285, -2.3646])\n",
      "Grad:  tensor([-0.4491,  2.5423])\n",
      "------\n",
      "------\n",
      "Epoch 95, Loss 22.478306, \n",
      "Params:  tensor([ 2.7330, -2.3900])\n",
      "Grad:  tensor([-0.4483,  2.5380])\n",
      "------\n",
      "------\n",
      "Epoch 96, Loss 22.411934, \n",
      "Params:  tensor([ 2.7374, -2.4153])\n",
      "Grad:  tensor([-0.4476,  2.5337])\n",
      "------\n",
      "------\n",
      "Epoch 97, Loss 22.345793, \n",
      "Params:  tensor([ 2.7419, -2.4406])\n",
      "Grad:  tensor([-0.4468,  2.5294])\n",
      "------\n",
      "------\n",
      "Epoch 98, Loss 22.279875, \n",
      "Params:  tensor([ 2.7464, -2.4658])\n",
      "Grad:  tensor([-0.4461,  2.5251])\n",
      "------\n",
      "------\n",
      "Epoch 99, Loss 22.214186, \n",
      "Params:  tensor([ 2.7508, -2.4910])\n",
      "Grad:  tensor([-0.4453,  2.5208])\n",
      "------\n",
      "------\n",
      "Epoch 100, Loss 22.148710, \n",
      "Params:  tensor([ 2.7553, -2.5162])\n",
      "Grad:  tensor([-0.4446,  2.5165])\n",
      "------\n",
      "------\n",
      "Epoch 101, Loss 22.083464, \n",
      "Params:  tensor([ 2.7597, -2.5413])\n",
      "Grad:  tensor([-0.4438,  2.5122])\n",
      "------\n",
      "------\n",
      "Epoch 102, Loss 22.018436, \n",
      "Params:  tensor([ 2.7641, -2.5664])\n",
      "Grad:  tensor([-0.4430,  2.5080])\n",
      "------\n",
      "------\n",
      "Epoch 103, Loss 21.953632, \n",
      "Params:  tensor([ 2.7686, -2.5914])\n",
      "Grad:  tensor([-0.4423,  2.5037])\n",
      "------\n",
      "------\n",
      "Epoch 104, Loss 21.889046, \n",
      "Params:  tensor([ 2.7730, -2.6164])\n",
      "Grad:  tensor([-0.4415,  2.4994])\n",
      "------\n",
      "------\n",
      "Epoch 105, Loss 21.824677, \n",
      "Params:  tensor([ 2.7774, -2.6414])\n",
      "Grad:  tensor([-0.4408,  2.4952])\n",
      "------\n",
      "------\n",
      "Epoch 106, Loss 21.760529, \n",
      "Params:  tensor([ 2.7818, -2.6663])\n",
      "Grad:  tensor([-0.4400,  2.4910])\n",
      "------\n",
      "------\n",
      "Epoch 107, Loss 21.696600, \n",
      "Params:  tensor([ 2.7862, -2.6912])\n",
      "Grad:  tensor([-0.4393,  2.4867])\n",
      "------\n",
      "------\n",
      "Epoch 108, Loss 21.632883, \n",
      "Params:  tensor([ 2.7906, -2.7160])\n",
      "Grad:  tensor([-0.4385,  2.4825])\n",
      "------\n",
      "------\n",
      "Epoch 109, Loss 21.569389, \n",
      "Params:  tensor([ 2.7949, -2.7408])\n",
      "Grad:  tensor([-0.4378,  2.4783])\n",
      "------\n",
      "------\n",
      "Epoch 110, Loss 21.506102, \n",
      "Params:  tensor([ 2.7993, -2.7655])\n",
      "Grad:  tensor([-0.4370,  2.4741])\n",
      "------\n",
      "------\n",
      "Epoch 111, Loss 21.443037, \n",
      "Params:  tensor([ 2.8037, -2.7902])\n",
      "Grad:  tensor([-0.4363,  2.4699])\n",
      "------\n",
      "------\n",
      "Epoch 112, Loss 21.380186, \n",
      "Params:  tensor([ 2.8080, -2.8149])\n",
      "Grad:  tensor([-0.4356,  2.4657])\n",
      "------\n",
      "------\n",
      "Epoch 113, Loss 21.317549, \n",
      "Params:  tensor([ 2.8124, -2.8395])\n",
      "Grad:  tensor([-0.4348,  2.4615])\n",
      "------\n",
      "------\n",
      "Epoch 114, Loss 21.255117, \n",
      "Params:  tensor([ 2.8167, -2.8641])\n",
      "Grad:  tensor([-0.4341,  2.4573])\n",
      "------\n",
      "------\n",
      "Epoch 115, Loss 21.192907, \n",
      "Params:  tensor([ 2.8211, -2.8886])\n",
      "Grad:  tensor([-0.4334,  2.4531])\n",
      "------\n",
      "------\n",
      "Epoch 116, Loss 21.130898, \n",
      "Params:  tensor([ 2.8254, -2.9131])\n",
      "Grad:  tensor([-0.4326,  2.4490])\n",
      "------\n",
      "------\n",
      "Epoch 117, Loss 21.069105, \n",
      "Params:  tensor([ 2.8297, -2.9375])\n",
      "Grad:  tensor([-0.4319,  2.4448])\n",
      "------\n",
      "------\n",
      "Epoch 118, Loss 21.007526, \n",
      "Params:  tensor([ 2.8340, -2.9619])\n",
      "Grad:  tensor([-0.4311,  2.4407])\n",
      "------\n",
      "------\n",
      "Epoch 119, Loss 20.946150, \n",
      "Params:  tensor([ 2.8383, -2.9863])\n",
      "Grad:  tensor([-0.4304,  2.4365])\n",
      "------\n",
      "------\n",
      "Epoch 120, Loss 20.884981, \n",
      "Params:  tensor([ 2.8426, -3.0106])\n",
      "Grad:  tensor([-0.4297,  2.4324])\n",
      "------\n",
      "------\n",
      "Epoch 121, Loss 20.824024, \n",
      "Params:  tensor([ 2.8469, -3.0349])\n",
      "Grad:  tensor([-0.4290,  2.4282])\n",
      "------\n",
      "------\n",
      "Epoch 122, Loss 20.763273, \n",
      "Params:  tensor([ 2.8512, -3.0592])\n",
      "Grad:  tensor([-0.4282,  2.4241])\n",
      "------\n",
      "------\n",
      "Epoch 123, Loss 20.702728, \n",
      "Params:  tensor([ 2.8555, -3.0834])\n",
      "Grad:  tensor([-0.4275,  2.4200])\n",
      "------\n",
      "------\n",
      "Epoch 124, Loss 20.642384, \n",
      "Params:  tensor([ 2.8597, -3.1075])\n",
      "Grad:  tensor([-0.4268,  2.4159])\n",
      "------\n",
      "------\n",
      "Epoch 125, Loss 20.582249, \n",
      "Params:  tensor([ 2.8640, -3.1316])\n",
      "Grad:  tensor([-0.4260,  2.4118])\n",
      "------\n",
      "------\n",
      "Epoch 126, Loss 20.522322, \n",
      "Params:  tensor([ 2.8682, -3.1557])\n",
      "Grad:  tensor([-0.4253,  2.4077])\n",
      "------\n",
      "------\n",
      "Epoch 127, Loss 20.462593, \n",
      "Params:  tensor([ 2.8725, -3.1797])\n",
      "Grad:  tensor([-0.4246,  2.4036])\n",
      "------\n",
      "------\n",
      "Epoch 128, Loss 20.403069, \n",
      "Params:  tensor([ 2.8767, -3.2037])\n",
      "Grad:  tensor([-0.4239,  2.3995])\n",
      "------\n",
      "------\n",
      "Epoch 129, Loss 20.343742, \n",
      "Params:  tensor([ 2.8810, -3.2277])\n",
      "Grad:  tensor([-0.4232,  2.3954])\n",
      "------\n",
      "------\n",
      "Epoch 130, Loss 20.284624, \n",
      "Params:  tensor([ 2.8852, -3.2516])\n",
      "Grad:  tensor([-0.4224,  2.3914])\n",
      "------\n",
      "------\n",
      "Epoch 131, Loss 20.225702, \n",
      "Params:  tensor([ 2.8894, -3.2755])\n",
      "Grad:  tensor([-0.4217,  2.3873])\n",
      "------\n",
      "------\n",
      "Epoch 132, Loss 20.166981, \n",
      "Params:  tensor([ 2.8936, -3.2993])\n",
      "Grad:  tensor([-0.4210,  2.3832])\n",
      "------\n",
      "------\n",
      "Epoch 133, Loss 20.108461, \n",
      "Params:  tensor([ 2.8978, -3.3231])\n",
      "Grad:  tensor([-0.4203,  2.3792])\n",
      "------\n",
      "------\n",
      "Epoch 134, Loss 20.050137, \n",
      "Params:  tensor([ 2.9020, -3.3469])\n",
      "Grad:  tensor([-0.4196,  2.3752])\n",
      "------\n",
      "------\n",
      "Epoch 135, Loss 19.992016, \n",
      "Params:  tensor([ 2.9062, -3.3706])\n",
      "Grad:  tensor([-0.4189,  2.3711])\n",
      "------\n",
      "------\n",
      "Epoch 136, Loss 19.934086, \n",
      "Params:  tensor([ 2.9104, -3.3942])\n",
      "Grad:  tensor([-0.4182,  2.3671])\n",
      "------\n",
      "------\n",
      "Epoch 137, Loss 19.876352, \n",
      "Params:  tensor([ 2.9146, -3.4179])\n",
      "Grad:  tensor([-0.4174,  2.3631])\n",
      "------\n",
      "------\n",
      "Epoch 138, Loss 19.818823, \n",
      "Params:  tensor([ 2.9187, -3.4415])\n",
      "Grad:  tensor([-0.4167,  2.3591])\n",
      "------\n",
      "------\n",
      "Epoch 139, Loss 19.761480, \n",
      "Params:  tensor([ 2.9229, -3.4650])\n",
      "Grad:  tensor([-0.4160,  2.3550])\n",
      "------\n",
      "------\n",
      "Epoch 140, Loss 19.704336, \n",
      "Params:  tensor([ 2.9270, -3.4885])\n",
      "Grad:  tensor([-0.4153,  2.3510])\n",
      "------\n",
      "------\n",
      "Epoch 141, Loss 19.647385, \n",
      "Params:  tensor([ 2.9312, -3.5120])\n",
      "Grad:  tensor([-0.4146,  2.3471])\n",
      "------\n",
      "------\n",
      "Epoch 142, Loss 19.590626, \n",
      "Params:  tensor([ 2.9353, -3.5354])\n",
      "Grad:  tensor([-0.4139,  2.3431])\n",
      "------\n",
      "------\n",
      "Epoch 143, Loss 19.534061, \n",
      "Params:  tensor([ 2.9395, -3.5588])\n",
      "Grad:  tensor([-0.4132,  2.3391])\n",
      "------\n",
      "------\n",
      "Epoch 144, Loss 19.477690, \n",
      "Params:  tensor([ 2.9436, -3.5822])\n",
      "Grad:  tensor([-0.4125,  2.3351])\n",
      "------\n",
      "------\n",
      "Epoch 145, Loss 19.421507, \n",
      "Params:  tensor([ 2.9477, -3.6055])\n",
      "Grad:  tensor([-0.4118,  2.3311])\n",
      "------\n",
      "------\n",
      "Epoch 146, Loss 19.365515, \n",
      "Params:  tensor([ 2.9518, -3.6287])\n",
      "Grad:  tensor([-0.4111,  2.3272])\n",
      "------\n",
      "------\n",
      "Epoch 147, Loss 19.309715, \n",
      "Params:  tensor([ 2.9559, -3.6520])\n",
      "Grad:  tensor([-0.4104,  2.3232])\n",
      "------\n",
      "------\n",
      "Epoch 148, Loss 19.254107, \n",
      "Params:  tensor([ 2.9600, -3.6752])\n",
      "Grad:  tensor([-0.4097,  2.3193])\n",
      "------\n",
      "------\n",
      "Epoch 149, Loss 19.198685, \n",
      "Params:  tensor([ 2.9641, -3.6983])\n",
      "Grad:  tensor([-0.4090,  2.3153])\n",
      "------\n",
      "------\n",
      "Epoch 150, Loss 19.143446, \n",
      "Params:  tensor([ 2.9682, -3.7214])\n",
      "Grad:  tensor([-0.4083,  2.3114])\n",
      "------\n",
      "------\n",
      "Epoch 151, Loss 19.088402, \n",
      "Params:  tensor([ 2.9723, -3.7445])\n",
      "Grad:  tensor([-0.4076,  2.3075])\n",
      "------\n",
      "------\n",
      "Epoch 152, Loss 19.033543, \n",
      "Params:  tensor([ 2.9763, -3.7675])\n",
      "Grad:  tensor([-0.4069,  2.3036])\n",
      "------\n",
      "------\n",
      "Epoch 153, Loss 18.978868, \n",
      "Params:  tensor([ 2.9804, -3.7905])\n",
      "Grad:  tensor([-0.4062,  2.2997])\n",
      "------\n",
      "------\n",
      "Epoch 154, Loss 18.924377, \n",
      "Params:  tensor([ 2.9844, -3.8135])\n",
      "Grad:  tensor([-0.4056,  2.2957])\n",
      "------\n",
      "------\n",
      "Epoch 155, Loss 18.870081, \n",
      "Params:  tensor([ 2.9885, -3.8364])\n",
      "Grad:  tensor([-0.4049,  2.2918])\n",
      "------\n",
      "------\n",
      "Epoch 156, Loss 18.815960, \n",
      "Params:  tensor([ 2.9925, -3.8593])\n",
      "Grad:  tensor([-0.4042,  2.2880])\n",
      "------\n",
      "------\n",
      "Epoch 157, Loss 18.762022, \n",
      "Params:  tensor([ 2.9966, -3.8821])\n",
      "Grad:  tensor([-0.4035,  2.2841])\n",
      "------\n",
      "------\n",
      "Epoch 158, Loss 18.708271, \n",
      "Params:  tensor([ 3.0006, -3.9049])\n",
      "Grad:  tensor([-0.4028,  2.2802])\n",
      "------\n",
      "------\n",
      "Epoch 159, Loss 18.654699, \n",
      "Params:  tensor([ 3.0046, -3.9277])\n",
      "Grad:  tensor([-0.4021,  2.2763])\n",
      "------\n",
      "------\n",
      "Epoch 160, Loss 18.601313, \n",
      "Params:  tensor([ 3.0086, -3.9504])\n",
      "Grad:  tensor([-0.4014,  2.2724])\n",
      "------\n",
      "------\n",
      "Epoch 161, Loss 18.548109, \n",
      "Params:  tensor([ 3.0126, -3.9731])\n",
      "Grad:  tensor([-0.4007,  2.2686])\n",
      "------\n",
      "------\n",
      "Epoch 162, Loss 18.495085, \n",
      "Params:  tensor([ 3.0166, -3.9958])\n",
      "Grad:  tensor([-0.4001,  2.2647])\n",
      "------\n",
      "------\n",
      "Epoch 163, Loss 18.442236, \n",
      "Params:  tensor([ 3.0206, -4.0184])\n",
      "Grad:  tensor([-0.3994,  2.2609])\n",
      "------\n",
      "------\n",
      "Epoch 164, Loss 18.389570, \n",
      "Params:  tensor([ 3.0246, -4.0409])\n",
      "Grad:  tensor([-0.3987,  2.2570])\n",
      "------\n",
      "------\n",
      "Epoch 165, Loss 18.337080, \n",
      "Params:  tensor([ 3.0286, -4.0635])\n",
      "Grad:  tensor([-0.3980,  2.2532])\n",
      "------\n",
      "------\n",
      "Epoch 166, Loss 18.284777, \n",
      "Params:  tensor([ 3.0326, -4.0860])\n",
      "Grad:  tensor([-0.3974,  2.2494])\n",
      "------\n",
      "------\n",
      "Epoch 167, Loss 18.232641, \n",
      "Params:  tensor([ 3.0365, -4.1084])\n",
      "Grad:  tensor([-0.3967,  2.2456])\n",
      "------\n",
      "------\n",
      "Epoch 168, Loss 18.180685, \n",
      "Params:  tensor([ 3.0405, -4.1308])\n",
      "Grad:  tensor([-0.3960,  2.2417])\n",
      "------\n",
      "------\n",
      "Epoch 169, Loss 18.128906, \n",
      "Params:  tensor([ 3.0445, -4.1532])\n",
      "Grad:  tensor([-0.3953,  2.2379])\n",
      "------\n",
      "------\n",
      "Epoch 170, Loss 18.077301, \n",
      "Params:  tensor([ 3.0484, -4.1756])\n",
      "Grad:  tensor([-0.3947,  2.2341])\n",
      "------\n",
      "------\n",
      "Epoch 171, Loss 18.025877, \n",
      "Params:  tensor([ 3.0523, -4.1979])\n",
      "Grad:  tensor([-0.3940,  2.2303])\n",
      "------\n",
      "------\n",
      "Epoch 172, Loss 17.974623, \n",
      "Params:  tensor([ 3.0563, -4.2201])\n",
      "Grad:  tensor([-0.3933,  2.2266])\n",
      "------\n",
      "------\n",
      "Epoch 173, Loss 17.923546, \n",
      "Params:  tensor([ 3.0602, -4.2424])\n",
      "Grad:  tensor([-0.3927,  2.2228])\n",
      "------\n",
      "------\n",
      "Epoch 174, Loss 17.872643, \n",
      "Params:  tensor([ 3.0641, -4.2646])\n",
      "Grad:  tensor([-0.3920,  2.2190])\n",
      "------\n",
      "------\n",
      "Epoch 175, Loss 17.821909, \n",
      "Params:  tensor([ 3.0680, -4.2867])\n",
      "Grad:  tensor([-0.3913,  2.2152])\n",
      "------\n",
      "------\n",
      "Epoch 176, Loss 17.771345, \n",
      "Params:  tensor([ 3.0719, -4.3088])\n",
      "Grad:  tensor([-0.3907,  2.2115])\n",
      "------\n",
      "------\n",
      "Epoch 177, Loss 17.720955, \n",
      "Params:  tensor([ 3.0758, -4.3309])\n",
      "Grad:  tensor([-0.3900,  2.2077])\n",
      "------\n",
      "------\n",
      "Epoch 178, Loss 17.670738, \n",
      "Params:  tensor([ 3.0797, -4.3529])\n",
      "Grad:  tensor([-0.3893,  2.2040])\n",
      "------\n",
      "------\n",
      "Epoch 179, Loss 17.620689, \n",
      "Params:  tensor([ 3.0836, -4.3749])\n",
      "Grad:  tensor([-0.3887,  2.2002])\n",
      "------\n",
      "------\n",
      "Epoch 180, Loss 17.570814, \n",
      "Params:  tensor([ 3.0875, -4.3969])\n",
      "Grad:  tensor([-0.3880,  2.1965])\n",
      "------\n",
      "------\n",
      "Epoch 181, Loss 17.521103, \n",
      "Params:  tensor([ 3.0914, -4.4188])\n",
      "Grad:  tensor([-0.3873,  2.1927])\n",
      "------\n",
      "------\n",
      "Epoch 182, Loss 17.471565, \n",
      "Params:  tensor([ 3.0952, -4.4407])\n",
      "Grad:  tensor([-0.3867,  2.1890])\n",
      "------\n",
      "------\n",
      "Epoch 183, Loss 17.422192, \n",
      "Params:  tensor([ 3.0991, -4.4626])\n",
      "Grad:  tensor([-0.3860,  2.1853])\n",
      "------\n",
      "------\n",
      "Epoch 184, Loss 17.372993, \n",
      "Params:  tensor([ 3.1030, -4.4844])\n",
      "Grad:  tensor([-0.3854,  2.1816])\n",
      "------\n",
      "------\n",
      "Epoch 185, Loss 17.323954, \n",
      "Params:  tensor([ 3.1068, -4.5062])\n",
      "Grad:  tensor([-0.3847,  2.1779])\n",
      "------\n",
      "------\n",
      "Epoch 186, Loss 17.275084, \n",
      "Params:  tensor([ 3.1106, -4.5279])\n",
      "Grad:  tensor([-0.3841,  2.1742])\n",
      "------\n",
      "------\n",
      "Epoch 187, Loss 17.226379, \n",
      "Params:  tensor([ 3.1145, -4.5496])\n",
      "Grad:  tensor([-0.3834,  2.1705])\n",
      "------\n",
      "------\n",
      "Epoch 188, Loss 17.177839, \n",
      "Params:  tensor([ 3.1183, -4.5713])\n",
      "Grad:  tensor([-0.3828,  2.1668])\n",
      "------\n",
      "------\n",
      "Epoch 189, Loss 17.129463, \n",
      "Params:  tensor([ 3.1221, -4.5929])\n",
      "Grad:  tensor([-0.3821,  2.1631])\n",
      "------\n",
      "------\n",
      "Epoch 190, Loss 17.081255, \n",
      "Params:  tensor([ 3.1259, -4.6145])\n",
      "Grad:  tensor([-0.3815,  2.1594])\n",
      "------\n",
      "------\n",
      "Epoch 191, Loss 17.033209, \n",
      "Params:  tensor([ 3.1298, -4.6361])\n",
      "Grad:  tensor([-0.3808,  2.1558])\n",
      "------\n",
      "------\n",
      "Epoch 192, Loss 16.985327, \n",
      "Params:  tensor([ 3.1336, -4.6576])\n",
      "Grad:  tensor([-0.3802,  2.1521])\n",
      "------\n",
      "------\n",
      "Epoch 193, Loss 16.937605, \n",
      "Params:  tensor([ 3.1374, -4.6791])\n",
      "Grad:  tensor([-0.3795,  2.1485])\n",
      "------\n",
      "------\n",
      "Epoch 194, Loss 16.890047, \n",
      "Params:  tensor([ 3.1411, -4.7005])\n",
      "Grad:  tensor([-0.3789,  2.1448])\n",
      "------\n",
      "------\n",
      "Epoch 195, Loss 16.842649, \n",
      "Params:  tensor([ 3.1449, -4.7219])\n",
      "Grad:  tensor([-0.3782,  2.1412])\n",
      "------\n",
      "------\n",
      "Epoch 196, Loss 16.795412, \n",
      "Params:  tensor([ 3.1487, -4.7433])\n",
      "Grad:  tensor([-0.3776,  2.1375])\n",
      "------\n",
      "------\n",
      "Epoch 197, Loss 16.748339, \n",
      "Params:  tensor([ 3.1525, -4.7646])\n",
      "Grad:  tensor([-0.3770,  2.1339])\n",
      "------\n",
      "------\n",
      "Epoch 198, Loss 16.701422, \n",
      "Params:  tensor([ 3.1562, -4.7859])\n",
      "Grad:  tensor([-0.3763,  2.1303])\n",
      "------\n",
      "------\n",
      "Epoch 199, Loss 16.654661, \n",
      "Params:  tensor([ 3.1600, -4.8072])\n",
      "Grad:  tensor([-0.3757,  2.1267])\n",
      "------\n",
      "------\n",
      "Epoch 200, Loss 16.608067, \n",
      "Params:  tensor([ 3.1637, -4.8284])\n",
      "Grad:  tensor([-0.3750,  2.1230])\n",
      "------\n",
      "------\n",
      "Epoch 201, Loss 16.561623, \n",
      "Params:  tensor([ 3.1675, -4.8496])\n",
      "Grad:  tensor([-0.3744,  2.1194])\n",
      "------\n",
      "------\n",
      "Epoch 202, Loss 16.515343, \n",
      "Params:  tensor([ 3.1712, -4.8708])\n",
      "Grad:  tensor([-0.3738,  2.1158])\n",
      "------\n",
      "------\n",
      "Epoch 203, Loss 16.469219, \n",
      "Params:  tensor([ 3.1750, -4.8919])\n",
      "Grad:  tensor([-0.3731,  2.1122])\n",
      "------\n",
      "------\n",
      "Epoch 204, Loss 16.423248, \n",
      "Params:  tensor([ 3.1787, -4.9130])\n",
      "Grad:  tensor([-0.3725,  2.1087])\n",
      "------\n",
      "------\n",
      "Epoch 205, Loss 16.377434, \n",
      "Params:  tensor([ 3.1824, -4.9341])\n",
      "Grad:  tensor([-0.3719,  2.1051])\n",
      "------\n",
      "------\n",
      "Epoch 206, Loss 16.331776, \n",
      "Params:  tensor([ 3.1861, -4.9551])\n",
      "Grad:  tensor([-0.3712,  2.1015])\n",
      "------\n",
      "------\n",
      "Epoch 207, Loss 16.286276, \n",
      "Params:  tensor([ 3.1898, -4.9760])\n",
      "Grad:  tensor([-0.3706,  2.0979])\n",
      "------\n",
      "------\n",
      "Epoch 208, Loss 16.240929, \n",
      "Params:  tensor([ 3.1935, -4.9970])\n",
      "Grad:  tensor([-0.3700,  2.0944])\n",
      "------\n",
      "------\n",
      "Epoch 209, Loss 16.195732, \n",
      "Params:  tensor([ 3.1972, -5.0179])\n",
      "Grad:  tensor([-0.3694,  2.0908])\n",
      "------\n",
      "------\n",
      "Epoch 210, Loss 16.150694, \n",
      "Params:  tensor([ 3.2009, -5.0388])\n",
      "Grad:  tensor([-0.3687,  2.0873])\n",
      "------\n",
      "------\n",
      "Epoch 211, Loss 16.105806, \n",
      "Params:  tensor([ 3.2046, -5.0596])\n",
      "Grad:  tensor([-0.3681,  2.0837])\n",
      "------\n",
      "------\n",
      "Epoch 212, Loss 16.061073, \n",
      "Params:  tensor([ 3.2082, -5.0804])\n",
      "Grad:  tensor([-0.3675,  2.0802])\n",
      "------\n",
      "------\n",
      "Epoch 213, Loss 16.016487, \n",
      "Params:  tensor([ 3.2119, -5.1012])\n",
      "Grad:  tensor([-0.3668,  2.0766])\n",
      "------\n",
      "------\n",
      "Epoch 214, Loss 15.972058, \n",
      "Params:  tensor([ 3.2156, -5.1219])\n",
      "Grad:  tensor([-0.3662,  2.0731])\n",
      "------\n",
      "------\n",
      "Epoch 215, Loss 15.927776, \n",
      "Params:  tensor([ 3.2192, -5.1426])\n",
      "Grad:  tensor([-0.3656,  2.0696])\n",
      "------\n",
      "------\n",
      "Epoch 216, Loss 15.883645, \n",
      "Params:  tensor([ 3.2229, -5.1633])\n",
      "Grad:  tensor([-0.3650,  2.0661])\n",
      "------\n",
      "------\n",
      "Epoch 217, Loss 15.839664, \n",
      "Params:  tensor([ 3.2265, -5.1839])\n",
      "Grad:  tensor([-0.3644,  2.0626])\n",
      "------\n",
      "------\n",
      "Epoch 218, Loss 15.795832, \n",
      "Params:  tensor([ 3.2302, -5.2045])\n",
      "Grad:  tensor([-0.3637,  2.0591])\n",
      "------\n",
      "------\n",
      "Epoch 219, Loss 15.752152, \n",
      "Params:  tensor([ 3.2338, -5.2250])\n",
      "Grad:  tensor([-0.3631,  2.0556])\n",
      "------\n",
      "------\n",
      "Epoch 220, Loss 15.708612, \n",
      "Params:  tensor([ 3.2374, -5.2456])\n",
      "Grad:  tensor([-0.3625,  2.0521])\n",
      "------\n",
      "------\n",
      "Epoch 221, Loss 15.665226, \n",
      "Params:  tensor([ 3.2410, -5.2660])\n",
      "Grad:  tensor([-0.3619,  2.0486])\n",
      "------\n",
      "------\n",
      "Epoch 222, Loss 15.621990, \n",
      "Params:  tensor([ 3.2447, -5.2865])\n",
      "Grad:  tensor([-0.3613,  2.0451])\n",
      "------\n",
      "------\n",
      "Epoch 223, Loss 15.578897, \n",
      "Params:  tensor([ 3.2483, -5.3069])\n",
      "Grad:  tensor([-0.3607,  2.0416])\n",
      "------\n",
      "------\n",
      "Epoch 224, Loss 15.535950, \n",
      "Params:  tensor([ 3.2519, -5.3273])\n",
      "Grad:  tensor([-0.3601,  2.0382])\n",
      "------\n",
      "------\n",
      "Epoch 225, Loss 15.493150, \n",
      "Params:  tensor([ 3.2555, -5.3476])\n",
      "Grad:  tensor([-0.3594,  2.0347])\n",
      "------\n",
      "------\n",
      "Epoch 226, Loss 15.450495, \n",
      "Params:  tensor([ 3.2590, -5.3680])\n",
      "Grad:  tensor([-0.3588,  2.0312])\n",
      "------\n",
      "------\n",
      "Epoch 227, Loss 15.407981, \n",
      "Params:  tensor([ 3.2626, -5.3882])\n",
      "Grad:  tensor([-0.3582,  2.0278])\n",
      "------\n",
      "------\n",
      "Epoch 228, Loss 15.365616, \n",
      "Params:  tensor([ 3.2662, -5.4085])\n",
      "Grad:  tensor([-0.3576,  2.0243])\n",
      "------\n",
      "------\n",
      "Epoch 229, Loss 15.323396, \n",
      "Params:  tensor([ 3.2698, -5.4287])\n",
      "Grad:  tensor([-0.3570,  2.0209])\n",
      "------\n",
      "------\n",
      "Epoch 230, Loss 15.281317, \n",
      "Params:  tensor([ 3.2733, -5.4489])\n",
      "Grad:  tensor([-0.3564,  2.0175])\n",
      "------\n",
      "------\n",
      "Epoch 231, Loss 15.239380, \n",
      "Params:  tensor([ 3.2769, -5.4690])\n",
      "Grad:  tensor([-0.3558,  2.0140])\n",
      "------\n",
      "------\n",
      "Epoch 232, Loss 15.197585, \n",
      "Params:  tensor([ 3.2804, -5.4891])\n",
      "Grad:  tensor([-0.3552,  2.0106])\n",
      "------\n",
      "------\n",
      "Epoch 233, Loss 15.155932, \n",
      "Params:  tensor([ 3.2840, -5.5092])\n",
      "Grad:  tensor([-0.3546,  2.0072])\n",
      "------\n",
      "------\n",
      "Epoch 234, Loss 15.114425, \n",
      "Params:  tensor([ 3.2875, -5.5292])\n",
      "Grad:  tensor([-0.3540,  2.0038])\n",
      "------\n",
      "------\n",
      "Epoch 235, Loss 15.073055, \n",
      "Params:  tensor([ 3.2911, -5.5492])\n",
      "Grad:  tensor([-0.3534,  2.0004])\n",
      "------\n",
      "------\n",
      "Epoch 236, Loss 15.031823, \n",
      "Params:  tensor([ 3.2946, -5.5692])\n",
      "Grad:  tensor([-0.3528,  1.9970])\n",
      "------\n",
      "------\n",
      "Epoch 237, Loss 14.990734, \n",
      "Params:  tensor([ 3.2981, -5.5891])\n",
      "Grad:  tensor([-0.3522,  1.9936])\n",
      "------\n",
      "------\n",
      "Epoch 238, Loss 14.949784, \n",
      "Params:  tensor([ 3.3016, -5.6090])\n",
      "Grad:  tensor([-0.3516,  1.9902])\n",
      "------\n",
      "------\n",
      "Epoch 239, Loss 14.908973, \n",
      "Params:  tensor([ 3.3051, -5.6289])\n",
      "Grad:  tensor([-0.3510,  1.9868])\n",
      "------\n",
      "------\n",
      "Epoch 240, Loss 14.868304, \n",
      "Params:  tensor([ 3.3086, -5.6487])\n",
      "Grad:  tensor([-0.3504,  1.9835])\n",
      "------\n",
      "------\n",
      "Epoch 241, Loss 14.827767, \n",
      "Params:  tensor([ 3.3121, -5.6685])\n",
      "Grad:  tensor([-0.3498,  1.9801])\n",
      "------\n",
      "------\n",
      "Epoch 242, Loss 14.787370, \n",
      "Params:  tensor([ 3.3156, -5.6883])\n",
      "Grad:  tensor([-0.3492,  1.9767])\n",
      "------\n",
      "------\n",
      "Epoch 243, Loss 14.747109, \n",
      "Params:  tensor([ 3.3191, -5.7080])\n",
      "Grad:  tensor([-0.3486,  1.9734])\n",
      "------\n",
      "------\n",
      "Epoch 244, Loss 14.706989, \n",
      "Params:  tensor([ 3.3226, -5.7277])\n",
      "Grad:  tensor([-0.3480,  1.9700])\n",
      "------\n",
      "------\n",
      "Epoch 245, Loss 14.667002, \n",
      "Params:  tensor([ 3.3261, -5.7474])\n",
      "Grad:  tensor([-0.3474,  1.9667])\n",
      "------\n",
      "------\n",
      "Epoch 246, Loss 14.627151, \n",
      "Params:  tensor([ 3.3295, -5.7670])\n",
      "Grad:  tensor([-0.3468,  1.9633])\n",
      "------\n",
      "------\n",
      "Epoch 247, Loss 14.587436, \n",
      "Params:  tensor([ 3.3330, -5.7866])\n",
      "Grad:  tensor([-0.3462,  1.9600])\n",
      "------\n",
      "------\n",
      "Epoch 248, Loss 14.547855, \n",
      "Params:  tensor([ 3.3365, -5.8062])\n",
      "Grad:  tensor([-0.3456,  1.9567])\n",
      "------\n",
      "------\n",
      "Epoch 249, Loss 14.508409, \n",
      "Params:  tensor([ 3.3399, -5.8257])\n",
      "Grad:  tensor([-0.3451,  1.9533])\n",
      "------\n",
      "------\n",
      "Epoch 250, Loss 14.469097, \n",
      "Params:  tensor([ 3.3434, -5.8452])\n",
      "Grad:  tensor([-0.3445,  1.9500])\n",
      "------\n",
      "------\n",
      "Epoch 251, Loss 14.429920, \n",
      "Params:  tensor([ 3.3468, -5.8647])\n",
      "Grad:  tensor([-0.3439,  1.9467])\n",
      "------\n",
      "------\n",
      "Epoch 252, Loss 14.390870, \n",
      "Params:  tensor([ 3.3502, -5.8841])\n",
      "Grad:  tensor([-0.3433,  1.9434])\n",
      "------\n",
      "------\n",
      "Epoch 253, Loss 14.351956, \n",
      "Params:  tensor([ 3.3537, -5.9035])\n",
      "Grad:  tensor([-0.3427,  1.9401])\n",
      "------\n",
      "------\n",
      "Epoch 254, Loss 14.313177, \n",
      "Params:  tensor([ 3.3571, -5.9229])\n",
      "Grad:  tensor([-0.3421,  1.9368])\n",
      "------\n",
      "------\n",
      "Epoch 255, Loss 14.274529, \n",
      "Params:  tensor([ 3.3605, -5.9422])\n",
      "Grad:  tensor([-0.3416,  1.9335])\n",
      "------\n",
      "------\n",
      "Epoch 256, Loss 14.236009, \n",
      "Params:  tensor([ 3.3639, -5.9615])\n",
      "Grad:  tensor([-0.3410,  1.9302])\n",
      "------\n",
      "------\n",
      "Epoch 257, Loss 14.197620, \n",
      "Params:  tensor([ 3.3673, -5.9808])\n",
      "Grad:  tensor([-0.3404,  1.9269])\n",
      "------\n",
      "------\n",
      "Epoch 258, Loss 14.159363, \n",
      "Params:  tensor([ 3.3707, -6.0000])\n",
      "Grad:  tensor([-0.3398,  1.9237])\n",
      "------\n",
      "------\n",
      "Epoch 259, Loss 14.121234, \n",
      "Params:  tensor([ 3.3741, -6.0192])\n",
      "Grad:  tensor([-0.3392,  1.9204])\n",
      "------\n",
      "------\n",
      "Epoch 260, Loss 14.083236, \n",
      "Params:  tensor([ 3.3775, -6.0384])\n",
      "Grad:  tensor([-0.3387,  1.9171])\n",
      "------\n",
      "------\n",
      "Epoch 261, Loss 14.045367, \n",
      "Params:  tensor([ 3.3809, -6.0576])\n",
      "Grad:  tensor([-0.3381,  1.9139])\n",
      "------\n",
      "------\n",
      "Epoch 262, Loss 14.007627, \n",
      "Params:  tensor([ 3.3842, -6.0767])\n",
      "Grad:  tensor([-0.3375,  1.9106])\n",
      "------\n",
      "------\n",
      "Epoch 263, Loss 13.970016, \n",
      "Params:  tensor([ 3.3876, -6.0957])\n",
      "Grad:  tensor([-0.3369,  1.9074])\n",
      "------\n",
      "------\n",
      "Epoch 264, Loss 13.932531, \n",
      "Params:  tensor([ 3.3910, -6.1148])\n",
      "Grad:  tensor([-0.3364,  1.9041])\n",
      "------\n",
      "------\n",
      "Epoch 265, Loss 13.895172, \n",
      "Params:  tensor([ 3.3943, -6.1338])\n",
      "Grad:  tensor([-0.3358,  1.9009])\n",
      "------\n",
      "------\n",
      "Epoch 266, Loss 13.857944, \n",
      "Params:  tensor([ 3.3977, -6.1528])\n",
      "Grad:  tensor([-0.3352,  1.8977])\n",
      "------\n",
      "------\n",
      "Epoch 267, Loss 13.820837, \n",
      "Params:  tensor([ 3.4010, -6.1717])\n",
      "Grad:  tensor([-0.3347,  1.8945])\n",
      "------\n",
      "------\n",
      "Epoch 268, Loss 13.783858, \n",
      "Params:  tensor([ 3.4044, -6.1906])\n",
      "Grad:  tensor([-0.3341,  1.8912])\n",
      "------\n",
      "------\n",
      "Epoch 269, Loss 13.747006, \n",
      "Params:  tensor([ 3.4077, -6.2095])\n",
      "Grad:  tensor([-0.3335,  1.8880])\n",
      "------\n",
      "------\n",
      "Epoch 270, Loss 13.710278, \n",
      "Params:  tensor([ 3.4110, -6.2284])\n",
      "Grad:  tensor([-0.3330,  1.8848])\n",
      "------\n",
      "------\n",
      "Epoch 271, Loss 13.673676, \n",
      "Params:  tensor([ 3.4144, -6.2472])\n",
      "Grad:  tensor([-0.3324,  1.8816])\n",
      "------\n",
      "------\n",
      "Epoch 272, Loss 13.637196, \n",
      "Params:  tensor([ 3.4177, -6.2660])\n",
      "Grad:  tensor([-0.3318,  1.8784])\n",
      "------\n",
      "------\n",
      "Epoch 273, Loss 13.600842, \n",
      "Params:  tensor([ 3.4210, -6.2847])\n",
      "Grad:  tensor([-0.3313,  1.8752])\n",
      "------\n",
      "------\n",
      "Epoch 274, Loss 13.564609, \n",
      "Params:  tensor([ 3.4243, -6.3034])\n",
      "Grad:  tensor([-0.3307,  1.8720])\n",
      "------\n",
      "------\n",
      "Epoch 275, Loss 13.528501, \n",
      "Params:  tensor([ 3.4276, -6.3221])\n",
      "Grad:  tensor([-0.3301,  1.8689])\n",
      "------\n",
      "------\n",
      "Epoch 276, Loss 13.492514, \n",
      "Params:  tensor([ 3.4309, -6.3408])\n",
      "Grad:  tensor([-0.3296,  1.8657])\n",
      "------\n",
      "------\n",
      "Epoch 277, Loss 13.456651, \n",
      "Params:  tensor([ 3.4342, -6.3594])\n",
      "Grad:  tensor([-0.3290,  1.8625])\n",
      "------\n",
      "------\n",
      "Epoch 278, Loss 13.420910, \n",
      "Params:  tensor([ 3.4375, -6.3780])\n",
      "Grad:  tensor([-0.3285,  1.8594])\n",
      "------\n",
      "------\n",
      "Epoch 279, Loss 13.385287, \n",
      "Params:  tensor([ 3.4407, -6.3966])\n",
      "Grad:  tensor([-0.3279,  1.8562])\n",
      "------\n",
      "------\n",
      "Epoch 280, Loss 13.349789, \n",
      "Params:  tensor([ 3.4440, -6.4151])\n",
      "Grad:  tensor([-0.3274,  1.8530])\n",
      "------\n",
      "------\n",
      "Epoch 281, Loss 13.314407, \n",
      "Params:  tensor([ 3.4473, -6.4336])\n",
      "Grad:  tensor([-0.3268,  1.8499])\n",
      "------\n",
      "------\n",
      "Epoch 282, Loss 13.279150, \n",
      "Params:  tensor([ 3.4506, -6.4520])\n",
      "Grad:  tensor([-0.3262,  1.8468])\n",
      "------\n",
      "------\n",
      "Epoch 283, Loss 13.244009, \n",
      "Params:  tensor([ 3.4538, -6.4705])\n",
      "Grad:  tensor([-0.3257,  1.8436])\n",
      "------\n",
      "------\n",
      "Epoch 284, Loss 13.208991, \n",
      "Params:  tensor([ 3.4571, -6.4889])\n",
      "Grad:  tensor([-0.3251,  1.8405])\n",
      "------\n",
      "------\n",
      "Epoch 285, Loss 13.174088, \n",
      "Params:  tensor([ 3.4603, -6.5073])\n",
      "Grad:  tensor([-0.3246,  1.8374])\n",
      "------\n",
      "------\n",
      "Epoch 286, Loss 13.139307, \n",
      "Params:  tensor([ 3.4635, -6.5256])\n",
      "Grad:  tensor([-0.3240,  1.8342])\n",
      "------\n",
      "------\n",
      "Epoch 287, Loss 13.104639, \n",
      "Params:  tensor([ 3.4668, -6.5439])\n",
      "Grad:  tensor([-0.3235,  1.8311])\n",
      "------\n",
      "------\n",
      "Epoch 288, Loss 13.070092, \n",
      "Params:  tensor([ 3.4700, -6.5622])\n",
      "Grad:  tensor([-0.3229,  1.8280])\n",
      "------\n",
      "------\n",
      "Epoch 289, Loss 13.035664, \n",
      "Params:  tensor([ 3.4732, -6.5804])\n",
      "Grad:  tensor([-0.3224,  1.8249])\n",
      "------\n",
      "------\n",
      "Epoch 290, Loss 13.001349, \n",
      "Params:  tensor([ 3.4765, -6.5987])\n",
      "Grad:  tensor([-0.3218,  1.8218])\n",
      "------\n",
      "------\n",
      "Epoch 291, Loss 12.967152, \n",
      "Params:  tensor([ 3.4797, -6.6169])\n",
      "Grad:  tensor([-0.3213,  1.8187])\n",
      "------\n",
      "------\n",
      "Epoch 292, Loss 12.933075, \n",
      "Params:  tensor([ 3.4829, -6.6350])\n",
      "Grad:  tensor([-0.3207,  1.8156])\n",
      "------\n",
      "------\n",
      "Epoch 293, Loss 12.899109, \n",
      "Params:  tensor([ 3.4861, -6.6531])\n",
      "Grad:  tensor([-0.3202,  1.8125])\n",
      "------\n",
      "------\n",
      "Epoch 294, Loss 12.865259, \n",
      "Params:  tensor([ 3.4893, -6.6712])\n",
      "Grad:  tensor([-0.3196,  1.8095])\n",
      "------\n",
      "------\n",
      "Epoch 295, Loss 12.831525, \n",
      "Params:  tensor([ 3.4925, -6.6893])\n",
      "Grad:  tensor([-0.3191,  1.8064])\n",
      "------\n",
      "------\n",
      "Epoch 296, Loss 12.797904, \n",
      "Params:  tensor([ 3.4956, -6.7073])\n",
      "Grad:  tensor([-0.3186,  1.8033])\n",
      "------\n",
      "------\n",
      "Epoch 297, Loss 12.764399, \n",
      "Params:  tensor([ 3.4988, -6.7253])\n",
      "Grad:  tensor([-0.3180,  1.8003])\n",
      "------\n",
      "------\n",
      "Epoch 298, Loss 12.731007, \n",
      "Params:  tensor([ 3.5020, -6.7433])\n",
      "Grad:  tensor([-0.3175,  1.7972])\n",
      "------\n",
      "------\n",
      "Epoch 299, Loss 12.697727, \n",
      "Params:  tensor([ 3.5052, -6.7612])\n",
      "Grad:  tensor([-0.3169,  1.7941])\n",
      "------\n",
      "------\n",
      "Epoch 300, Loss 12.664559, \n",
      "Params:  tensor([ 3.5083, -6.7792])\n",
      "Grad:  tensor([-0.3164,  1.7911])\n",
      "------\n",
      "------\n",
      "Epoch 301, Loss 12.631507, \n",
      "Params:  tensor([ 3.5115, -6.7970])\n",
      "Grad:  tensor([-0.3159,  1.7881])\n",
      "------\n",
      "------\n",
      "Epoch 302, Loss 12.598568, \n",
      "Params:  tensor([ 3.5146, -6.8149])\n",
      "Grad:  tensor([-0.3153,  1.7850])\n",
      "------\n",
      "------\n",
      "Epoch 303, Loss 12.565738, \n",
      "Params:  tensor([ 3.5178, -6.8327])\n",
      "Grad:  tensor([-0.3148,  1.7820])\n",
      "------\n",
      "------\n",
      "Epoch 304, Loss 12.533021, \n",
      "Params:  tensor([ 3.5209, -6.8505])\n",
      "Grad:  tensor([-0.3143,  1.7790])\n",
      "------\n",
      "------\n",
      "Epoch 305, Loss 12.500413, \n",
      "Params:  tensor([ 3.5241, -6.8683])\n",
      "Grad:  tensor([-0.3137,  1.7759])\n",
      "------\n",
      "------\n",
      "Epoch 306, Loss 12.467919, \n",
      "Params:  tensor([ 3.5272, -6.8860])\n",
      "Grad:  tensor([-0.3132,  1.7729])\n",
      "------\n",
      "------\n",
      "Epoch 307, Loss 12.435532, \n",
      "Params:  tensor([ 3.5303, -6.9037])\n",
      "Grad:  tensor([-0.3127,  1.7699])\n",
      "------\n",
      "------\n",
      "Epoch 308, Loss 12.403256, \n",
      "Params:  tensor([ 3.5335, -6.9213])\n",
      "Grad:  tensor([-0.3121,  1.7669])\n",
      "------\n",
      "------\n",
      "Epoch 309, Loss 12.371090, \n",
      "Params:  tensor([ 3.5366, -6.9390])\n",
      "Grad:  tensor([-0.3116,  1.7639])\n",
      "------\n",
      "------\n",
      "Epoch 310, Loss 12.339031, \n",
      "Params:  tensor([ 3.5397, -6.9566])\n",
      "Grad:  tensor([-0.3111,  1.7609])\n",
      "------\n",
      "------\n",
      "Epoch 311, Loss 12.307082, \n",
      "Params:  tensor([ 3.5428, -6.9742])\n",
      "Grad:  tensor([-0.3105,  1.7579])\n",
      "------\n",
      "------\n",
      "Epoch 312, Loss 12.275247, \n",
      "Params:  tensor([ 3.5459, -6.9917])\n",
      "Grad:  tensor([-0.3100,  1.7549])\n",
      "------\n",
      "------\n",
      "Epoch 313, Loss 12.243509, \n",
      "Params:  tensor([ 3.5490, -7.0092])\n",
      "Grad:  tensor([-0.3095,  1.7519])\n",
      "------\n",
      "------\n",
      "Epoch 314, Loss 12.211887, \n",
      "Params:  tensor([ 3.5521, -7.0267])\n",
      "Grad:  tensor([-0.3090,  1.7490])\n",
      "------\n",
      "------\n",
      "Epoch 315, Loss 12.180370, \n",
      "Params:  tensor([ 3.5552, -7.0442])\n",
      "Grad:  tensor([-0.3084,  1.7460])\n",
      "------\n",
      "------\n",
      "Epoch 316, Loss 12.148962, \n",
      "Params:  tensor([ 3.5582, -7.0616])\n",
      "Grad:  tensor([-0.3079,  1.7430])\n",
      "------\n",
      "------\n",
      "Epoch 317, Loss 12.117657, \n",
      "Params:  tensor([ 3.5613, -7.0790])\n",
      "Grad:  tensor([-0.3074,  1.7401])\n",
      "------\n",
      "------\n",
      "Epoch 318, Loss 12.086462, \n",
      "Params:  tensor([ 3.5644, -7.0964])\n",
      "Grad:  tensor([-0.3069,  1.7371])\n",
      "------\n",
      "------\n",
      "Epoch 319, Loss 12.055373, \n",
      "Params:  tensor([ 3.5674, -7.1137])\n",
      "Grad:  tensor([-0.3063,  1.7342])\n",
      "------\n",
      "------\n",
      "Epoch 320, Loss 12.024384, \n",
      "Params:  tensor([ 3.5705, -7.1310])\n",
      "Grad:  tensor([-0.3058,  1.7312])\n",
      "------\n",
      "------\n",
      "Epoch 321, Loss 11.993508, \n",
      "Params:  tensor([ 3.5736, -7.1483])\n",
      "Grad:  tensor([-0.3053,  1.7283])\n",
      "------\n",
      "------\n",
      "Epoch 322, Loss 11.962731, \n",
      "Params:  tensor([ 3.5766, -7.1656])\n",
      "Grad:  tensor([-0.3048,  1.7253])\n",
      "------\n",
      "------\n",
      "Epoch 323, Loss 11.932056, \n",
      "Params:  tensor([ 3.5796, -7.1828])\n",
      "Grad:  tensor([-0.3043,  1.7224])\n",
      "------\n",
      "------\n",
      "Epoch 324, Loss 11.901492, \n",
      "Params:  tensor([ 3.5827, -7.2000])\n",
      "Grad:  tensor([-0.3037,  1.7195])\n",
      "------\n",
      "------\n",
      "Epoch 325, Loss 11.871029, \n",
      "Params:  tensor([ 3.5857, -7.2172])\n",
      "Grad:  tensor([-0.3032,  1.7166])\n",
      "------\n",
      "------\n",
      "Epoch 326, Loss 11.840671, \n",
      "Params:  tensor([ 3.5887, -7.2343])\n",
      "Grad:  tensor([-0.3027,  1.7136])\n",
      "------\n",
      "------\n",
      "Epoch 327, Loss 11.810413, \n",
      "Params:  tensor([ 3.5918, -7.2514])\n",
      "Grad:  tensor([-0.3022,  1.7107])\n",
      "------\n",
      "------\n",
      "Epoch 328, Loss 11.780257, \n",
      "Params:  tensor([ 3.5948, -7.2685])\n",
      "Grad:  tensor([-0.3017,  1.7078])\n",
      "------\n",
      "------\n",
      "Epoch 329, Loss 11.750208, \n",
      "Params:  tensor([ 3.5978, -7.2855])\n",
      "Grad:  tensor([-0.3012,  1.7049])\n",
      "------\n",
      "------\n",
      "Epoch 330, Loss 11.720258, \n",
      "Params:  tensor([ 3.6008, -7.3026])\n",
      "Grad:  tensor([-0.3007,  1.7020])\n",
      "------\n",
      "------\n",
      "Epoch 331, Loss 11.690412, \n",
      "Params:  tensor([ 3.6038, -7.3196])\n",
      "Grad:  tensor([-0.3002,  1.6991])\n",
      "------\n",
      "------\n",
      "Epoch 332, Loss 11.660664, \n",
      "Params:  tensor([ 3.6068, -7.3365])\n",
      "Grad:  tensor([-0.2996,  1.6963])\n",
      "------\n",
      "------\n",
      "Epoch 333, Loss 11.631015, \n",
      "Params:  tensor([ 3.6098, -7.3535])\n",
      "Grad:  tensor([-0.2991,  1.6934])\n",
      "------\n",
      "------\n",
      "Epoch 334, Loss 11.601473, \n",
      "Params:  tensor([ 3.6128, -7.3704])\n",
      "Grad:  tensor([-0.2986,  1.6905])\n",
      "------\n",
      "------\n",
      "Epoch 335, Loss 11.572030, \n",
      "Params:  tensor([ 3.6158, -7.3872])\n",
      "Grad:  tensor([-0.2981,  1.6876])\n",
      "------\n",
      "------\n",
      "Epoch 336, Loss 11.542686, \n",
      "Params:  tensor([ 3.6187, -7.4041])\n",
      "Grad:  tensor([-0.2976,  1.6848])\n",
      "------\n",
      "------\n",
      "Epoch 337, Loss 11.513440, \n",
      "Params:  tensor([ 3.6217, -7.4209])\n",
      "Grad:  tensor([-0.2971,  1.6819])\n",
      "------\n",
      "------\n",
      "Epoch 338, Loss 11.484293, \n",
      "Params:  tensor([ 3.6247, -7.4377])\n",
      "Grad:  tensor([-0.2966,  1.6790])\n",
      "------\n",
      "------\n",
      "Epoch 339, Loss 11.455246, \n",
      "Params:  tensor([ 3.6276, -7.4545])\n",
      "Grad:  tensor([-0.2961,  1.6762])\n",
      "------\n",
      "------\n",
      "Epoch 340, Loss 11.426300, \n",
      "Params:  tensor([ 3.6306, -7.4712])\n",
      "Grad:  tensor([-0.2956,  1.6733])\n",
      "------\n",
      "------\n",
      "Epoch 341, Loss 11.397448, \n",
      "Params:  tensor([ 3.6335, -7.4879])\n",
      "Grad:  tensor([-0.2951,  1.6705])\n",
      "------\n",
      "------\n",
      "Epoch 342, Loss 11.368696, \n",
      "Params:  tensor([ 3.6365, -7.5046])\n",
      "Grad:  tensor([-0.2946,  1.6677])\n",
      "------\n",
      "------\n",
      "Epoch 343, Loss 11.340043, \n",
      "Params:  tensor([ 3.6394, -7.5212])\n",
      "Grad:  tensor([-0.2941,  1.6648])\n",
      "------\n",
      "------\n",
      "Epoch 344, Loss 11.311487, \n",
      "Params:  tensor([ 3.6424, -7.5378])\n",
      "Grad:  tensor([-0.2936,  1.6620])\n",
      "------\n",
      "------\n",
      "Epoch 345, Loss 11.283028, \n",
      "Params:  tensor([ 3.6453, -7.5544])\n",
      "Grad:  tensor([-0.2931,  1.6592])\n",
      "------\n",
      "------\n",
      "Epoch 346, Loss 11.254662, \n",
      "Params:  tensor([ 3.6482, -7.5710])\n",
      "Grad:  tensor([-0.2926,  1.6564])\n",
      "------\n",
      "------\n",
      "Epoch 347, Loss 11.226396, \n",
      "Params:  tensor([ 3.6511, -7.5875])\n",
      "Grad:  tensor([-0.2921,  1.6535])\n",
      "------\n",
      "------\n",
      "Epoch 348, Loss 11.198220, \n",
      "Params:  tensor([ 3.6541, -7.6040])\n",
      "Grad:  tensor([-0.2916,  1.6507])\n",
      "------\n",
      "------\n",
      "Epoch 349, Loss 11.170150, \n",
      "Params:  tensor([ 3.6570, -7.6205])\n",
      "Grad:  tensor([-0.2911,  1.6479])\n",
      "------\n",
      "------\n",
      "Epoch 350, Loss 11.142170, \n",
      "Params:  tensor([ 3.6599, -7.6370])\n",
      "Grad:  tensor([-0.2906,  1.6451])\n",
      "------\n",
      "------\n",
      "Epoch 351, Loss 11.114282, \n",
      "Params:  tensor([ 3.6628, -7.6534])\n",
      "Grad:  tensor([-0.2901,  1.6423])\n",
      "------\n",
      "------\n",
      "Epoch 352, Loss 11.086491, \n",
      "Params:  tensor([ 3.6657, -7.6698])\n",
      "Grad:  tensor([-0.2896,  1.6395])\n",
      "------\n",
      "------\n",
      "Epoch 353, Loss 11.058797, \n",
      "Params:  tensor([ 3.6686, -7.6861])\n",
      "Grad:  tensor([-0.2892,  1.6368])\n",
      "------\n",
      "------\n",
      "Epoch 354, Loss 11.031193, \n",
      "Params:  tensor([ 3.6714, -7.7025])\n",
      "Grad:  tensor([-0.2886,  1.6340])\n",
      "------\n",
      "------\n",
      "Epoch 355, Loss 11.003686, \n",
      "Params:  tensor([ 3.6743, -7.7188])\n",
      "Grad:  tensor([-0.2882,  1.6312])\n",
      "------\n",
      "------\n",
      "Epoch 356, Loss 10.976270, \n",
      "Params:  tensor([ 3.6772, -7.7351])\n",
      "Grad:  tensor([-0.2877,  1.6284])\n",
      "------\n",
      "------\n",
      "Epoch 357, Loss 10.948948, \n",
      "Params:  tensor([ 3.6801, -7.7513])\n",
      "Grad:  tensor([-0.2872,  1.6257])\n",
      "------\n",
      "------\n",
      "Epoch 358, Loss 10.921719, \n",
      "Params:  tensor([ 3.6829, -7.7676])\n",
      "Grad:  tensor([-0.2867,  1.6229])\n",
      "------\n",
      "------\n",
      "Epoch 359, Loss 10.894581, \n",
      "Params:  tensor([ 3.6858, -7.7838])\n",
      "Grad:  tensor([-0.2862,  1.6201])\n",
      "------\n",
      "------\n",
      "Epoch 360, Loss 10.867537, \n",
      "Params:  tensor([ 3.6887, -7.7999])\n",
      "Grad:  tensor([-0.2857,  1.6174])\n",
      "------\n",
      "------\n",
      "Epoch 361, Loss 10.840583, \n",
      "Params:  tensor([ 3.6915, -7.8161])\n",
      "Grad:  tensor([-0.2852,  1.6146])\n",
      "------\n",
      "------\n",
      "Epoch 362, Loss 10.813721, \n",
      "Params:  tensor([ 3.6944, -7.8322])\n",
      "Grad:  tensor([-0.2847,  1.6119])\n",
      "------\n",
      "------\n",
      "Epoch 363, Loss 10.786950, \n",
      "Params:  tensor([ 3.6972, -7.8483])\n",
      "Grad:  tensor([-0.2843,  1.6092])\n",
      "------\n",
      "------\n",
      "Epoch 364, Loss 10.760270, \n",
      "Params:  tensor([ 3.7000, -7.8644])\n",
      "Grad:  tensor([-0.2838,  1.6064])\n",
      "------\n",
      "------\n",
      "Epoch 365, Loss 10.733681, \n",
      "Params:  tensor([ 3.7029, -7.8804])\n",
      "Grad:  tensor([-0.2833,  1.6037])\n",
      "------\n",
      "------\n",
      "Epoch 366, Loss 10.707184, \n",
      "Params:  tensor([ 3.7057, -7.8964])\n",
      "Grad:  tensor([-0.2828,  1.6010])\n",
      "------\n",
      "------\n",
      "Epoch 367, Loss 10.680775, \n",
      "Params:  tensor([ 3.7085, -7.9124])\n",
      "Grad:  tensor([-0.2823,  1.5983])\n",
      "------\n",
      "------\n",
      "Epoch 368, Loss 10.654454, \n",
      "Params:  tensor([ 3.7113, -7.9284])\n",
      "Grad:  tensor([-0.2819,  1.5955])\n",
      "------\n",
      "------\n",
      "Epoch 369, Loss 10.628225, \n",
      "Params:  tensor([ 3.7142, -7.9443])\n",
      "Grad:  tensor([-0.2814,  1.5928])\n",
      "------\n",
      "------\n",
      "Epoch 370, Loss 10.602086, \n",
      "Params:  tensor([ 3.7170, -7.9602])\n",
      "Grad:  tensor([-0.2809,  1.5901])\n",
      "------\n",
      "------\n",
      "Epoch 371, Loss 10.576034, \n",
      "Params:  tensor([ 3.7198, -7.9761])\n",
      "Grad:  tensor([-0.2804,  1.5874])\n",
      "------\n",
      "------\n",
      "Epoch 372, Loss 10.550071, \n",
      "Params:  tensor([ 3.7226, -7.9919])\n",
      "Grad:  tensor([-0.2799,  1.5847])\n",
      "------\n",
      "------\n",
      "Epoch 373, Loss 10.524194, \n",
      "Params:  tensor([ 3.7254, -8.0077])\n",
      "Grad:  tensor([-0.2795,  1.5820])\n",
      "------\n",
      "------\n",
      "Epoch 374, Loss 10.498409, \n",
      "Params:  tensor([ 3.7282, -8.0235])\n",
      "Grad:  tensor([-0.2790,  1.5794])\n",
      "------\n",
      "------\n",
      "Epoch 375, Loss 10.472707, \n",
      "Params:  tensor([ 3.7309, -8.0393])\n",
      "Grad:  tensor([-0.2785,  1.5767])\n",
      "------\n",
      "------\n",
      "Epoch 376, Loss 10.447093, \n",
      "Params:  tensor([ 3.7337, -8.0550])\n",
      "Grad:  tensor([-0.2780,  1.5740])\n",
      "------\n",
      "------\n",
      "Epoch 377, Loss 10.421569, \n",
      "Params:  tensor([ 3.7365, -8.0707])\n",
      "Grad:  tensor([-0.2776,  1.5713])\n",
      "------\n",
      "------\n",
      "Epoch 378, Loss 10.396132, \n",
      "Params:  tensor([ 3.7393, -8.0864])\n",
      "Grad:  tensor([-0.2771,  1.5686])\n",
      "------\n",
      "------\n",
      "Epoch 379, Loss 10.370779, \n",
      "Params:  tensor([ 3.7420, -8.1021])\n",
      "Grad:  tensor([-0.2766,  1.5660])\n",
      "------\n",
      "------\n",
      "Epoch 380, Loss 10.345510, \n",
      "Params:  tensor([ 3.7448, -8.1177])\n",
      "Grad:  tensor([-0.2762,  1.5633])\n",
      "------\n",
      "------\n",
      "Epoch 381, Loss 10.320328, \n",
      "Params:  tensor([ 3.7476, -8.1333])\n",
      "Grad:  tensor([-0.2757,  1.5607])\n",
      "------\n",
      "------\n",
      "Epoch 382, Loss 10.295234, \n",
      "Params:  tensor([ 3.7503, -8.1489])\n",
      "Grad:  tensor([-0.2752,  1.5580])\n",
      "------\n",
      "------\n",
      "Epoch 383, Loss 10.270224, \n",
      "Params:  tensor([ 3.7531, -8.1645])\n",
      "Grad:  tensor([-0.2748,  1.5554])\n",
      "------\n",
      "------\n",
      "Epoch 384, Loss 10.245296, \n",
      "Params:  tensor([ 3.7558, -8.1800])\n",
      "Grad:  tensor([-0.2743,  1.5527])\n",
      "------\n",
      "------\n",
      "Epoch 385, Loss 10.220457, \n",
      "Params:  tensor([ 3.7585, -8.1955])\n",
      "Grad:  tensor([-0.2738,  1.5501])\n",
      "------\n",
      "------\n",
      "Epoch 386, Loss 10.195701, \n",
      "Params:  tensor([ 3.7613, -8.2110])\n",
      "Grad:  tensor([-0.2734,  1.5475])\n",
      "------\n",
      "------\n",
      "Epoch 387, Loss 10.171029, \n",
      "Params:  tensor([ 3.7640, -8.2264])\n",
      "Grad:  tensor([-0.2729,  1.5448])\n",
      "------\n",
      "------\n",
      "Epoch 388, Loss 10.146438, \n",
      "Params:  tensor([ 3.7667, -8.2418])\n",
      "Grad:  tensor([-0.2724,  1.5422])\n",
      "------\n",
      "------\n",
      "Epoch 389, Loss 10.121935, \n",
      "Params:  tensor([ 3.7694, -8.2572])\n",
      "Grad:  tensor([-0.2720,  1.5396])\n",
      "------\n",
      "------\n",
      "Epoch 390, Loss 10.097512, \n",
      "Params:  tensor([ 3.7722, -8.2726])\n",
      "Grad:  tensor([-0.2715,  1.5370])\n",
      "------\n",
      "------\n",
      "Epoch 391, Loss 10.073173, \n",
      "Params:  tensor([ 3.7749, -8.2879])\n",
      "Grad:  tensor([-0.2711,  1.5344])\n",
      "------\n",
      "------\n",
      "Epoch 392, Loss 10.048919, \n",
      "Params:  tensor([ 3.7776, -8.3033])\n",
      "Grad:  tensor([-0.2706,  1.5317])\n",
      "------\n",
      "------\n",
      "Epoch 393, Loss 10.024743, \n",
      "Params:  tensor([ 3.7803, -8.3185])\n",
      "Grad:  tensor([-0.2701,  1.5291])\n",
      "------\n",
      "------\n",
      "Epoch 394, Loss 10.000652, \n",
      "Params:  tensor([ 3.7830, -8.3338])\n",
      "Grad:  tensor([-0.2697,  1.5265])\n",
      "------\n",
      "------\n",
      "Epoch 395, Loss 9.976640, \n",
      "Params:  tensor([ 3.7857, -8.3491])\n",
      "Grad:  tensor([-0.2692,  1.5240])\n",
      "------\n",
      "------\n",
      "Epoch 396, Loss 9.952712, \n",
      "Params:  tensor([ 3.7884, -8.3643])\n",
      "Grad:  tensor([-0.2688,  1.5214])\n",
      "------\n",
      "------\n",
      "Epoch 397, Loss 9.928862, \n",
      "Params:  tensor([ 3.7910, -8.3795])\n",
      "Grad:  tensor([-0.2683,  1.5188])\n",
      "------\n",
      "------\n",
      "Epoch 398, Loss 9.905093, \n",
      "Params:  tensor([ 3.7937, -8.3946])\n",
      "Grad:  tensor([-0.2678,  1.5162])\n",
      "------\n",
      "------\n",
      "Epoch 399, Loss 9.881409, \n",
      "Params:  tensor([ 3.7964, -8.4098])\n",
      "Grad:  tensor([-0.2674,  1.5136])\n",
      "------\n",
      "------\n",
      "Epoch 400, Loss 9.857804, \n",
      "Params:  tensor([ 3.7991, -8.4249])\n",
      "Grad:  tensor([-0.2669,  1.5111])\n",
      "------\n",
      "------\n",
      "Epoch 401, Loss 9.834277, \n",
      "Params:  tensor([ 3.8017, -8.4399])\n",
      "Grad:  tensor([-0.2665,  1.5085])\n",
      "------\n",
      "------\n",
      "Epoch 402, Loss 9.810831, \n",
      "Params:  tensor([ 3.8044, -8.4550])\n",
      "Grad:  tensor([-0.2660,  1.5059])\n",
      "------\n",
      "------\n",
      "Epoch 403, Loss 9.787466, \n",
      "Params:  tensor([ 3.8070, -8.4700])\n",
      "Grad:  tensor([-0.2656,  1.5034])\n",
      "------\n",
      "------\n",
      "Epoch 404, Loss 9.764176, \n",
      "Params:  tensor([ 3.8097, -8.4851])\n",
      "Grad:  tensor([-0.2651,  1.5008])\n",
      "------\n",
      "------\n",
      "Epoch 405, Loss 9.740973, \n",
      "Params:  tensor([ 3.8123, -8.5000])\n",
      "Grad:  tensor([-0.2647,  1.4983])\n",
      "------\n",
      "------\n",
      "Epoch 406, Loss 9.717843, \n",
      "Params:  tensor([ 3.8150, -8.5150])\n",
      "Grad:  tensor([-0.2642,  1.4957])\n",
      "------\n",
      "------\n",
      "Epoch 407, Loss 9.694793, \n",
      "Params:  tensor([ 3.8176, -8.5299])\n",
      "Grad:  tensor([-0.2638,  1.4932])\n",
      "------\n",
      "------\n",
      "Epoch 408, Loss 9.671824, \n",
      "Params:  tensor([ 3.8202, -8.5448])\n",
      "Grad:  tensor([-0.2633,  1.4906])\n",
      "------\n",
      "------\n",
      "Epoch 409, Loss 9.648926, \n",
      "Params:  tensor([ 3.8229, -8.5597])\n",
      "Grad:  tensor([-0.2629,  1.4881])\n",
      "------\n",
      "------\n",
      "Epoch 410, Loss 9.626110, \n",
      "Params:  tensor([ 3.8255, -8.5746])\n",
      "Grad:  tensor([-0.2624,  1.4856])\n",
      "------\n",
      "------\n",
      "Epoch 411, Loss 9.603373, \n",
      "Params:  tensor([ 3.8281, -8.5894])\n",
      "Grad:  tensor([-0.2620,  1.4831])\n",
      "------\n",
      "------\n",
      "Epoch 412, Loss 9.580709, \n",
      "Params:  tensor([ 3.8307, -8.6042])\n",
      "Grad:  tensor([-0.2615,  1.4805])\n",
      "------\n",
      "------\n",
      "Epoch 413, Loss 9.558125, \n",
      "Params:  tensor([ 3.8333, -8.6190])\n",
      "Grad:  tensor([-0.2611,  1.4780])\n",
      "------\n",
      "------\n",
      "Epoch 414, Loss 9.535617, \n",
      "Params:  tensor([ 3.8360, -8.6337])\n",
      "Grad:  tensor([-0.2606,  1.4755])\n",
      "------\n",
      "------\n",
      "Epoch 415, Loss 9.513184, \n",
      "Params:  tensor([ 3.8386, -8.6485])\n",
      "Grad:  tensor([-0.2602,  1.4730])\n",
      "------\n",
      "------\n",
      "Epoch 416, Loss 9.490829, \n",
      "Params:  tensor([ 3.8412, -8.6632])\n",
      "Grad:  tensor([-0.2598,  1.4705])\n",
      "------\n",
      "------\n",
      "Epoch 417, Loss 9.468551, \n",
      "Params:  tensor([ 3.8437, -8.6779])\n",
      "Grad:  tensor([-0.2593,  1.4680])\n",
      "------\n",
      "------\n",
      "Epoch 418, Loss 9.446347, \n",
      "Params:  tensor([ 3.8463, -8.6925])\n",
      "Grad:  tensor([-0.2589,  1.4655])\n",
      "------\n",
      "------\n",
      "Epoch 419, Loss 9.424216, \n",
      "Params:  tensor([ 3.8489, -8.7071])\n",
      "Grad:  tensor([-0.2584,  1.4630])\n",
      "------\n",
      "------\n",
      "Epoch 420, Loss 9.402164, \n",
      "Params:  tensor([ 3.8515, -8.7217])\n",
      "Grad:  tensor([-0.2580,  1.4605])\n",
      "------\n",
      "------\n",
      "Epoch 421, Loss 9.380184, \n",
      "Params:  tensor([ 3.8541, -8.7363])\n",
      "Grad:  tensor([-0.2576,  1.4581])\n",
      "------\n",
      "------\n",
      "Epoch 422, Loss 9.358282, \n",
      "Params:  tensor([ 3.8566, -8.7509])\n",
      "Grad:  tensor([-0.2571,  1.4556])\n",
      "------\n",
      "------\n",
      "Epoch 423, Loss 9.336448, \n",
      "Params:  tensor([ 3.8592, -8.7654])\n",
      "Grad:  tensor([-0.2567,  1.4531])\n",
      "------\n",
      "------\n",
      "Epoch 424, Loss 9.314695, \n",
      "Params:  tensor([ 3.8618, -8.7799])\n",
      "Grad:  tensor([-0.2563,  1.4506])\n",
      "------\n",
      "------\n",
      "Epoch 425, Loss 9.293012, \n",
      "Params:  tensor([ 3.8643, -8.7944])\n",
      "Grad:  tensor([-0.2558,  1.4482])\n",
      "------\n",
      "------\n",
      "Epoch 426, Loss 9.271403, \n",
      "Params:  tensor([ 3.8669, -8.8089])\n",
      "Grad:  tensor([-0.2554,  1.4457])\n",
      "------\n",
      "------\n",
      "Epoch 427, Loss 9.249871, \n",
      "Params:  tensor([ 3.8694, -8.8233])\n",
      "Grad:  tensor([-0.2550,  1.4433])\n",
      "------\n",
      "------\n",
      "Epoch 428, Loss 9.228410, \n",
      "Params:  tensor([ 3.8720, -8.8377])\n",
      "Grad:  tensor([-0.2545,  1.4408])\n",
      "------\n",
      "------\n",
      "Epoch 429, Loss 9.207022, \n",
      "Params:  tensor([ 3.8745, -8.8521])\n",
      "Grad:  tensor([-0.2541,  1.4384])\n",
      "------\n",
      "------\n",
      "Epoch 430, Loss 9.185704, \n",
      "Params:  tensor([ 3.8771, -8.8664])\n",
      "Grad:  tensor([-0.2537,  1.4359])\n",
      "------\n",
      "------\n",
      "Epoch 431, Loss 9.164462, \n",
      "Params:  tensor([ 3.8796, -8.8808])\n",
      "Grad:  tensor([-0.2532,  1.4335])\n",
      "------\n",
      "------\n",
      "Epoch 432, Loss 9.143289, \n",
      "Params:  tensor([ 3.8821, -8.8951])\n",
      "Grad:  tensor([-0.2528,  1.4310])\n",
      "------\n",
      "------\n",
      "Epoch 433, Loss 9.122189, \n",
      "Params:  tensor([ 3.8846, -8.9094])\n",
      "Grad:  tensor([-0.2524,  1.4286])\n",
      "------\n",
      "------\n",
      "Epoch 434, Loss 9.101160, \n",
      "Params:  tensor([ 3.8872, -8.9236])\n",
      "Grad:  tensor([-0.2519,  1.4262])\n",
      "------\n",
      "------\n",
      "Epoch 435, Loss 9.080204, \n",
      "Params:  tensor([ 3.8897, -8.9379])\n",
      "Grad:  tensor([-0.2515,  1.4238])\n",
      "------\n",
      "------\n",
      "Epoch 436, Loss 9.059318, \n",
      "Params:  tensor([ 3.8922, -8.9521])\n",
      "Grad:  tensor([-0.2511,  1.4213])\n",
      "------\n",
      "------\n",
      "Epoch 437, Loss 9.038502, \n",
      "Params:  tensor([ 3.8947, -8.9663])\n",
      "Grad:  tensor([-0.2507,  1.4189])\n",
      "------\n",
      "------\n",
      "Epoch 438, Loss 9.017757, \n",
      "Params:  tensor([ 3.8972, -8.9804])\n",
      "Grad:  tensor([-0.2502,  1.4165])\n",
      "------\n",
      "------\n",
      "Epoch 439, Loss 8.997084, \n",
      "Params:  tensor([ 3.8997, -8.9946])\n",
      "Grad:  tensor([-0.2498,  1.4141])\n",
      "------\n",
      "------\n",
      "Epoch 440, Loss 8.976479, \n",
      "Params:  tensor([ 3.9022, -9.0087])\n",
      "Grad:  tensor([-0.2494,  1.4117])\n",
      "------\n",
      "------\n",
      "Epoch 441, Loss 8.955944, \n",
      "Params:  tensor([ 3.9047, -9.0228])\n",
      "Grad:  tensor([-0.2489,  1.4093])\n",
      "------\n",
      "------\n",
      "Epoch 442, Loss 8.935480, \n",
      "Params:  tensor([ 3.9072, -9.0369])\n",
      "Grad:  tensor([-0.2485,  1.4069])\n",
      "------\n",
      "------\n",
      "Epoch 443, Loss 8.915089, \n",
      "Params:  tensor([ 3.9096, -9.0509])\n",
      "Grad:  tensor([-0.2481,  1.4045])\n",
      "------\n",
      "------\n",
      "Epoch 444, Loss 8.894762, \n",
      "Params:  tensor([ 3.9121, -9.0649])\n",
      "Grad:  tensor([-0.2477,  1.4021])\n",
      "------\n",
      "------\n",
      "Epoch 445, Loss 8.874508, \n",
      "Params:  tensor([ 3.9146, -9.0789])\n",
      "Grad:  tensor([-0.2473,  1.3998])\n",
      "------\n",
      "------\n",
      "Epoch 446, Loss 8.854318, \n",
      "Params:  tensor([ 3.9171, -9.0929])\n",
      "Grad:  tensor([-0.2468,  1.3974])\n",
      "------\n",
      "------\n",
      "Epoch 447, Loss 8.834197, \n",
      "Params:  tensor([ 3.9195, -9.1068])\n",
      "Grad:  tensor([-0.2464,  1.3950])\n",
      "------\n",
      "------\n",
      "Epoch 448, Loss 8.814149, \n",
      "Params:  tensor([ 3.9220, -9.1208])\n",
      "Grad:  tensor([-0.2460,  1.3926])\n",
      "------\n",
      "------\n",
      "Epoch 449, Loss 8.794162, \n",
      "Params:  tensor([ 3.9244, -9.1347])\n",
      "Grad:  tensor([-0.2456,  1.3903])\n",
      "------\n",
      "------\n",
      "Epoch 450, Loss 8.774253, \n",
      "Params:  tensor([ 3.9269, -9.1486])\n",
      "Grad:  tensor([-0.2452,  1.3879])\n",
      "------\n",
      "------\n",
      "Epoch 451, Loss 8.754405, \n",
      "Params:  tensor([ 3.9293, -9.1624])\n",
      "Grad:  tensor([-0.2448,  1.3856])\n",
      "------\n",
      "------\n",
      "Epoch 452, Loss 8.734623, \n",
      "Params:  tensor([ 3.9318, -9.1762])\n",
      "Grad:  tensor([-0.2443,  1.3832])\n",
      "------\n",
      "------\n",
      "Epoch 453, Loss 8.714911, \n",
      "Params:  tensor([ 3.9342, -9.1901])\n",
      "Grad:  tensor([-0.2439,  1.3808])\n",
      "------\n",
      "------\n",
      "Epoch 454, Loss 8.695266, \n",
      "Params:  tensor([ 3.9367, -9.2038])\n",
      "Grad:  tensor([-0.2435,  1.3785])\n",
      "------\n",
      "------\n",
      "Epoch 455, Loss 8.675688, \n",
      "Params:  tensor([ 3.9391, -9.2176])\n",
      "Grad:  tensor([-0.2431,  1.3762])\n",
      "------\n",
      "------\n",
      "Epoch 456, Loss 8.656173, \n",
      "Params:  tensor([ 3.9415, -9.2313])\n",
      "Grad:  tensor([-0.2427,  1.3738])\n",
      "------\n",
      "------\n",
      "Epoch 457, Loss 8.636729, \n",
      "Params:  tensor([ 3.9439, -9.2451])\n",
      "Grad:  tensor([-0.2423,  1.3715])\n",
      "------\n",
      "------\n",
      "Epoch 458, Loss 8.617347, \n",
      "Params:  tensor([ 3.9464, -9.2587])\n",
      "Grad:  tensor([-0.2419,  1.3692])\n",
      "------\n",
      "------\n",
      "Epoch 459, Loss 8.598029, \n",
      "Params:  tensor([ 3.9488, -9.2724])\n",
      "Grad:  tensor([-0.2414,  1.3668])\n",
      "------\n",
      "------\n",
      "Epoch 460, Loss 8.578781, \n",
      "Params:  tensor([ 3.9512, -9.2861])\n",
      "Grad:  tensor([-0.2410,  1.3645])\n",
      "------\n",
      "------\n",
      "Epoch 461, Loss 8.559597, \n",
      "Params:  tensor([ 3.9536, -9.2997])\n",
      "Grad:  tensor([-0.2406,  1.3622])\n",
      "------\n",
      "------\n",
      "Epoch 462, Loss 8.540479, \n",
      "Params:  tensor([ 3.9560, -9.3133])\n",
      "Grad:  tensor([-0.2402,  1.3599])\n",
      "------\n",
      "------\n",
      "Epoch 463, Loss 8.521426, \n",
      "Params:  tensor([ 3.9584, -9.3269])\n",
      "Grad:  tensor([-0.2398,  1.3576])\n",
      "------\n",
      "------\n",
      "Epoch 464, Loss 8.502437, \n",
      "Params:  tensor([ 3.9608, -9.3404])\n",
      "Grad:  tensor([-0.2394,  1.3553])\n",
      "------\n",
      "------\n",
      "Epoch 465, Loss 8.483517, \n",
      "Params:  tensor([ 3.9632, -9.3539])\n",
      "Grad:  tensor([-0.2390,  1.3530])\n",
      "------\n",
      "------\n",
      "Epoch 466, Loss 8.464652, \n",
      "Params:  tensor([ 3.9656, -9.3674])\n",
      "Grad:  tensor([-0.2386,  1.3507])\n",
      "------\n",
      "------\n",
      "Epoch 467, Loss 8.445858, \n",
      "Params:  tensor([ 3.9679, -9.3809])\n",
      "Grad:  tensor([-0.2382,  1.3484])\n",
      "------\n",
      "------\n",
      "Epoch 468, Loss 8.427128, \n",
      "Params:  tensor([ 3.9703, -9.3944])\n",
      "Grad:  tensor([-0.2378,  1.3461])\n",
      "------\n",
      "------\n",
      "Epoch 469, Loss 8.408454, \n",
      "Params:  tensor([ 3.9727, -9.4078])\n",
      "Grad:  tensor([-0.2374,  1.3438])\n",
      "------\n",
      "------\n",
      "Epoch 470, Loss 8.389848, \n",
      "Params:  tensor([ 3.9751, -9.4212])\n",
      "Grad:  tensor([-0.2370,  1.3415])\n",
      "------\n",
      "------\n",
      "Epoch 471, Loss 8.371305, \n",
      "Params:  tensor([ 3.9774, -9.4346])\n",
      "Grad:  tensor([-0.2366,  1.3392])\n",
      "------\n",
      "------\n",
      "Epoch 472, Loss 8.352828, \n",
      "Params:  tensor([ 3.9798, -9.4480])\n",
      "Grad:  tensor([-0.2362,  1.3370])\n",
      "------\n",
      "------\n",
      "Epoch 473, Loss 8.334409, \n",
      "Params:  tensor([ 3.9822, -9.4614])\n",
      "Grad:  tensor([-0.2358,  1.3347])\n",
      "------\n",
      "------\n",
      "Epoch 474, Loss 8.316054, \n",
      "Params:  tensor([ 3.9845, -9.4747])\n",
      "Grad:  tensor([-0.2354,  1.3324])\n",
      "------\n",
      "------\n",
      "Epoch 475, Loss 8.297764, \n",
      "Params:  tensor([ 3.9869, -9.4880])\n",
      "Grad:  tensor([-0.2350,  1.3301])\n",
      "------\n",
      "------\n",
      "Epoch 476, Loss 8.279534, \n",
      "Params:  tensor([ 3.9892, -9.5013])\n",
      "Grad:  tensor([-0.2346,  1.3279])\n",
      "------\n",
      "------\n",
      "Epoch 477, Loss 8.261369, \n",
      "Params:  tensor([ 3.9915, -9.5145])\n",
      "Grad:  tensor([-0.2342,  1.3256])\n",
      "------\n",
      "------\n",
      "Epoch 478, Loss 8.243259, \n",
      "Params:  tensor([ 3.9939, -9.5277])\n",
      "Grad:  tensor([-0.2338,  1.3234])\n",
      "------\n",
      "------\n",
      "Epoch 479, Loss 8.225213, \n",
      "Params:  tensor([ 3.9962, -9.5410])\n",
      "Grad:  tensor([-0.2334,  1.3211])\n",
      "------\n",
      "------\n",
      "Epoch 480, Loss 8.207231, \n",
      "Params:  tensor([ 3.9985, -9.5541])\n",
      "Grad:  tensor([-0.2330,  1.3189])\n",
      "------\n",
      "------\n",
      "Epoch 481, Loss 8.189310, \n",
      "Params:  tensor([ 4.0009, -9.5673])\n",
      "Grad:  tensor([-0.2326,  1.3166])\n",
      "------\n",
      "------\n",
      "Epoch 482, Loss 8.171452, \n",
      "Params:  tensor([ 4.0032, -9.5805])\n",
      "Grad:  tensor([-0.2322,  1.3144])\n",
      "------\n",
      "------\n",
      "Epoch 483, Loss 8.153647, \n",
      "Params:  tensor([ 4.0055, -9.5936])\n",
      "Grad:  tensor([-0.2318,  1.3122])\n",
      "------\n",
      "------\n",
      "Epoch 484, Loss 8.135906, \n",
      "Params:  tensor([ 4.0078, -9.6067])\n",
      "Grad:  tensor([-0.2314,  1.3100])\n",
      "------\n",
      "------\n",
      "Epoch 485, Loss 8.118226, \n",
      "Params:  tensor([ 4.0101, -9.6198])\n",
      "Grad:  tensor([-0.2310,  1.3077])\n",
      "------\n",
      "------\n",
      "Epoch 486, Loss 8.100607, \n",
      "Params:  tensor([ 4.0124, -9.6328])\n",
      "Grad:  tensor([-0.2306,  1.3055])\n",
      "------\n",
      "------\n",
      "Epoch 487, Loss 8.083045, \n",
      "Params:  tensor([ 4.0147, -9.6458])\n",
      "Grad:  tensor([-0.2302,  1.3033])\n",
      "------\n",
      "------\n",
      "Epoch 488, Loss 8.065548, \n",
      "Params:  tensor([ 4.0170, -9.6589])\n",
      "Grad:  tensor([-0.2298,  1.3011])\n",
      "------\n",
      "------\n",
      "Epoch 489, Loss 8.048104, \n",
      "Params:  tensor([ 4.0193, -9.6718])\n",
      "Grad:  tensor([-0.2295,  1.2989])\n",
      "------\n",
      "------\n",
      "Epoch 490, Loss 8.030724, \n",
      "Params:  tensor([ 4.0216, -9.6848])\n",
      "Grad:  tensor([-0.2291,  1.2967])\n",
      "------\n",
      "------\n",
      "Epoch 491, Loss 8.013401, \n",
      "Params:  tensor([ 4.0239, -9.6978])\n",
      "Grad:  tensor([-0.2287,  1.2945])\n",
      "------\n",
      "------\n",
      "Epoch 492, Loss 7.996137, \n",
      "Params:  tensor([ 4.0262, -9.7107])\n",
      "Grad:  tensor([-0.2283,  1.2923])\n",
      "------\n",
      "------\n",
      "Epoch 493, Loss 7.978930, \n",
      "Params:  tensor([ 4.0285, -9.7236])\n",
      "Grad:  tensor([-0.2279,  1.2901])\n",
      "------\n",
      "------\n",
      "Epoch 494, Loss 7.961783, \n",
      "Params:  tensor([ 4.0308, -9.7365])\n",
      "Grad:  tensor([-0.2275,  1.2879])\n",
      "------\n",
      "------\n",
      "Epoch 495, Loss 7.944690, \n",
      "Params:  tensor([ 4.0330, -9.7493])\n",
      "Grad:  tensor([-0.2271,  1.2857])\n",
      "------\n",
      "------\n",
      "Epoch 496, Loss 7.927663, \n",
      "Params:  tensor([ 4.0353, -9.7621])\n",
      "Grad:  tensor([-0.2267,  1.2835])\n",
      "------\n",
      "------\n",
      "Epoch 497, Loss 7.910690, \n",
      "Params:  tensor([ 4.0376, -9.7750])\n",
      "Grad:  tensor([-0.2263,  1.2813])\n",
      "------\n",
      "------\n",
      "Epoch 498, Loss 7.893775, \n",
      "Params:  tensor([ 4.0398, -9.7878])\n",
      "Grad:  tensor([-0.2260,  1.2791])\n",
      "------\n",
      "------\n",
      "Epoch 499, Loss 7.876915, \n",
      "Params:  tensor([ 4.0421, -9.8005])\n",
      "Grad:  tensor([-0.2256,  1.2770])\n",
      "------\n",
      "------\n",
      "Epoch 500, Loss 7.860115, \n",
      "Params:  tensor([ 4.0443, -9.8133])\n",
      "Grad:  tensor([-0.2252,  1.2748])\n",
      "------\n",
      "------\n",
      "Epoch 501, Loss 7.843369, \n",
      "Params:  tensor([ 4.0466, -9.8260])\n",
      "Grad:  tensor([-0.2248,  1.2726])\n",
      "------\n",
      "------\n",
      "Epoch 502, Loss 7.826683, \n",
      "Params:  tensor([ 4.0488, -9.8387])\n",
      "Grad:  tensor([-0.2244,  1.2705])\n",
      "------\n",
      "------\n",
      "Epoch 503, Loss 7.810053, \n",
      "Params:  tensor([ 4.0511, -9.8514])\n",
      "Grad:  tensor([-0.2241,  1.2683])\n",
      "------\n",
      "------\n",
      "Epoch 504, Loss 7.793481, \n",
      "Params:  tensor([ 4.0533, -9.8640])\n",
      "Grad:  tensor([-0.2237,  1.2662])\n",
      "------\n",
      "------\n",
      "Epoch 505, Loss 7.776962, \n",
      "Params:  tensor([ 4.0555, -9.8767])\n",
      "Grad:  tensor([-0.2233,  1.2640])\n",
      "------\n",
      "------\n",
      "Epoch 506, Loss 7.760498, \n",
      "Params:  tensor([ 4.0578, -9.8893])\n",
      "Grad:  tensor([-0.2229,  1.2619])\n",
      "------\n",
      "------\n",
      "Epoch 507, Loss 7.744092, \n",
      "Params:  tensor([ 4.0600, -9.9019])\n",
      "Grad:  tensor([-0.2225,  1.2597])\n",
      "------\n",
      "------\n",
      "Epoch 508, Loss 7.727745, \n",
      "Params:  tensor([ 4.0622, -9.9145])\n",
      "Grad:  tensor([-0.2222,  1.2576])\n",
      "------\n",
      "------\n",
      "Epoch 509, Loss 7.711449, \n",
      "Params:  tensor([ 4.0644, -9.9270])\n",
      "Grad:  tensor([-0.2218,  1.2554])\n",
      "------\n",
      "------\n",
      "Epoch 510, Loss 7.695211, \n",
      "Params:  tensor([ 4.0666, -9.9396])\n",
      "Grad:  tensor([-0.2214,  1.2533])\n",
      "------\n",
      "------\n",
      "Epoch 511, Loss 7.679024, \n",
      "Params:  tensor([ 4.0688, -9.9521])\n",
      "Grad:  tensor([-0.2210,  1.2512])\n",
      "------\n",
      "------\n",
      "Epoch 512, Loss 7.662896, \n",
      "Params:  tensor([ 4.0710, -9.9646])\n",
      "Grad:  tensor([-0.2207,  1.2490])\n",
      "------\n",
      "------\n",
      "Epoch 513, Loss 7.646820, \n",
      "Params:  tensor([ 4.0733, -9.9770])\n",
      "Grad:  tensor([-0.2203,  1.2469])\n",
      "------\n",
      "------\n",
      "Epoch 514, Loss 7.630803, \n",
      "Params:  tensor([ 4.0754, -9.9895])\n",
      "Grad:  tensor([-0.2199,  1.2448])\n",
      "------\n",
      "------\n",
      "Epoch 515, Loss 7.614836, \n",
      "Params:  tensor([  4.0776, -10.0019])\n",
      "Grad:  tensor([-0.2195,  1.2427])\n",
      "------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Epoch 516, Loss 7.598925, \n",
      "Params:  tensor([  4.0798, -10.0143])\n",
      "Grad:  tensor([-0.2192,  1.2406])\n",
      "------\n",
      "------\n",
      "Epoch 517, Loss 7.583069, \n",
      "Params:  tensor([  4.0820, -10.0267])\n",
      "Grad:  tensor([-0.2188,  1.2385])\n",
      "------\n",
      "------\n",
      "Epoch 518, Loss 7.567265, \n",
      "Params:  tensor([  4.0842, -10.0391])\n",
      "Grad:  tensor([-0.2184,  1.2364])\n",
      "------\n",
      "------\n",
      "Epoch 519, Loss 7.551515, \n",
      "Params:  tensor([  4.0864, -10.0514])\n",
      "Grad:  tensor([-0.2180,  1.2343])\n",
      "------\n",
      "------\n",
      "Epoch 520, Loss 7.535818, \n",
      "Params:  tensor([  4.0886, -10.0637])\n",
      "Grad:  tensor([-0.2177,  1.2322])\n",
      "------\n",
      "------\n",
      "Epoch 521, Loss 7.520176, \n",
      "Params:  tensor([  4.0907, -10.0760])\n",
      "Grad:  tensor([-0.2173,  1.2301])\n",
      "------\n",
      "------\n",
      "Epoch 522, Loss 7.504587, \n",
      "Params:  tensor([  4.0929, -10.0883])\n",
      "Grad:  tensor([-0.2169,  1.2280])\n",
      "------\n",
      "------\n",
      "Epoch 523, Loss 7.489048, \n",
      "Params:  tensor([  4.0951, -10.1006])\n",
      "Grad:  tensor([-0.2165,  1.2259])\n",
      "------\n",
      "------\n",
      "Epoch 524, Loss 7.473566, \n",
      "Params:  tensor([  4.0972, -10.1128])\n",
      "Grad:  tensor([-0.2162,  1.2238])\n",
      "------\n",
      "------\n",
      "Epoch 525, Loss 7.458135, \n",
      "Params:  tensor([  4.0994, -10.1250])\n",
      "Grad:  tensor([-0.2158,  1.2217])\n",
      "------\n",
      "------\n",
      "Epoch 526, Loss 7.442750, \n",
      "Params:  tensor([  4.1015, -10.1372])\n",
      "Grad:  tensor([-0.2155,  1.2197])\n",
      "------\n",
      "------\n",
      "Epoch 527, Loss 7.427427, \n",
      "Params:  tensor([  4.1037, -10.1494])\n",
      "Grad:  tensor([-0.2151,  1.2176])\n",
      "------\n",
      "------\n",
      "Epoch 528, Loss 7.412152, \n",
      "Params:  tensor([  4.1058, -10.1616])\n",
      "Grad:  tensor([-0.2147,  1.2155])\n",
      "------\n",
      "------\n",
      "Epoch 529, Loss 7.396928, \n",
      "Params:  tensor([  4.1080, -10.1737])\n",
      "Grad:  tensor([-0.2144,  1.2135])\n",
      "------\n",
      "------\n",
      "Epoch 530, Loss 7.381757, \n",
      "Params:  tensor([  4.1101, -10.1858])\n",
      "Grad:  tensor([-0.2140,  1.2114])\n",
      "------\n",
      "------\n",
      "Epoch 531, Loss 7.366637, \n",
      "Params:  tensor([  4.1123, -10.1979])\n",
      "Grad:  tensor([-0.2136,  1.2093])\n",
      "------\n",
      "------\n",
      "Epoch 532, Loss 7.351567, \n",
      "Params:  tensor([  4.1144, -10.2100])\n",
      "Grad:  tensor([-0.2133,  1.2073])\n",
      "------\n",
      "------\n",
      "Epoch 533, Loss 7.336549, \n",
      "Params:  tensor([  4.1165, -10.2220])\n",
      "Grad:  tensor([-0.2129,  1.2052])\n",
      "------\n",
      "------\n",
      "Epoch 534, Loss 7.321584, \n",
      "Params:  tensor([  4.1187, -10.2340])\n",
      "Grad:  tensor([-0.2125,  1.2032])\n",
      "------\n",
      "------\n",
      "Epoch 535, Loss 7.306671, \n",
      "Params:  tensor([  4.1208, -10.2461])\n",
      "Grad:  tensor([-0.2122,  1.2012])\n",
      "------\n",
      "------\n",
      "Epoch 536, Loss 7.291804, \n",
      "Params:  tensor([  4.1229, -10.2581])\n",
      "Grad:  tensor([-0.2118,  1.1991])\n",
      "------\n",
      "------\n",
      "Epoch 537, Loss 7.276989, \n",
      "Params:  tensor([  4.1250, -10.2700])\n",
      "Grad:  tensor([-0.2115,  1.1971])\n",
      "------\n",
      "------\n",
      "Epoch 538, Loss 7.262227, \n",
      "Params:  tensor([  4.1271, -10.2820])\n",
      "Grad:  tensor([-0.2111,  1.1950])\n",
      "------\n",
      "------\n",
      "Epoch 539, Loss 7.247512, \n",
      "Params:  tensor([  4.1292, -10.2939])\n",
      "Grad:  tensor([-0.2108,  1.1930])\n",
      "------\n",
      "------\n",
      "Epoch 540, Loss 7.232845, \n",
      "Params:  tensor([  4.1313, -10.3058])\n",
      "Grad:  tensor([-0.2104,  1.1910])\n",
      "------\n",
      "------\n",
      "Epoch 541, Loss 7.218231, \n",
      "Params:  tensor([  4.1334, -10.3177])\n",
      "Grad:  tensor([-0.2100,  1.1890])\n",
      "------\n",
      "------\n",
      "Epoch 542, Loss 7.203665, \n",
      "Params:  tensor([  4.1355, -10.3296])\n",
      "Grad:  tensor([-0.2097,  1.1869])\n",
      "------\n",
      "------\n",
      "Epoch 543, Loss 7.189151, \n",
      "Params:  tensor([  4.1376, -10.3414])\n",
      "Grad:  tensor([-0.2093,  1.1849])\n",
      "------\n",
      "------\n",
      "Epoch 544, Loss 7.174683, \n",
      "Params:  tensor([  4.1397, -10.3533])\n",
      "Grad:  tensor([-0.2090,  1.1829])\n",
      "------\n",
      "------\n",
      "Epoch 545, Loss 7.160266, \n",
      "Params:  tensor([  4.1418, -10.3651])\n",
      "Grad:  tensor([-0.2086,  1.1809])\n",
      "------\n",
      "------\n",
      "Epoch 546, Loss 7.145897, \n",
      "Params:  tensor([  4.1439, -10.3769])\n",
      "Grad:  tensor([-0.2083,  1.1789])\n",
      "------\n",
      "------\n",
      "Epoch 547, Loss 7.131581, \n",
      "Params:  tensor([  4.1460, -10.3886])\n",
      "Grad:  tensor([-0.2079,  1.1769])\n",
      "------\n",
      "------\n",
      "Epoch 548, Loss 7.117305, \n",
      "Params:  tensor([  4.1480, -10.4004])\n",
      "Grad:  tensor([-0.2075,  1.1749])\n",
      "------\n",
      "------\n",
      "Epoch 549, Loss 7.103083, \n",
      "Params:  tensor([  4.1501, -10.4121])\n",
      "Grad:  tensor([-0.2072,  1.1729])\n",
      "------\n",
      "------\n",
      "Epoch 550, Loss 7.088911, \n",
      "Params:  tensor([  4.1522, -10.4238])\n",
      "Grad:  tensor([-0.2068,  1.1709])\n",
      "------\n",
      "------\n",
      "Epoch 551, Loss 7.074785, \n",
      "Params:  tensor([  4.1542, -10.4355])\n",
      "Grad:  tensor([-0.2065,  1.1689])\n",
      "------\n",
      "------\n",
      "Epoch 552, Loss 7.060707, \n",
      "Params:  tensor([  4.1563, -10.4472])\n",
      "Grad:  tensor([-0.2062,  1.1669])\n",
      "------\n",
      "------\n",
      "Epoch 553, Loss 7.046676, \n",
      "Params:  tensor([  4.1584, -10.4588])\n",
      "Grad:  tensor([-0.2058,  1.1649])\n",
      "------\n",
      "------\n",
      "Epoch 554, Loss 7.032695, \n",
      "Params:  tensor([  4.1604, -10.4704])\n",
      "Grad:  tensor([-0.2054,  1.1630])\n",
      "------\n",
      "------\n",
      "Epoch 555, Loss 7.018755, \n",
      "Params:  tensor([  4.1625, -10.4821])\n",
      "Grad:  tensor([-0.2051,  1.1610])\n",
      "------\n",
      "------\n",
      "Epoch 556, Loss 7.004870, \n",
      "Params:  tensor([  4.1645, -10.4936])\n",
      "Grad:  tensor([-0.2047,  1.1590])\n",
      "------\n",
      "------\n",
      "Epoch 557, Loss 6.991028, \n",
      "Params:  tensor([  4.1666, -10.5052])\n",
      "Grad:  tensor([-0.2044,  1.1571])\n",
      "------\n",
      "------\n",
      "Epoch 558, Loss 6.977232, \n",
      "Params:  tensor([  4.1686, -10.5168])\n",
      "Grad:  tensor([-0.2041,  1.1551])\n",
      "------\n",
      "------\n",
      "Epoch 559, Loss 6.963488, \n",
      "Params:  tensor([  4.1706, -10.5283])\n",
      "Grad:  tensor([-0.2037,  1.1531])\n",
      "------\n",
      "------\n",
      "Epoch 560, Loss 6.949787, \n",
      "Params:  tensor([  4.1727, -10.5398])\n",
      "Grad:  tensor([-0.2034,  1.1512])\n",
      "------\n",
      "------\n",
      "Epoch 561, Loss 6.936135, \n",
      "Params:  tensor([  4.1747, -10.5513])\n",
      "Grad:  tensor([-0.2030,  1.1492])\n",
      "------\n",
      "------\n",
      "Epoch 562, Loss 6.922528, \n",
      "Params:  tensor([  4.1767, -10.5628])\n",
      "Grad:  tensor([-0.2027,  1.1473])\n",
      "------\n",
      "------\n",
      "Epoch 563, Loss 6.908967, \n",
      "Params:  tensor([  4.1787, -10.5742])\n",
      "Grad:  tensor([-0.2023,  1.1453])\n",
      "------\n",
      "------\n",
      "Epoch 564, Loss 6.895452, \n",
      "Params:  tensor([  4.1808, -10.5857])\n",
      "Grad:  tensor([-0.2020,  1.1434])\n",
      "------\n",
      "------\n",
      "Epoch 565, Loss 6.881980, \n",
      "Params:  tensor([  4.1828, -10.5971])\n",
      "Grad:  tensor([-0.2016,  1.1414])\n",
      "------\n",
      "------\n",
      "Epoch 566, Loss 6.868559, \n",
      "Params:  tensor([  4.1848, -10.6085])\n",
      "Grad:  tensor([-0.2013,  1.1395])\n",
      "------\n",
      "------\n",
      "Epoch 567, Loss 6.855180, \n",
      "Params:  tensor([  4.1868, -10.6198])\n",
      "Grad:  tensor([-0.2010,  1.1375])\n",
      "------\n",
      "------\n",
      "Epoch 568, Loss 6.841848, \n",
      "Params:  tensor([  4.1888, -10.6312])\n",
      "Grad:  tensor([-0.2006,  1.1356])\n",
      "------\n",
      "------\n",
      "Epoch 569, Loss 6.828561, \n",
      "Params:  tensor([  4.1908, -10.6425])\n",
      "Grad:  tensor([-0.2003,  1.1337])\n",
      "------\n",
      "------\n",
      "Epoch 570, Loss 6.815319, \n",
      "Params:  tensor([  4.1928, -10.6539])\n",
      "Grad:  tensor([-0.1999,  1.1318])\n",
      "------\n",
      "------\n",
      "Epoch 571, Loss 6.802118, \n",
      "Params:  tensor([  4.1948, -10.6652])\n",
      "Grad:  tensor([-0.1996,  1.1298])\n",
      "------\n",
      "------\n",
      "Epoch 572, Loss 6.788968, \n",
      "Params:  tensor([  4.1968, -10.6764])\n",
      "Grad:  tensor([-0.1993,  1.1279])\n",
      "------\n",
      "------\n",
      "Epoch 573, Loss 6.775864, \n",
      "Params:  tensor([  4.1988, -10.6877])\n",
      "Grad:  tensor([-0.1989,  1.1260])\n",
      "------\n",
      "------\n",
      "Epoch 574, Loss 6.762797, \n",
      "Params:  tensor([  4.2008, -10.6989])\n",
      "Grad:  tensor([-0.1986,  1.1241])\n",
      "------\n",
      "------\n",
      "Epoch 575, Loss 6.749779, \n",
      "Params:  tensor([  4.2028, -10.7102])\n",
      "Grad:  tensor([-0.1982,  1.1222])\n",
      "------\n",
      "------\n",
      "Epoch 576, Loss 6.736804, \n",
      "Params:  tensor([  4.2047, -10.7214])\n",
      "Grad:  tensor([-0.1979,  1.1203])\n",
      "------\n",
      "------\n",
      "Epoch 577, Loss 6.723876, \n",
      "Params:  tensor([  4.2067, -10.7325])\n",
      "Grad:  tensor([-0.1976,  1.1184])\n",
      "------\n",
      "------\n",
      "Epoch 578, Loss 6.710987, \n",
      "Params:  tensor([  4.2087, -10.7437])\n",
      "Grad:  tensor([-0.1972,  1.1165])\n",
      "------\n",
      "------\n",
      "Epoch 579, Loss 6.698142, \n",
      "Params:  tensor([  4.2107, -10.7549])\n",
      "Grad:  tensor([-0.1969,  1.1146])\n",
      "------\n",
      "------\n",
      "Epoch 580, Loss 6.685345, \n",
      "Params:  tensor([  4.2126, -10.7660])\n",
      "Grad:  tensor([-0.1966,  1.1127])\n",
      "------\n",
      "------\n",
      "Epoch 581, Loss 6.672589, \n",
      "Params:  tensor([  4.2146, -10.7771])\n",
      "Grad:  tensor([-0.1962,  1.1108])\n",
      "------\n",
      "------\n",
      "Epoch 582, Loss 6.659873, \n",
      "Params:  tensor([  4.2165, -10.7882])\n",
      "Grad:  tensor([-0.1959,  1.1089])\n",
      "------\n",
      "------\n",
      "Epoch 583, Loss 6.647207, \n",
      "Params:  tensor([  4.2185, -10.7992])\n",
      "Grad:  tensor([-0.1956,  1.1070])\n",
      "------\n",
      "------\n",
      "Epoch 584, Loss 6.634578, \n",
      "Params:  tensor([  4.2204, -10.8103])\n",
      "Grad:  tensor([-0.1952,  1.1051])\n",
      "------\n",
      "------\n",
      "Epoch 585, Loss 6.621994, \n",
      "Params:  tensor([  4.2224, -10.8213])\n",
      "Grad:  tensor([-0.1949,  1.1033])\n",
      "------\n",
      "------\n",
      "Epoch 586, Loss 6.609454, \n",
      "Params:  tensor([  4.2243, -10.8323])\n",
      "Grad:  tensor([-0.1946,  1.1014])\n",
      "------\n",
      "------\n",
      "Epoch 587, Loss 6.596953, \n",
      "Params:  tensor([  4.2263, -10.8433])\n",
      "Grad:  tensor([-0.1942,  1.0995])\n",
      "------\n",
      "------\n",
      "Epoch 588, Loss 6.584499, \n",
      "Params:  tensor([  4.2282, -10.8543])\n",
      "Grad:  tensor([-0.1939,  1.0976])\n",
      "------\n",
      "------\n",
      "Epoch 589, Loss 6.572087, \n",
      "Params:  tensor([  4.2302, -10.8653])\n",
      "Grad:  tensor([-0.1936,  1.0958])\n",
      "------\n",
      "------\n",
      "Epoch 590, Loss 6.559712, \n",
      "Params:  tensor([  4.2321, -10.8762])\n",
      "Grad:  tensor([-0.1932,  1.0939])\n",
      "------\n",
      "------\n",
      "Epoch 591, Loss 6.547384, \n",
      "Params:  tensor([  4.2340, -10.8871])\n",
      "Grad:  tensor([-0.1929,  1.0921])\n",
      "------\n",
      "------\n",
      "Epoch 592, Loss 6.535097, \n",
      "Params:  tensor([  4.2359, -10.8980])\n",
      "Grad:  tensor([-0.1926,  1.0902])\n",
      "------\n",
      "------\n",
      "Epoch 593, Loss 6.522851, \n",
      "Params:  tensor([  4.2379, -10.9089])\n",
      "Grad:  tensor([-0.1923,  1.0884])\n",
      "------\n",
      "------\n",
      "Epoch 594, Loss 6.510646, \n",
      "Params:  tensor([  4.2398, -10.9198])\n",
      "Grad:  tensor([-0.1919,  1.0865])\n",
      "------\n",
      "------\n",
      "Epoch 595, Loss 6.498482, \n",
      "Params:  tensor([  4.2417, -10.9306])\n",
      "Grad:  tensor([-0.1916,  1.0847])\n",
      "------\n",
      "------\n",
      "Epoch 596, Loss 6.486361, \n",
      "Params:  tensor([  4.2436, -10.9415])\n",
      "Grad:  tensor([-0.1913,  1.0828])\n",
      "------\n",
      "------\n",
      "Epoch 597, Loss 6.474282, \n",
      "Params:  tensor([  4.2455, -10.9523])\n",
      "Grad:  tensor([-0.1910,  1.0810])\n",
      "------\n",
      "------\n",
      "Epoch 598, Loss 6.462241, \n",
      "Params:  tensor([  4.2474, -10.9631])\n",
      "Grad:  tensor([-0.1906,  1.0791])\n",
      "------\n",
      "------\n",
      "Epoch 599, Loss 6.450243, \n",
      "Params:  tensor([  4.2493, -10.9738])\n",
      "Grad:  tensor([-0.1903,  1.0773])\n",
      "------\n",
      "------\n",
      "Epoch 600, Loss 6.438284, \n",
      "Params:  tensor([  4.2512, -10.9846])\n",
      "Grad:  tensor([-0.1900,  1.0755])\n",
      "------\n",
      "------\n",
      "Epoch 601, Loss 6.426368, \n",
      "Params:  tensor([  4.2531, -10.9953])\n",
      "Grad:  tensor([-0.1897,  1.0737])\n",
      "------\n",
      "------\n",
      "Epoch 602, Loss 6.414490, \n",
      "Params:  tensor([  4.2550, -11.0060])\n",
      "Grad:  tensor([-0.1893,  1.0718])\n",
      "------\n",
      "------\n",
      "Epoch 603, Loss 6.402653, \n",
      "Params:  tensor([  4.2569, -11.0167])\n",
      "Grad:  tensor([-0.1890,  1.0700])\n",
      "------\n",
      "------\n",
      "Epoch 604, Loss 6.390859, \n",
      "Params:  tensor([  4.2588, -11.0274])\n",
      "Grad:  tensor([-0.1887,  1.0682])\n",
      "------\n",
      "------\n",
      "Epoch 605, Loss 6.379103, \n",
      "Params:  tensor([  4.2607, -11.0381])\n",
      "Grad:  tensor([-0.1884,  1.0664])\n",
      "------\n",
      "------\n",
      "Epoch 606, Loss 6.367385, \n",
      "Params:  tensor([  4.2626, -11.0487])\n",
      "Grad:  tensor([-0.1880,  1.0646])\n",
      "------\n",
      "------\n",
      "Epoch 607, Loss 6.355706, \n",
      "Params:  tensor([  4.2644, -11.0594])\n",
      "Grad:  tensor([-0.1877,  1.0628])\n",
      "------\n",
      "------\n",
      "Epoch 608, Loss 6.344070, \n",
      "Params:  tensor([  4.2663, -11.0700])\n",
      "Grad:  tensor([-0.1874,  1.0609])\n",
      "------\n",
      "------\n",
      "Epoch 609, Loss 6.332472, \n",
      "Params:  tensor([  4.2682, -11.0806])\n",
      "Grad:  tensor([-0.1871,  1.0591])\n",
      "------\n",
      "------\n",
      "Epoch 610, Loss 6.320912, \n",
      "Params:  tensor([  4.2701, -11.0911])\n",
      "Grad:  tensor([-0.1868,  1.0573])\n",
      "------\n",
      "------\n",
      "Epoch 611, Loss 6.309395, \n",
      "Params:  tensor([  4.2719, -11.1017])\n",
      "Grad:  tensor([-0.1865,  1.0555])\n",
      "------\n",
      "------\n",
      "Epoch 612, Loss 6.297915, \n",
      "Params:  tensor([  4.2738, -11.1122])\n",
      "Grad:  tensor([-0.1861,  1.0538])\n",
      "------\n",
      "------\n",
      "Epoch 613, Loss 6.286473, \n",
      "Params:  tensor([  4.2756, -11.1227])\n",
      "Grad:  tensor([-0.1858,  1.0520])\n",
      "------\n",
      "------\n",
      "Epoch 614, Loss 6.275074, \n",
      "Params:  tensor([  4.2775, -11.1333])\n",
      "Grad:  tensor([-0.1855,  1.0502])\n",
      "------\n",
      "------\n",
      "Epoch 615, Loss 6.263708, \n",
      "Params:  tensor([  4.2794, -11.1437])\n",
      "Grad:  tensor([-0.1852,  1.0484])\n",
      "------\n",
      "------\n",
      "Epoch 616, Loss 6.252382, \n",
      "Params:  tensor([  4.2812, -11.1542])\n",
      "Grad:  tensor([-0.1849,  1.0466])\n",
      "------\n",
      "------\n",
      "Epoch 617, Loss 6.241098, \n",
      "Params:  tensor([  4.2830, -11.1646])\n",
      "Grad:  tensor([-0.1846,  1.0448])\n",
      "------\n",
      "------\n",
      "Epoch 618, Loss 6.229849, \n",
      "Params:  tensor([  4.2849, -11.1751])\n",
      "Grad:  tensor([-0.1843,  1.0431])\n",
      "------\n",
      "------\n",
      "Epoch 619, Loss 6.218639, \n",
      "Params:  tensor([  4.2867, -11.1855])\n",
      "Grad:  tensor([-0.1840,  1.0413])\n",
      "------\n",
      "------\n",
      "Epoch 620, Loss 6.207470, \n",
      "Params:  tensor([  4.2886, -11.1959])\n",
      "Grad:  tensor([-0.1836,  1.0395])\n",
      "------\n",
      "------\n",
      "Epoch 621, Loss 6.196334, \n",
      "Params:  tensor([  4.2904, -11.2063])\n",
      "Grad:  tensor([-0.1833,  1.0378])\n",
      "------\n",
      "------\n",
      "Epoch 622, Loss 6.185240, \n",
      "Params:  tensor([  4.2922, -11.2166])\n",
      "Grad:  tensor([-0.1830,  1.0360])\n",
      "------\n",
      "------\n",
      "Epoch 623, Loss 6.174181, \n",
      "Params:  tensor([  4.2941, -11.2270])\n",
      "Grad:  tensor([-0.1827,  1.0342])\n",
      "------\n",
      "------\n",
      "Epoch 624, Loss 6.163159, \n",
      "Params:  tensor([  4.2959, -11.2373])\n",
      "Grad:  tensor([-0.1824,  1.0325])\n",
      "------\n",
      "------\n",
      "Epoch 625, Loss 6.152177, \n",
      "Params:  tensor([  4.2977, -11.2476])\n",
      "Grad:  tensor([-0.1821,  1.0307])\n",
      "------\n",
      "------\n",
      "Epoch 626, Loss 6.141230, \n",
      "Params:  tensor([  4.2995, -11.2579])\n",
      "Grad:  tensor([-0.1818,  1.0290])\n",
      "------\n",
      "------\n",
      "Epoch 627, Loss 6.130322, \n",
      "Params:  tensor([  4.3013, -11.2682])\n",
      "Grad:  tensor([-0.1815,  1.0272])\n",
      "------\n",
      "------\n",
      "Epoch 628, Loss 6.119448, \n",
      "Params:  tensor([  4.3031, -11.2784])\n",
      "Grad:  tensor([-0.1811,  1.0255])\n",
      "------\n",
      "------\n",
      "Epoch 629, Loss 6.108614, \n",
      "Params:  tensor([  4.3050, -11.2887])\n",
      "Grad:  tensor([-0.1808,  1.0237])\n",
      "------\n",
      "------\n",
      "Epoch 630, Loss 6.097815, \n",
      "Params:  tensor([  4.3068, -11.2989])\n",
      "Grad:  tensor([-0.1805,  1.0220])\n",
      "------\n",
      "------\n",
      "Epoch 631, Loss 6.087054, \n",
      "Params:  tensor([  4.3086, -11.3091])\n",
      "Grad:  tensor([-0.1802,  1.0203])\n",
      "------\n",
      "------\n",
      "Epoch 632, Loss 6.076329, \n",
      "Params:  tensor([  4.3104, -11.3193])\n",
      "Grad:  tensor([-0.1799,  1.0185])\n",
      "------\n",
      "------\n",
      "Epoch 633, Loss 6.065644, \n",
      "Params:  tensor([  4.3122, -11.3294])\n",
      "Grad:  tensor([-0.1796,  1.0168])\n",
      "------\n",
      "------\n",
      "Epoch 634, Loss 6.054988, \n",
      "Params:  tensor([  4.3139, -11.3396])\n",
      "Grad:  tensor([-0.1793,  1.0151])\n",
      "------\n",
      "------\n",
      "Epoch 635, Loss 6.044372, \n",
      "Params:  tensor([  4.3157, -11.3497])\n",
      "Grad:  tensor([-0.1790,  1.0133])\n",
      "------\n",
      "------\n",
      "Epoch 636, Loss 6.033794, \n",
      "Params:  tensor([  4.3175, -11.3598])\n",
      "Grad:  tensor([-0.1787,  1.0116])\n",
      "------\n",
      "------\n",
      "Epoch 637, Loss 6.023247, \n",
      "Params:  tensor([  4.3193, -11.3699])\n",
      "Grad:  tensor([-0.1784,  1.0099])\n",
      "------\n",
      "------\n",
      "Epoch 638, Loss 6.012738, \n",
      "Params:  tensor([  4.3211, -11.3800])\n",
      "Grad:  tensor([-0.1781,  1.0082])\n",
      "------\n",
      "------\n",
      "Epoch 639, Loss 6.002264, \n",
      "Params:  tensor([  4.3229, -11.3901])\n",
      "Grad:  tensor([-0.1778,  1.0065])\n",
      "------\n",
      "------\n",
      "Epoch 640, Loss 5.991828, \n",
      "Params:  tensor([  4.3246, -11.4001])\n",
      "Grad:  tensor([-0.1775,  1.0048])\n",
      "------\n",
      "------\n",
      "Epoch 641, Loss 5.981425, \n",
      "Params:  tensor([  4.3264, -11.4102])\n",
      "Grad:  tensor([-0.1772,  1.0031])\n",
      "------\n",
      "------\n",
      "Epoch 642, Loss 5.971058, \n",
      "Params:  tensor([  4.3282, -11.4202])\n",
      "Grad:  tensor([-0.1769,  1.0014])\n",
      "------\n",
      "------\n",
      "Epoch 643, Loss 5.960727, \n",
      "Params:  tensor([  4.3300, -11.4302])\n",
      "Grad:  tensor([-0.1766,  0.9997])\n",
      "------\n",
      "------\n",
      "Epoch 644, Loss 5.950432, \n",
      "Params:  tensor([  4.3317, -11.4401])\n",
      "Grad:  tensor([-0.1763,  0.9980])\n",
      "------\n",
      "------\n",
      "Epoch 645, Loss 5.940171, \n",
      "Params:  tensor([  4.3335, -11.4501])\n",
      "Grad:  tensor([-0.1760,  0.9963])\n",
      "------\n",
      "------\n",
      "Epoch 646, Loss 5.929944, \n",
      "Params:  tensor([  4.3352, -11.4601])\n",
      "Grad:  tensor([-0.1757,  0.9946])\n",
      "------\n",
      "------\n",
      "Epoch 647, Loss 5.919752, \n",
      "Params:  tensor([  4.3370, -11.4700])\n",
      "Grad:  tensor([-0.1754,  0.9929])\n",
      "------\n",
      "------\n",
      "Epoch 648, Loss 5.909596, \n",
      "Params:  tensor([  4.3387, -11.4799])\n",
      "Grad:  tensor([-0.1751,  0.9912])\n",
      "------\n",
      "------\n",
      "Epoch 649, Loss 5.899472, \n",
      "Params:  tensor([  4.3405, -11.4898])\n",
      "Grad:  tensor([-0.1748,  0.9895])\n",
      "------\n",
      "------\n",
      "Epoch 650, Loss 5.889383, \n",
      "Params:  tensor([  4.3422, -11.4997])\n",
      "Grad:  tensor([-0.1745,  0.9878])\n",
      "------\n",
      "------\n",
      "Epoch 651, Loss 5.879326, \n",
      "Params:  tensor([  4.3440, -11.5095])\n",
      "Grad:  tensor([-0.1742,  0.9862])\n",
      "------\n",
      "------\n",
      "Epoch 652, Loss 5.869310, \n",
      "Params:  tensor([  4.3457, -11.5194])\n",
      "Grad:  tensor([-0.1739,  0.9845])\n",
      "------\n",
      "------\n",
      "Epoch 653, Loss 5.859322, \n",
      "Params:  tensor([  4.3474, -11.5292])\n",
      "Grad:  tensor([-0.1736,  0.9828])\n",
      "------\n",
      "------\n",
      "Epoch 654, Loss 5.849374, \n",
      "Params:  tensor([  4.3492, -11.5390])\n",
      "Grad:  tensor([-0.1733,  0.9811])\n",
      "------\n",
      "------\n",
      "Epoch 655, Loss 5.839453, \n",
      "Params:  tensor([  4.3509, -11.5488])\n",
      "Grad:  tensor([-0.1730,  0.9795])\n",
      "------\n",
      "------\n",
      "Epoch 656, Loss 5.829570, \n",
      "Params:  tensor([  4.3526, -11.5586])\n",
      "Grad:  tensor([-0.1727,  0.9778])\n",
      "------\n",
      "------\n",
      "Epoch 657, Loss 5.819718, \n",
      "Params:  tensor([  4.3544, -11.5683])\n",
      "Grad:  tensor([-0.1724,  0.9761])\n",
      "------\n",
      "------\n",
      "Epoch 658, Loss 5.809901, \n",
      "Params:  tensor([  4.3561, -11.5781])\n",
      "Grad:  tensor([-0.1722,  0.9745])\n",
      "------\n",
      "------\n",
      "Epoch 659, Loss 5.800116, \n",
      "Params:  tensor([  4.3578, -11.5878])\n",
      "Grad:  tensor([-0.1719,  0.9728])\n",
      "------\n",
      "------\n",
      "Epoch 660, Loss 5.790367, \n",
      "Params:  tensor([  4.3595, -11.5975])\n",
      "Grad:  tensor([-0.1716,  0.9712])\n",
      "------\n",
      "------\n",
      "Epoch 661, Loss 5.780646, \n",
      "Params:  tensor([  4.3612, -11.6072])\n",
      "Grad:  tensor([-0.1713,  0.9695])\n",
      "------\n",
      "------\n",
      "Epoch 662, Loss 5.770962, \n",
      "Params:  tensor([  4.3629, -11.6169])\n",
      "Grad:  tensor([-0.1710,  0.9679])\n",
      "------\n",
      "------\n",
      "Epoch 663, Loss 5.761312, \n",
      "Params:  tensor([  4.3646, -11.6266])\n",
      "Grad:  tensor([-0.1707,  0.9662])\n",
      "------\n",
      "------\n",
      "Epoch 664, Loss 5.751694, \n",
      "Params:  tensor([  4.3664, -11.6362])\n",
      "Grad:  tensor([-0.1704,  0.9646])\n",
      "------\n",
      "------\n",
      "Epoch 665, Loss 5.742105, \n",
      "Params:  tensor([  4.3681, -11.6458])\n",
      "Grad:  tensor([-0.1701,  0.9630])\n",
      "------\n",
      "------\n",
      "Epoch 666, Loss 5.732550, \n",
      "Params:  tensor([  4.3697, -11.6555])\n",
      "Grad:  tensor([-0.1698,  0.9613])\n",
      "------\n",
      "------\n",
      "Epoch 667, Loss 5.723031, \n",
      "Params:  tensor([  4.3714, -11.6651])\n",
      "Grad:  tensor([-0.1695,  0.9597])\n",
      "------\n",
      "------\n",
      "Epoch 668, Loss 5.713540, \n",
      "Params:  tensor([  4.3731, -11.6746])\n",
      "Grad:  tensor([-0.1692,  0.9581])\n",
      "------\n",
      "------\n",
      "Epoch 669, Loss 5.704083, \n",
      "Params:  tensor([  4.3748, -11.6842])\n",
      "Grad:  tensor([-0.1690,  0.9564])\n",
      "------\n",
      "------\n",
      "Epoch 670, Loss 5.694659, \n",
      "Params:  tensor([  4.3765, -11.6937])\n",
      "Grad:  tensor([-0.1687,  0.9548])\n",
      "------\n",
      "------\n",
      "Epoch 671, Loss 5.685266, \n",
      "Params:  tensor([  4.3782, -11.7033])\n",
      "Grad:  tensor([-0.1684,  0.9532])\n",
      "------\n",
      "------\n",
      "Epoch 672, Loss 5.675904, \n",
      "Params:  tensor([  4.3799, -11.7128])\n",
      "Grad:  tensor([-0.1681,  0.9516])\n",
      "------\n",
      "------\n",
      "Epoch 673, Loss 5.666573, \n",
      "Params:  tensor([  4.3816, -11.7223])\n",
      "Grad:  tensor([-0.1678,  0.9499])\n",
      "------\n",
      "------\n",
      "Epoch 674, Loss 5.657277, \n",
      "Params:  tensor([  4.3832, -11.7318])\n",
      "Grad:  tensor([-0.1675,  0.9483])\n",
      "------\n",
      "------\n",
      "Epoch 675, Loss 5.648010, \n",
      "Params:  tensor([  4.3849, -11.7412])\n",
      "Grad:  tensor([-0.1673,  0.9467])\n",
      "------\n",
      "------\n",
      "Epoch 676, Loss 5.638776, \n",
      "Params:  tensor([  4.3866, -11.7507])\n",
      "Grad:  tensor([-0.1670,  0.9451])\n",
      "------\n",
      "------\n",
      "Epoch 677, Loss 5.629574, \n",
      "Params:  tensor([  4.3882, -11.7601])\n",
      "Grad:  tensor([-0.1667,  0.9435])\n",
      "------\n",
      "------\n",
      "Epoch 678, Loss 5.620402, \n",
      "Params:  tensor([  4.3899, -11.7696])\n",
      "Grad:  tensor([-0.1664,  0.9419])\n",
      "------\n",
      "------\n",
      "Epoch 679, Loss 5.611260, \n",
      "Params:  tensor([  4.3916, -11.7790])\n",
      "Grad:  tensor([-0.1661,  0.9403])\n",
      "------\n",
      "------\n",
      "Epoch 680, Loss 5.602149, \n",
      "Params:  tensor([  4.3932, -11.7883])\n",
      "Grad:  tensor([-0.1658,  0.9387])\n",
      "------\n",
      "------\n",
      "Epoch 681, Loss 5.593071, \n",
      "Params:  tensor([  4.3949, -11.7977])\n",
      "Grad:  tensor([-0.1656,  0.9371])\n",
      "------\n",
      "------\n",
      "Epoch 682, Loss 5.584022, \n",
      "Params:  tensor([  4.3965, -11.8071])\n",
      "Grad:  tensor([-0.1653,  0.9355])\n",
      "------\n",
      "------\n",
      "Epoch 683, Loss 5.575005, \n",
      "Params:  tensor([  4.3982, -11.8164])\n",
      "Grad:  tensor([-0.1650,  0.9339])\n",
      "------\n",
      "------\n",
      "Epoch 684, Loss 5.566019, \n",
      "Params:  tensor([  4.3998, -11.8257])\n",
      "Grad:  tensor([-0.1647,  0.9323])\n",
      "------\n",
      "------\n",
      "Epoch 685, Loss 5.557063, \n",
      "Params:  tensor([  4.4015, -11.8350])\n",
      "Grad:  tensor([-0.1644,  0.9308])\n",
      "------\n",
      "------\n",
      "Epoch 686, Loss 5.548136, \n",
      "Params:  tensor([  4.4031, -11.8443])\n",
      "Grad:  tensor([-0.1641,  0.9292])\n",
      "------\n",
      "------\n",
      "Epoch 687, Loss 5.539241, \n",
      "Params:  tensor([  4.4048, -11.8536])\n",
      "Grad:  tensor([-0.1639,  0.9276])\n",
      "------\n",
      "------\n",
      "Epoch 688, Loss 5.530376, \n",
      "Params:  tensor([  4.4064, -11.8629])\n",
      "Grad:  tensor([-0.1636,  0.9260])\n",
      "------\n",
      "------\n",
      "Epoch 689, Loss 5.521540, \n",
      "Params:  tensor([  4.4080, -11.8721])\n",
      "Grad:  tensor([-0.1633,  0.9245])\n",
      "------\n",
      "------\n",
      "Epoch 690, Loss 5.512734, \n",
      "Params:  tensor([  4.4097, -11.8813])\n",
      "Grad:  tensor([-0.1630,  0.9229])\n",
      "------\n",
      "------\n",
      "Epoch 691, Loss 5.503958, \n",
      "Params:  tensor([  4.4113, -11.8906])\n",
      "Grad:  tensor([-0.1628,  0.9213])\n",
      "------\n",
      "------\n",
      "Epoch 692, Loss 5.495212, \n",
      "Params:  tensor([  4.4129, -11.8998])\n",
      "Grad:  tensor([-0.1625,  0.9197])\n",
      "------\n",
      "------\n",
      "Epoch 693, Loss 5.486496, \n",
      "Params:  tensor([  4.4145, -11.9089])\n",
      "Grad:  tensor([-0.1622,  0.9182])\n",
      "------\n",
      "------\n",
      "Epoch 694, Loss 5.477808, \n",
      "Params:  tensor([  4.4161, -11.9181])\n",
      "Grad:  tensor([-0.1619,  0.9166])\n",
      "------\n",
      "------\n",
      "Epoch 695, Loss 5.469152, \n",
      "Params:  tensor([  4.4178, -11.9272])\n",
      "Grad:  tensor([-0.1617,  0.9151])\n",
      "------\n",
      "------\n",
      "Epoch 696, Loss 5.460525, \n",
      "Params:  tensor([  4.4194, -11.9364])\n",
      "Grad:  tensor([-0.1614,  0.9135])\n",
      "------\n",
      "------\n",
      "Epoch 697, Loss 5.451928, \n",
      "Params:  tensor([  4.4210, -11.9455])\n",
      "Grad:  tensor([-0.1611,  0.9120])\n",
      "------\n",
      "------\n",
      "Epoch 698, Loss 5.443358, \n",
      "Params:  tensor([  4.4226, -11.9546])\n",
      "Grad:  tensor([-0.1608,  0.9104])\n",
      "------\n",
      "------\n",
      "Epoch 699, Loss 5.434819, \n",
      "Params:  tensor([  4.4242, -11.9637])\n",
      "Grad:  tensor([-0.1605,  0.9089])\n",
      "------\n",
      "------\n",
      "Epoch 700, Loss 5.426309, \n",
      "Params:  tensor([  4.4258, -11.9728])\n",
      "Grad:  tensor([-0.1603,  0.9073])\n",
      "------\n",
      "------\n",
      "Epoch 701, Loss 5.417827, \n",
      "Params:  tensor([  4.4274, -11.9818])\n",
      "Grad:  tensor([-0.1600,  0.9058])\n",
      "------\n",
      "------\n",
      "Epoch 702, Loss 5.409372, \n",
      "Params:  tensor([  4.4290, -11.9909])\n",
      "Grad:  tensor([-0.1597,  0.9042])\n",
      "------\n",
      "------\n",
      "Epoch 703, Loss 5.400949, \n",
      "Params:  tensor([  4.4306, -11.9999])\n",
      "Grad:  tensor([-0.1595,  0.9027])\n",
      "------\n",
      "------\n",
      "Epoch 704, Loss 5.392550, \n",
      "Params:  tensor([  4.4322, -12.0089])\n",
      "Grad:  tensor([-0.1592,  0.9012])\n",
      "------\n",
      "------\n",
      "Epoch 705, Loss 5.384184, \n",
      "Params:  tensor([  4.4338, -12.0179])\n",
      "Grad:  tensor([-0.1589,  0.8996])\n",
      "------\n",
      "------\n",
      "Epoch 706, Loss 5.375846, \n",
      "Params:  tensor([  4.4354, -12.0269])\n",
      "Grad:  tensor([-0.1586,  0.8981])\n",
      "------\n",
      "------\n",
      "Epoch 707, Loss 5.367537, \n",
      "Params:  tensor([  4.4369, -12.0359])\n",
      "Grad:  tensor([-0.1584,  0.8966])\n",
      "------\n",
      "------\n",
      "Epoch 708, Loss 5.359253, \n",
      "Params:  tensor([  4.4385, -12.0448])\n",
      "Grad:  tensor([-0.1581,  0.8951])\n",
      "------\n",
      "------\n",
      "Epoch 709, Loss 5.350999, \n",
      "Params:  tensor([  4.4401, -12.0537])\n",
      "Grad:  tensor([-0.1578,  0.8935])\n",
      "------\n",
      "------\n",
      "Epoch 710, Loss 5.342772, \n",
      "Params:  tensor([  4.4417, -12.0627])\n",
      "Grad:  tensor([-0.1576,  0.8920])\n",
      "------\n",
      "------\n",
      "Epoch 711, Loss 5.334575, \n",
      "Params:  tensor([  4.4433, -12.0716])\n",
      "Grad:  tensor([-0.1573,  0.8905])\n",
      "------\n",
      "------\n",
      "Epoch 712, Loss 5.326402, \n",
      "Params:  tensor([  4.4448, -12.0805])\n",
      "Grad:  tensor([-0.1570,  0.8890])\n",
      "------\n",
      "------\n",
      "Epoch 713, Loss 5.318260, \n",
      "Params:  tensor([  4.4464, -12.0893])\n",
      "Grad:  tensor([-0.1568,  0.8875])\n",
      "------\n",
      "------\n",
      "Epoch 714, Loss 5.310144, \n",
      "Params:  tensor([  4.4480, -12.0982])\n",
      "Grad:  tensor([-0.1565,  0.8860])\n",
      "------\n",
      "------\n",
      "Epoch 715, Loss 5.302055, \n",
      "Params:  tensor([  4.4495, -12.1070])\n",
      "Grad:  tensor([-0.1562,  0.8845])\n",
      "------\n",
      "------\n",
      "Epoch 716, Loss 5.293994, \n",
      "Params:  tensor([  4.4511, -12.1159])\n",
      "Grad:  tensor([-0.1560,  0.8830])\n",
      "------\n",
      "------\n",
      "Epoch 717, Loss 5.285964, \n",
      "Params:  tensor([  4.4526, -12.1247])\n",
      "Grad:  tensor([-0.1557,  0.8815])\n",
      "------\n",
      "------\n",
      "Epoch 718, Loss 5.277958, \n",
      "Params:  tensor([  4.4542, -12.1335])\n",
      "Grad:  tensor([-0.1555,  0.8800])\n",
      "------\n",
      "------\n",
      "Epoch 719, Loss 5.269979, \n",
      "Params:  tensor([  4.4557, -12.1423])\n",
      "Grad:  tensor([-0.1552,  0.8785])\n",
      "------\n",
      "------\n",
      "Epoch 720, Loss 5.262027, \n",
      "Params:  tensor([  4.4573, -12.1510])\n",
      "Grad:  tensor([-0.1549,  0.8770])\n",
      "------\n",
      "------\n",
      "Epoch 721, Loss 5.254103, \n",
      "Params:  tensor([  4.4588, -12.1598])\n",
      "Grad:  tensor([-0.1547,  0.8755])\n",
      "------\n",
      "------\n",
      "Epoch 722, Loss 5.246205, \n",
      "Params:  tensor([  4.4604, -12.1685])\n",
      "Grad:  tensor([-0.1544,  0.8740])\n",
      "------\n",
      "------\n",
      "Epoch 723, Loss 5.238335, \n",
      "Params:  tensor([  4.4619, -12.1773])\n",
      "Grad:  tensor([-0.1541,  0.8725])\n",
      "------\n",
      "------\n",
      "Epoch 724, Loss 5.230492, \n",
      "Params:  tensor([  4.4635, -12.1860])\n",
      "Grad:  tensor([-0.1539,  0.8710])\n",
      "------\n",
      "------\n",
      "Epoch 725, Loss 5.222674, \n",
      "Params:  tensor([  4.4650, -12.1947])\n",
      "Grad:  tensor([-0.1536,  0.8696])\n",
      "------\n",
      "------\n",
      "Epoch 726, Loss 5.214881, \n",
      "Params:  tensor([  4.4665, -12.2033])\n",
      "Grad:  tensor([-0.1533,  0.8681])\n",
      "------\n",
      "------\n",
      "Epoch 727, Loss 5.207120, \n",
      "Params:  tensor([  4.4681, -12.2120])\n",
      "Grad:  tensor([-0.1531,  0.8666])\n",
      "------\n",
      "------\n",
      "Epoch 728, Loss 5.199381, \n",
      "Params:  tensor([  4.4696, -12.2207])\n",
      "Grad:  tensor([-0.1528,  0.8651])\n",
      "------\n",
      "------\n",
      "Epoch 729, Loss 5.191670, \n",
      "Params:  tensor([  4.4711, -12.2293])\n",
      "Grad:  tensor([-0.1526,  0.8637])\n",
      "------\n",
      "------\n",
      "Epoch 730, Loss 5.183985, \n",
      "Params:  tensor([  4.4726, -12.2379])\n",
      "Grad:  tensor([-0.1523,  0.8622])\n",
      "------\n",
      "------\n",
      "Epoch 731, Loss 5.176324, \n",
      "Params:  tensor([  4.4742, -12.2465])\n",
      "Grad:  tensor([-0.1520,  0.8607])\n",
      "------\n",
      "------\n",
      "Epoch 732, Loss 5.168688, \n",
      "Params:  tensor([  4.4757, -12.2551])\n",
      "Grad:  tensor([-0.1518,  0.8593])\n",
      "------\n",
      "------\n",
      "Epoch 733, Loss 5.161084, \n",
      "Params:  tensor([  4.4772, -12.2637])\n",
      "Grad:  tensor([-0.1515,  0.8578])\n",
      "------\n",
      "------\n",
      "Epoch 734, Loss 5.153500, \n",
      "Params:  tensor([  4.4787, -12.2723])\n",
      "Grad:  tensor([-0.1513,  0.8564])\n",
      "------\n",
      "------\n",
      "Epoch 735, Loss 5.145944, \n",
      "Params:  tensor([  4.4802, -12.2808])\n",
      "Grad:  tensor([-0.1510,  0.8549])\n",
      "------\n",
      "------\n",
      "Epoch 736, Loss 5.138413, \n",
      "Params:  tensor([  4.4817, -12.2893])\n",
      "Grad:  tensor([-0.1508,  0.8535])\n",
      "------\n",
      "------\n",
      "Epoch 737, Loss 5.130910, \n",
      "Params:  tensor([  4.4832, -12.2979])\n",
      "Grad:  tensor([-0.1505,  0.8520])\n",
      "------\n",
      "------\n",
      "Epoch 738, Loss 5.123428, \n",
      "Params:  tensor([  4.4847, -12.3064])\n",
      "Grad:  tensor([-0.1502,  0.8506])\n",
      "------\n",
      "------\n",
      "Epoch 739, Loss 5.115978, \n",
      "Params:  tensor([  4.4862, -12.3149])\n",
      "Grad:  tensor([-0.1500,  0.8491])\n",
      "------\n",
      "------\n",
      "Epoch 740, Loss 5.108547, \n",
      "Params:  tensor([  4.4877, -12.3233])\n",
      "Grad:  tensor([-0.1497,  0.8477])\n",
      "------\n",
      "------\n",
      "Epoch 741, Loss 5.101143, \n",
      "Params:  tensor([  4.4892, -12.3318])\n",
      "Grad:  tensor([-0.1495,  0.8462])\n",
      "------\n",
      "------\n",
      "Epoch 742, Loss 5.093765, \n",
      "Params:  tensor([  4.4907, -12.3402])\n",
      "Grad:  tensor([-0.1492,  0.8448])\n",
      "------\n",
      "------\n",
      "Epoch 743, Loss 5.086414, \n",
      "Params:  tensor([  4.4922, -12.3487])\n",
      "Grad:  tensor([-0.1490,  0.8434])\n",
      "------\n",
      "------\n",
      "Epoch 744, Loss 5.079086, \n",
      "Params:  tensor([  4.4937, -12.3571])\n",
      "Grad:  tensor([-0.1487,  0.8419])\n",
      "------\n",
      "------\n",
      "Epoch 745, Loss 5.071781, \n",
      "Params:  tensor([  4.4952, -12.3655])\n",
      "Grad:  tensor([-0.1485,  0.8405])\n",
      "------\n",
      "------\n",
      "Epoch 746, Loss 5.064505, \n",
      "Params:  tensor([  4.4967, -12.3739])\n",
      "Grad:  tensor([-0.1482,  0.8391])\n",
      "------\n",
      "------\n",
      "Epoch 747, Loss 5.057247, \n",
      "Params:  tensor([  4.4981, -12.3823])\n",
      "Grad:  tensor([-0.1480,  0.8376])\n",
      "------\n",
      "------\n",
      "Epoch 748, Loss 5.050021, \n",
      "Params:  tensor([  4.4996, -12.3906])\n",
      "Grad:  tensor([-0.1477,  0.8362])\n",
      "------\n",
      "------\n",
      "Epoch 749, Loss 5.042817, \n",
      "Params:  tensor([  4.5011, -12.3990])\n",
      "Grad:  tensor([-0.1475,  0.8348])\n",
      "------\n",
      "------\n",
      "Epoch 750, Loss 5.035636, \n",
      "Params:  tensor([  4.5026, -12.4073])\n",
      "Grad:  tensor([-0.1472,  0.8334])\n",
      "------\n",
      "------\n",
      "Epoch 751, Loss 5.028476, \n",
      "Params:  tensor([  4.5040, -12.4156])\n",
      "Grad:  tensor([-0.1470,  0.8320])\n",
      "------\n",
      "------\n",
      "Epoch 752, Loss 5.021346, \n",
      "Params:  tensor([  4.5055, -12.4239])\n",
      "Grad:  tensor([-0.1467,  0.8305])\n",
      "------\n",
      "------\n",
      "Epoch 753, Loss 5.014240, \n",
      "Params:  tensor([  4.5070, -12.4322])\n",
      "Grad:  tensor([-0.1465,  0.8291])\n",
      "------\n",
      "------\n",
      "Epoch 754, Loss 5.007157, \n",
      "Params:  tensor([  4.5084, -12.4405])\n",
      "Grad:  tensor([-0.1462,  0.8277])\n",
      "------\n",
      "------\n",
      "Epoch 755, Loss 5.000099, \n",
      "Params:  tensor([  4.5099, -12.4488])\n",
      "Grad:  tensor([-0.1460,  0.8263])\n",
      "------\n",
      "------\n",
      "Epoch 756, Loss 4.993064, \n",
      "Params:  tensor([  4.5113, -12.4570])\n",
      "Grad:  tensor([-0.1457,  0.8249])\n",
      "------\n",
      "------\n",
      "Epoch 757, Loss 4.986051, \n",
      "Params:  tensor([  4.5128, -12.4653])\n",
      "Grad:  tensor([-0.1455,  0.8235])\n",
      "------\n",
      "------\n",
      "Epoch 758, Loss 4.979064, \n",
      "Params:  tensor([  4.5143, -12.4735])\n",
      "Grad:  tensor([-0.1452,  0.8221])\n",
      "------\n",
      "------\n",
      "Epoch 759, Loss 4.972100, \n",
      "Params:  tensor([  4.5157, -12.4817])\n",
      "Grad:  tensor([-0.1450,  0.8207])\n",
      "------\n",
      "------\n",
      "Epoch 760, Loss 4.965159, \n",
      "Params:  tensor([  4.5172, -12.4899])\n",
      "Grad:  tensor([-0.1447,  0.8193])\n",
      "------\n",
      "------\n",
      "Epoch 761, Loss 4.958245, \n",
      "Params:  tensor([  4.5186, -12.4981])\n",
      "Grad:  tensor([-0.1445,  0.8179])\n",
      "------\n",
      "------\n",
      "Epoch 762, Loss 4.951351, \n",
      "Params:  tensor([  4.5200, -12.5062])\n",
      "Grad:  tensor([-0.1443,  0.8165])\n",
      "------\n",
      "------\n",
      "Epoch 763, Loss 4.944479, \n",
      "Params:  tensor([  4.5215, -12.5144])\n",
      "Grad:  tensor([-0.1440,  0.8152])\n",
      "------\n",
      "------\n",
      "Epoch 764, Loss 4.937633, \n",
      "Params:  tensor([  4.5229, -12.5225])\n",
      "Grad:  tensor([-0.1438,  0.8138])\n",
      "------\n",
      "------\n",
      "Epoch 765, Loss 4.930812, \n",
      "Params:  tensor([  4.5244, -12.5306])\n",
      "Grad:  tensor([-0.1435,  0.8124])\n",
      "------\n",
      "------\n",
      "Epoch 766, Loss 4.924009, \n",
      "Params:  tensor([  4.5258, -12.5387])\n",
      "Grad:  tensor([-0.1433,  0.8110])\n",
      "------\n",
      "------\n",
      "Epoch 767, Loss 4.917234, \n",
      "Params:  tensor([  4.5272, -12.5468])\n",
      "Grad:  tensor([-0.1430,  0.8096])\n",
      "------\n",
      "------\n",
      "Epoch 768, Loss 4.910480, \n",
      "Params:  tensor([  4.5286, -12.5549])\n",
      "Grad:  tensor([-0.1428,  0.8083])\n",
      "------\n",
      "------\n",
      "Epoch 769, Loss 4.903749, \n",
      "Params:  tensor([  4.5301, -12.5630])\n",
      "Grad:  tensor([-0.1426,  0.8069])\n",
      "------\n",
      "------\n",
      "Epoch 770, Loss 4.897040, \n",
      "Params:  tensor([  4.5315, -12.5711])\n",
      "Grad:  tensor([-0.1423,  0.8055])\n",
      "------\n",
      "------\n",
      "Epoch 771, Loss 4.890356, \n",
      "Params:  tensor([  4.5329, -12.5791])\n",
      "Grad:  tensor([-0.1420,  0.8042])\n",
      "------\n",
      "------\n",
      "Epoch 772, Loss 4.883692, \n",
      "Params:  tensor([  4.5343, -12.5871])\n",
      "Grad:  tensor([-0.1418,  0.8028])\n",
      "------\n",
      "------\n",
      "Epoch 773, Loss 4.877052, \n",
      "Params:  tensor([  4.5357, -12.5951])\n",
      "Grad:  tensor([-0.1416,  0.8014])\n",
      "------\n",
      "------\n",
      "Epoch 774, Loss 4.870436, \n",
      "Params:  tensor([  4.5372, -12.6031])\n",
      "Grad:  tensor([-0.1413,  0.8001])\n",
      "------\n",
      "------\n",
      "Epoch 775, Loss 4.863839, \n",
      "Params:  tensor([  4.5386, -12.6111])\n",
      "Grad:  tensor([-0.1411,  0.7987])\n",
      "------\n",
      "------\n",
      "Epoch 776, Loss 4.857268, \n",
      "Params:  tensor([  4.5400, -12.6191])\n",
      "Grad:  tensor([-0.1408,  0.7973])\n",
      "------\n",
      "------\n",
      "Epoch 777, Loss 4.850718, \n",
      "Params:  tensor([  4.5414, -12.6271])\n",
      "Grad:  tensor([-0.1406,  0.7960])\n",
      "------\n",
      "------\n",
      "Epoch 778, Loss 4.844189, \n",
      "Params:  tensor([  4.5428, -12.6350])\n",
      "Grad:  tensor([-0.1404,  0.7946])\n",
      "------\n",
      "------\n",
      "Epoch 779, Loss 4.837683, \n",
      "Params:  tensor([  4.5442, -12.6429])\n",
      "Grad:  tensor([-0.1401,  0.7933])\n",
      "------\n",
      "------\n",
      "Epoch 780, Loss 4.831196, \n",
      "Params:  tensor([  4.5456, -12.6509])\n",
      "Grad:  tensor([-0.1399,  0.7919])\n",
      "------\n",
      "------\n",
      "Epoch 781, Loss 4.824737, \n",
      "Params:  tensor([  4.5470, -12.6588])\n",
      "Grad:  tensor([-0.1397,  0.7906])\n",
      "------\n",
      "------\n",
      "Epoch 782, Loss 4.818298, \n",
      "Params:  tensor([  4.5484, -12.6667])\n",
      "Grad:  tensor([-0.1394,  0.7893])\n",
      "------\n",
      "------\n",
      "Epoch 783, Loss 4.811879, \n",
      "Params:  tensor([  4.5498, -12.6745])\n",
      "Grad:  tensor([-0.1392,  0.7879])\n",
      "------\n",
      "------\n",
      "Epoch 784, Loss 4.805481, \n",
      "Params:  tensor([  4.5512, -12.6824])\n",
      "Grad:  tensor([-0.1389,  0.7866])\n",
      "------\n",
      "------\n",
      "Epoch 785, Loss 4.799106, \n",
      "Params:  tensor([  4.5525, -12.6902])\n",
      "Grad:  tensor([-0.1387,  0.7852])\n",
      "------\n",
      "------\n",
      "Epoch 786, Loss 4.792755, \n",
      "Params:  tensor([  4.5539, -12.6981])\n",
      "Grad:  tensor([-0.1385,  0.7839])\n",
      "------\n",
      "------\n",
      "Epoch 787, Loss 4.786422, \n",
      "Params:  tensor([  4.5553, -12.7059])\n",
      "Grad:  tensor([-0.1383,  0.7826])\n",
      "------\n",
      "------\n",
      "Epoch 788, Loss 4.780112, \n",
      "Params:  tensor([  4.5567, -12.7137])\n",
      "Grad:  tensor([-0.1380,  0.7812])\n",
      "------\n",
      "------\n",
      "Epoch 789, Loss 4.773824, \n",
      "Params:  tensor([  4.5581, -12.7215])\n",
      "Grad:  tensor([-0.1378,  0.7799])\n",
      "------\n",
      "------\n",
      "Epoch 790, Loss 4.767558, \n",
      "Params:  tensor([  4.5594, -12.7293])\n",
      "Grad:  tensor([-0.1375,  0.7786])\n",
      "------\n",
      "------\n",
      "Epoch 791, Loss 4.761312, \n",
      "Params:  tensor([  4.5608, -12.7371])\n",
      "Grad:  tensor([-0.1373,  0.7773])\n",
      "------\n",
      "------\n",
      "Epoch 792, Loss 4.755087, \n",
      "Params:  tensor([  4.5622, -12.7448])\n",
      "Grad:  tensor([-0.1371,  0.7759])\n",
      "------\n",
      "------\n",
      "Epoch 793, Loss 4.748885, \n",
      "Params:  tensor([  4.5636, -12.7526])\n",
      "Grad:  tensor([-0.1368,  0.7746])\n",
      "------\n",
      "------\n",
      "Epoch 794, Loss 4.742700, \n",
      "Params:  tensor([  4.5649, -12.7603])\n",
      "Grad:  tensor([-0.1366,  0.7733])\n",
      "------\n",
      "------\n",
      "Epoch 795, Loss 4.736537, \n",
      "Params:  tensor([  4.5663, -12.7680])\n",
      "Grad:  tensor([-0.1364,  0.7720])\n",
      "------\n",
      "------\n",
      "Epoch 796, Loss 4.730397, \n",
      "Params:  tensor([  4.5677, -12.7758])\n",
      "Grad:  tensor([-0.1361,  0.7707])\n",
      "------\n",
      "------\n",
      "Epoch 797, Loss 4.724279, \n",
      "Params:  tensor([  4.5690, -12.7834])\n",
      "Grad:  tensor([-0.1359,  0.7694])\n",
      "------\n",
      "------\n",
      "Epoch 798, Loss 4.718181, \n",
      "Params:  tensor([  4.5704, -12.7911])\n",
      "Grad:  tensor([-0.1357,  0.7681])\n",
      "------\n",
      "------\n",
      "Epoch 799, Loss 4.712101, \n",
      "Params:  tensor([  4.5717, -12.7988])\n",
      "Grad:  tensor([-0.1354,  0.7668])\n",
      "------\n",
      "------\n",
      "Epoch 800, Loss 4.706046, \n",
      "Params:  tensor([  4.5731, -12.8064])\n",
      "Grad:  tensor([-0.1352,  0.7655])\n",
      "------\n",
      "------\n",
      "Epoch 801, Loss 4.700009, \n",
      "Params:  tensor([  4.5744, -12.8141])\n",
      "Grad:  tensor([-0.1350,  0.7642])\n",
      "------\n",
      "------\n",
      "Epoch 802, Loss 4.693990, \n",
      "Params:  tensor([  4.5758, -12.8217])\n",
      "Grad:  tensor([-0.1347,  0.7629])\n",
      "------\n",
      "------\n",
      "Epoch 803, Loss 4.687995, \n",
      "Params:  tensor([  4.5771, -12.8293])\n",
      "Grad:  tensor([-0.1345,  0.7616])\n",
      "------\n",
      "------\n",
      "Epoch 804, Loss 4.682020, \n",
      "Params:  tensor([  4.5785, -12.8369])\n",
      "Grad:  tensor([-0.1343,  0.7603])\n",
      "------\n",
      "------\n",
      "Epoch 805, Loss 4.676063, \n",
      "Params:  tensor([  4.5798, -12.8445])\n",
      "Grad:  tensor([-0.1341,  0.7590])\n",
      "------\n",
      "------\n",
      "Epoch 806, Loss 4.670130, \n",
      "Params:  tensor([  4.5811, -12.8521])\n",
      "Grad:  tensor([-0.1338,  0.7577])\n",
      "------\n",
      "------\n",
      "Epoch 807, Loss 4.664214, \n",
      "Params:  tensor([  4.5825, -12.8597])\n",
      "Grad:  tensor([-0.1336,  0.7564])\n",
      "------\n",
      "------\n",
      "Epoch 808, Loss 4.658319, \n",
      "Params:  tensor([  4.5838, -12.8672])\n",
      "Grad:  tensor([-0.1334,  0.7551])\n",
      "------\n",
      "------\n",
      "Epoch 809, Loss 4.652445, \n",
      "Params:  tensor([  4.5851, -12.8748])\n",
      "Grad:  tensor([-0.1332,  0.7538])\n",
      "------\n",
      "------\n",
      "Epoch 810, Loss 4.646592, \n",
      "Params:  tensor([  4.5865, -12.8823])\n",
      "Grad:  tensor([-0.1330,  0.7526])\n",
      "------\n",
      "------\n",
      "Epoch 811, Loss 4.640754, \n",
      "Params:  tensor([  4.5878, -12.8898])\n",
      "Grad:  tensor([-0.1327,  0.7513])\n",
      "------\n",
      "------\n",
      "Epoch 812, Loss 4.634938, \n",
      "Params:  tensor([  4.5891, -12.8973])\n",
      "Grad:  tensor([-0.1325,  0.7500])\n",
      "------\n",
      "------\n",
      "Epoch 813, Loss 4.629142, \n",
      "Params:  tensor([  4.5904, -12.9048])\n",
      "Grad:  tensor([-0.1323,  0.7487])\n",
      "------\n",
      "------\n",
      "Epoch 814, Loss 4.623367, \n",
      "Params:  tensor([  4.5918, -12.9123])\n",
      "Grad:  tensor([-0.1320,  0.7475])\n",
      "------\n",
      "------\n",
      "Epoch 815, Loss 4.617611, \n",
      "Params:  tensor([  4.5931, -12.9197])\n",
      "Grad:  tensor([-0.1318,  0.7462])\n",
      "------\n",
      "------\n",
      "Epoch 816, Loss 4.611872, \n",
      "Params:  tensor([  4.5944, -12.9272])\n",
      "Grad:  tensor([-0.1316,  0.7449])\n",
      "------\n",
      "------\n",
      "Epoch 817, Loss 4.606156, \n",
      "Params:  tensor([  4.5957, -12.9346])\n",
      "Grad:  tensor([-0.1314,  0.7437])\n",
      "------\n",
      "------\n",
      "Epoch 818, Loss 4.600458, \n",
      "Params:  tensor([  4.5970, -12.9420])\n",
      "Grad:  tensor([-0.1311,  0.7424])\n",
      "------\n",
      "------\n",
      "Epoch 819, Loss 4.594780, \n",
      "Params:  tensor([  4.5983, -12.9494])\n",
      "Grad:  tensor([-0.1309,  0.7411])\n",
      "------\n",
      "------\n",
      "Epoch 820, Loss 4.589119, \n",
      "Params:  tensor([  4.5996, -12.9568])\n",
      "Grad:  tensor([-0.1307,  0.7399])\n",
      "------\n",
      "------\n",
      "Epoch 821, Loss 4.583479, \n",
      "Params:  tensor([  4.6009, -12.9642])\n",
      "Grad:  tensor([-0.1305,  0.7386])\n",
      "------\n",
      "------\n",
      "Epoch 822, Loss 4.577857, \n",
      "Params:  tensor([  4.6022, -12.9716])\n",
      "Grad:  tensor([-0.1303,  0.7374])\n",
      "------\n",
      "------\n",
      "Epoch 823, Loss 4.572256, \n",
      "Params:  tensor([  4.6035, -12.9790])\n",
      "Grad:  tensor([-0.1300,  0.7361])\n",
      "------\n",
      "------\n",
      "Epoch 824, Loss 4.566675, \n",
      "Params:  tensor([  4.6048, -12.9863])\n",
      "Grad:  tensor([-0.1298,  0.7349])\n",
      "------\n",
      "------\n",
      "Epoch 825, Loss 4.561108, \n",
      "Params:  tensor([  4.6061, -12.9936])\n",
      "Grad:  tensor([-0.1296,  0.7336])\n",
      "------\n",
      "------\n",
      "Epoch 826, Loss 4.555565, \n",
      "Params:  tensor([  4.6074, -13.0010])\n",
      "Grad:  tensor([-0.1294,  0.7324])\n",
      "------\n",
      "------\n",
      "Epoch 827, Loss 4.550039, \n",
      "Params:  tensor([  4.6087, -13.0083])\n",
      "Grad:  tensor([-0.1292,  0.7311])\n",
      "------\n",
      "------\n",
      "Epoch 828, Loss 4.544534, \n",
      "Params:  tensor([  4.6100, -13.0156])\n",
      "Grad:  tensor([-0.1289,  0.7299])\n",
      "------\n",
      "------\n",
      "Epoch 829, Loss 4.539044, \n",
      "Params:  tensor([  4.6113, -13.0229])\n",
      "Grad:  tensor([-0.1287,  0.7286])\n",
      "------\n",
      "------\n",
      "Epoch 830, Loss 4.533575, \n",
      "Params:  tensor([  4.6126, -13.0301])\n",
      "Grad:  tensor([-0.1285,  0.7274])\n",
      "------\n",
      "------\n",
      "Epoch 831, Loss 4.528122, \n",
      "Params:  tensor([  4.6139, -13.0374])\n",
      "Grad:  tensor([-0.1283,  0.7262])\n",
      "------\n",
      "------\n",
      "Epoch 832, Loss 4.522691, \n",
      "Params:  tensor([  4.6152, -13.0446])\n",
      "Grad:  tensor([-0.1280,  0.7249])\n",
      "------\n",
      "------\n",
      "Epoch 833, Loss 4.517276, \n",
      "Params:  tensor([  4.6164, -13.0519])\n",
      "Grad:  tensor([-0.1278,  0.7237])\n",
      "------\n",
      "------\n",
      "Epoch 834, Loss 4.511879, \n",
      "Params:  tensor([  4.6177, -13.0591])\n",
      "Grad:  tensor([-0.1276,  0.7225])\n",
      "------\n",
      "------\n",
      "Epoch 835, Loss 4.506505, \n",
      "Params:  tensor([  4.6190, -13.0663])\n",
      "Grad:  tensor([-0.1274,  0.7212])\n",
      "------\n",
      "------\n",
      "Epoch 836, Loss 4.501141, \n",
      "Params:  tensor([  4.6203, -13.0735])\n",
      "Grad:  tensor([-0.1272,  0.7200])\n",
      "------\n",
      "------\n",
      "Epoch 837, Loss 4.495801, \n",
      "Params:  tensor([  4.6215, -13.0807])\n",
      "Grad:  tensor([-0.1270,  0.7188])\n",
      "------\n",
      "------\n",
      "Epoch 838, Loss 4.490475, \n",
      "Params:  tensor([  4.6228, -13.0879])\n",
      "Grad:  tensor([-0.1268,  0.7176])\n",
      "------\n",
      "------\n",
      "Epoch 839, Loss 4.485169, \n",
      "Params:  tensor([  4.6241, -13.0950])\n",
      "Grad:  tensor([-0.1266,  0.7163])\n",
      "------\n",
      "------\n",
      "Epoch 840, Loss 4.479884, \n",
      "Params:  tensor([  4.6253, -13.1022])\n",
      "Grad:  tensor([-0.1263,  0.7151])\n",
      "------\n",
      "------\n",
      "Epoch 841, Loss 4.474613, \n",
      "Params:  tensor([  4.6266, -13.1093])\n",
      "Grad:  tensor([-0.1261,  0.7139])\n",
      "------\n",
      "------\n",
      "Epoch 842, Loss 4.469364, \n",
      "Params:  tensor([  4.6278, -13.1165])\n",
      "Grad:  tensor([-0.1259,  0.7127])\n",
      "------\n",
      "------\n",
      "Epoch 843, Loss 4.464130, \n",
      "Params:  tensor([  4.6291, -13.1236])\n",
      "Grad:  tensor([-0.1257,  0.7115])\n",
      "------\n",
      "------\n",
      "Epoch 844, Loss 4.458913, \n",
      "Params:  tensor([  4.6304, -13.1307])\n",
      "Grad:  tensor([-0.1255,  0.7103])\n",
      "------\n",
      "------\n",
      "Epoch 845, Loss 4.453716, \n",
      "Params:  tensor([  4.6316, -13.1378])\n",
      "Grad:  tensor([-0.1253,  0.7091])\n",
      "------\n",
      "------\n",
      "Epoch 846, Loss 4.448535, \n",
      "Params:  tensor([  4.6329, -13.1449])\n",
      "Grad:  tensor([-0.1250,  0.7079])\n",
      "------\n",
      "------\n",
      "Epoch 847, Loss 4.443372, \n",
      "Params:  tensor([  4.6341, -13.1519])\n",
      "Grad:  tensor([-0.1249,  0.7067])\n",
      "------\n",
      "------\n",
      "Epoch 848, Loss 4.438226, \n",
      "Params:  tensor([  4.6353, -13.1590])\n",
      "Grad:  tensor([-0.1246,  0.7055])\n",
      "------\n",
      "------\n",
      "Epoch 849, Loss 4.433099, \n",
      "Params:  tensor([  4.6366, -13.1660])\n",
      "Grad:  tensor([-0.1244,  0.7043])\n",
      "------\n",
      "------\n",
      "Epoch 850, Loss 4.427990, \n",
      "Params:  tensor([  4.6378, -13.1730])\n",
      "Grad:  tensor([-0.1242,  0.7031])\n",
      "------\n",
      "------\n",
      "Epoch 851, Loss 4.422897, \n",
      "Params:  tensor([  4.6391, -13.1801])\n",
      "Grad:  tensor([-0.1240,  0.7019])\n",
      "------\n",
      "------\n",
      "Epoch 852, Loss 4.417819, \n",
      "Params:  tensor([  4.6403, -13.1871])\n",
      "Grad:  tensor([-0.1238,  0.7007])\n",
      "------\n",
      "------\n",
      "Epoch 853, Loss 4.412762, \n",
      "Params:  tensor([  4.6415, -13.1941])\n",
      "Grad:  tensor([-0.1236,  0.6995])\n",
      "------\n",
      "------\n",
      "Epoch 854, Loss 4.407721, \n",
      "Params:  tensor([  4.6428, -13.2010])\n",
      "Grad:  tensor([-0.1234,  0.6983])\n",
      "------\n",
      "------\n",
      "Epoch 855, Loss 4.402698, \n",
      "Params:  tensor([  4.6440, -13.2080])\n",
      "Grad:  tensor([-0.1232,  0.6971])\n",
      "------\n",
      "------\n",
      "Epoch 856, Loss 4.397688, \n",
      "Params:  tensor([  4.6452, -13.2150])\n",
      "Grad:  tensor([-0.1229,  0.6959])\n",
      "------\n",
      "------\n",
      "Epoch 857, Loss 4.392697, \n",
      "Params:  tensor([  4.6465, -13.2219])\n",
      "Grad:  tensor([-0.1227,  0.6948])\n",
      "------\n",
      "------\n",
      "Epoch 858, Loss 4.387725, \n",
      "Params:  tensor([  4.6477, -13.2289])\n",
      "Grad:  tensor([-0.1225,  0.6936])\n",
      "------\n",
      "------\n",
      "Epoch 859, Loss 4.382770, \n",
      "Params:  tensor([  4.6489, -13.2358])\n",
      "Grad:  tensor([-0.1223,  0.6924])\n",
      "------\n",
      "------\n",
      "Epoch 860, Loss 4.377828, \n",
      "Params:  tensor([  4.6501, -13.2427])\n",
      "Grad:  tensor([-0.1221,  0.6912])\n",
      "------\n",
      "------\n",
      "Epoch 861, Loss 4.372905, \n",
      "Params:  tensor([  4.6514, -13.2496])\n",
      "Grad:  tensor([-0.1219,  0.6901])\n",
      "------\n",
      "------\n",
      "Epoch 862, Loss 4.368000, \n",
      "Params:  tensor([  4.6526, -13.2565])\n",
      "Grad:  tensor([-0.1217,  0.6889])\n",
      "------\n",
      "------\n",
      "Epoch 863, Loss 4.363111, \n",
      "Params:  tensor([  4.6538, -13.2634])\n",
      "Grad:  tensor([-0.1215,  0.6877])\n",
      "------\n",
      "------\n",
      "Epoch 864, Loss 4.358238, \n",
      "Params:  tensor([  4.6550, -13.2702])\n",
      "Grad:  tensor([-0.1213,  0.6865])\n",
      "------\n",
      "------\n",
      "Epoch 865, Loss 4.353383, \n",
      "Params:  tensor([  4.6562, -13.2771])\n",
      "Grad:  tensor([-0.1211,  0.6854])\n",
      "------\n",
      "------\n",
      "Epoch 866, Loss 4.348542, \n",
      "Params:  tensor([  4.6574, -13.2839])\n",
      "Grad:  tensor([-0.1209,  0.6842])\n",
      "------\n",
      "------\n",
      "Epoch 867, Loss 4.343716, \n",
      "Params:  tensor([  4.6586, -13.2908])\n",
      "Grad:  tensor([-0.1207,  0.6830])\n",
      "------\n",
      "------\n",
      "Epoch 868, Loss 4.338911, \n",
      "Params:  tensor([  4.6598, -13.2976])\n",
      "Grad:  tensor([-0.1205,  0.6819])\n",
      "------\n",
      "------\n",
      "Epoch 869, Loss 4.334120, \n",
      "Params:  tensor([  4.6610, -13.3044])\n",
      "Grad:  tensor([-0.1203,  0.6807])\n",
      "------\n",
      "------\n",
      "Epoch 870, Loss 4.329345, \n",
      "Params:  tensor([  4.6622, -13.3112])\n",
      "Grad:  tensor([-0.1201,  0.6796])\n",
      "------\n",
      "------\n",
      "Epoch 871, Loss 4.324588, \n",
      "Params:  tensor([  4.6634, -13.3180])\n",
      "Grad:  tensor([-0.1198,  0.6784])\n",
      "------\n",
      "------\n",
      "Epoch 872, Loss 4.319846, \n",
      "Params:  tensor([  4.6646, -13.3247])\n",
      "Grad:  tensor([-0.1196,  0.6773])\n",
      "------\n",
      "------\n",
      "Epoch 873, Loss 4.315117, \n",
      "Params:  tensor([  4.6658, -13.3315])\n",
      "Grad:  tensor([-0.1195,  0.6761])\n",
      "------\n",
      "------\n",
      "Epoch 874, Loss 4.310409, \n",
      "Params:  tensor([  4.6670, -13.3382])\n",
      "Grad:  tensor([-0.1192,  0.6750])\n",
      "------\n",
      "------\n",
      "Epoch 875, Loss 4.305714, \n",
      "Params:  tensor([  4.6682, -13.3450])\n",
      "Grad:  tensor([-0.1190,  0.6738])\n",
      "------\n",
      "------\n",
      "Epoch 876, Loss 4.301036, \n",
      "Params:  tensor([  4.6694, -13.3517])\n",
      "Grad:  tensor([-0.1188,  0.6727])\n",
      "------\n",
      "------\n",
      "Epoch 877, Loss 4.296376, \n",
      "Params:  tensor([  4.6706, -13.3584])\n",
      "Grad:  tensor([-0.1186,  0.6715])\n",
      "------\n",
      "------\n",
      "Epoch 878, Loss 4.291727, \n",
      "Params:  tensor([  4.6718, -13.3651])\n",
      "Grad:  tensor([-0.1184,  0.6704])\n",
      "------\n",
      "------\n",
      "Epoch 879, Loss 4.287098, \n",
      "Params:  tensor([  4.6730, -13.3718])\n",
      "Grad:  tensor([-0.1182,  0.6693])\n",
      "------\n",
      "------\n",
      "Epoch 880, Loss 4.282482, \n",
      "Params:  tensor([  4.6741, -13.3785])\n",
      "Grad:  tensor([-0.1180,  0.6681])\n",
      "------\n",
      "------\n",
      "Epoch 881, Loss 4.277882, \n",
      "Params:  tensor([  4.6753, -13.3852])\n",
      "Grad:  tensor([-0.1178,  0.6670])\n",
      "------\n",
      "------\n",
      "Epoch 882, Loss 4.273299, \n",
      "Params:  tensor([  4.6765, -13.3918])\n",
      "Grad:  tensor([-0.1176,  0.6658])\n",
      "------\n",
      "------\n",
      "Epoch 883, Loss 4.268732, \n",
      "Params:  tensor([  4.6777, -13.3985])\n",
      "Grad:  tensor([-0.1174,  0.6647])\n",
      "------\n",
      "------\n",
      "Epoch 884, Loss 4.264178, \n",
      "Params:  tensor([  4.6788, -13.4051])\n",
      "Grad:  tensor([-0.1172,  0.6636])\n",
      "------\n",
      "------\n",
      "Epoch 885, Loss 4.259643, \n",
      "Params:  tensor([  4.6800, -13.4117])\n",
      "Grad:  tensor([-0.1170,  0.6625])\n",
      "------\n",
      "------\n",
      "Epoch 886, Loss 4.255120, \n",
      "Params:  tensor([  4.6812, -13.4184])\n",
      "Grad:  tensor([-0.1168,  0.6613])\n",
      "------\n",
      "------\n",
      "Epoch 887, Loss 4.250614, \n",
      "Params:  tensor([  4.6823, -13.4250])\n",
      "Grad:  tensor([-0.1166,  0.6602])\n",
      "------\n",
      "------\n",
      "Epoch 888, Loss 4.246124, \n",
      "Params:  tensor([  4.6835, -13.4316])\n",
      "Grad:  tensor([-0.1164,  0.6591])\n",
      "------\n",
      "------\n",
      "Epoch 889, Loss 4.241648, \n",
      "Params:  tensor([  4.6847, -13.4381])\n",
      "Grad:  tensor([-0.1162,  0.6580])\n",
      "------\n",
      "------\n",
      "Epoch 890, Loss 4.237185, \n",
      "Params:  tensor([  4.6858, -13.4447])\n",
      "Grad:  tensor([-0.1160,  0.6569])\n",
      "------\n",
      "------\n",
      "Epoch 891, Loss 4.232740, \n",
      "Params:  tensor([  4.6870, -13.4513])\n",
      "Grad:  tensor([-0.1158,  0.6557])\n",
      "------\n",
      "------\n",
      "Epoch 892, Loss 4.228308, \n",
      "Params:  tensor([  4.6881, -13.4578])\n",
      "Grad:  tensor([-0.1157,  0.6546])\n",
      "------\n",
      "------\n",
      "Epoch 893, Loss 4.223895, \n",
      "Params:  tensor([  4.6893, -13.4643])\n",
      "Grad:  tensor([-0.1154,  0.6535])\n",
      "------\n",
      "------\n",
      "Epoch 894, Loss 4.219494, \n",
      "Params:  tensor([  4.6904, -13.4709])\n",
      "Grad:  tensor([-0.1153,  0.6524])\n",
      "------\n",
      "------\n",
      "Epoch 895, Loss 4.215109, \n",
      "Params:  tensor([  4.6916, -13.4774])\n",
      "Grad:  tensor([-0.1151,  0.6513])\n",
      "------\n",
      "------\n",
      "Epoch 896, Loss 4.210737, \n",
      "Params:  tensor([  4.6927, -13.4839])\n",
      "Grad:  tensor([-0.1148,  0.6502])\n",
      "------\n",
      "------\n",
      "Epoch 897, Loss 4.206383, \n",
      "Params:  tensor([  4.6939, -13.4904])\n",
      "Grad:  tensor([-0.1147,  0.6491])\n",
      "------\n",
      "------\n",
      "Epoch 898, Loss 4.202043, \n",
      "Params:  tensor([  4.6950, -13.4968])\n",
      "Grad:  tensor([-0.1145,  0.6480])\n",
      "------\n",
      "------\n",
      "Epoch 899, Loss 4.197715, \n",
      "Params:  tensor([  4.6962, -13.5033])\n",
      "Grad:  tensor([-0.1143,  0.6469])\n",
      "------\n",
      "------\n",
      "Epoch 900, Loss 4.193405, \n",
      "Params:  tensor([  4.6973, -13.5098])\n",
      "Grad:  tensor([-0.1141,  0.6458])\n",
      "------\n",
      "------\n",
      "Epoch 901, Loss 4.189108, \n",
      "Params:  tensor([  4.6985, -13.5162])\n",
      "Grad:  tensor([-0.1139,  0.6447])\n",
      "------\n",
      "------\n",
      "Epoch 902, Loss 4.184825, \n",
      "Params:  tensor([  4.6996, -13.5227])\n",
      "Grad:  tensor([-0.1137,  0.6436])\n",
      "------\n",
      "------\n",
      "Epoch 903, Loss 4.180559, \n",
      "Params:  tensor([  4.7007, -13.5291])\n",
      "Grad:  tensor([-0.1135,  0.6425])\n",
      "------\n",
      "------\n",
      "Epoch 904, Loss 4.176305, \n",
      "Params:  tensor([  4.7019, -13.5355])\n",
      "Grad:  tensor([-0.1133,  0.6414])\n",
      "------\n",
      "------\n",
      "Epoch 905, Loss 4.172065, \n",
      "Params:  tensor([  4.7030, -13.5419])\n",
      "Grad:  tensor([-0.1131,  0.6403])\n",
      "------\n",
      "------\n",
      "Epoch 906, Loss 4.167842, \n",
      "Params:  tensor([  4.7041, -13.5483])\n",
      "Grad:  tensor([-0.1129,  0.6392])\n",
      "------\n",
      "------\n",
      "Epoch 907, Loss 4.163630, \n",
      "Params:  tensor([  4.7053, -13.5547])\n",
      "Grad:  tensor([-0.1127,  0.6381])\n",
      "------\n",
      "------\n",
      "Epoch 908, Loss 4.159436, \n",
      "Params:  tensor([  4.7064, -13.5610])\n",
      "Grad:  tensor([-0.1125,  0.6371])\n",
      "------\n",
      "------\n",
      "Epoch 909, Loss 4.155253, \n",
      "Params:  tensor([  4.7075, -13.5674])\n",
      "Grad:  tensor([-0.1124,  0.6360])\n",
      "------\n",
      "------\n",
      "Epoch 910, Loss 4.151086, \n",
      "Params:  tensor([  4.7086, -13.5738])\n",
      "Grad:  tensor([-0.1122,  0.6349])\n",
      "------\n",
      "------\n",
      "Epoch 911, Loss 4.146934, \n",
      "Params:  tensor([  4.7097, -13.5801])\n",
      "Grad:  tensor([-0.1120,  0.6338])\n",
      "------\n",
      "------\n",
      "Epoch 912, Loss 4.142794, \n",
      "Params:  tensor([  4.7109, -13.5864])\n",
      "Grad:  tensor([-0.1118,  0.6327])\n",
      "------\n",
      "------\n",
      "Epoch 913, Loss 4.138669, \n",
      "Params:  tensor([  4.7120, -13.5927])\n",
      "Grad:  tensor([-0.1116,  0.6317])\n",
      "------\n",
      "------\n",
      "Epoch 914, Loss 4.134559, \n",
      "Params:  tensor([  4.7131, -13.5990])\n",
      "Grad:  tensor([-0.1114,  0.6306])\n",
      "------\n",
      "------\n",
      "Epoch 915, Loss 4.130465, \n",
      "Params:  tensor([  4.7142, -13.6053])\n",
      "Grad:  tensor([-0.1112,  0.6295])\n",
      "------\n",
      "------\n",
      "Epoch 916, Loss 4.126378, \n",
      "Params:  tensor([  4.7153, -13.6116])\n",
      "Grad:  tensor([-0.1110,  0.6284])\n",
      "------\n",
      "------\n",
      "Epoch 917, Loss 4.122310, \n",
      "Params:  tensor([  4.7164, -13.6179])\n",
      "Grad:  tensor([-0.1108,  0.6274])\n",
      "------\n",
      "------\n",
      "Epoch 918, Loss 4.118253, \n",
      "Params:  tensor([  4.7175, -13.6242])\n",
      "Grad:  tensor([-0.1107,  0.6263])\n",
      "------\n",
      "------\n",
      "Epoch 919, Loss 4.114213, \n",
      "Params:  tensor([  4.7186, -13.6304])\n",
      "Grad:  tensor([-0.1104,  0.6253])\n",
      "------\n",
      "------\n",
      "Epoch 920, Loss 4.110184, \n",
      "Params:  tensor([  4.7197, -13.6367])\n",
      "Grad:  tensor([-0.1103,  0.6242])\n",
      "------\n",
      "------\n",
      "Epoch 921, Loss 4.106170, \n",
      "Params:  tensor([  4.7208, -13.6429])\n",
      "Grad:  tensor([-0.1101,  0.6231])\n",
      "------\n",
      "------\n",
      "Epoch 922, Loss 4.102171, \n",
      "Params:  tensor([  4.7219, -13.6491])\n",
      "Grad:  tensor([-0.1099,  0.6221])\n",
      "------\n",
      "------\n",
      "Epoch 923, Loss 4.098181, \n",
      "Params:  tensor([  4.7230, -13.6553])\n",
      "Grad:  tensor([-0.1097,  0.6210])\n",
      "------\n",
      "------\n",
      "Epoch 924, Loss 4.094209, \n",
      "Params:  tensor([  4.7241, -13.6615])\n",
      "Grad:  tensor([-0.1095,  0.6200])\n",
      "------\n",
      "------\n",
      "Epoch 925, Loss 4.090250, \n",
      "Params:  tensor([  4.7252, -13.6677])\n",
      "Grad:  tensor([-0.1093,  0.6189])\n",
      "------\n",
      "------\n",
      "Epoch 926, Loss 4.086300, \n",
      "Params:  tensor([  4.7263, -13.6739])\n",
      "Grad:  tensor([-0.1091,  0.6179])\n",
      "------\n",
      "------\n",
      "Epoch 927, Loss 4.082366, \n",
      "Params:  tensor([  4.7274, -13.6800])\n",
      "Grad:  tensor([-0.1090,  0.6168])\n",
      "------\n",
      "------\n",
      "Epoch 928, Loss 4.078448, \n",
      "Params:  tensor([  4.7285, -13.6862])\n",
      "Grad:  tensor([-0.1088,  0.6158])\n",
      "------\n",
      "------\n",
      "Epoch 929, Loss 4.074540, \n",
      "Params:  tensor([  4.7296, -13.6924])\n",
      "Grad:  tensor([-0.1086,  0.6147])\n",
      "------\n",
      "------\n",
      "Epoch 930, Loss 4.070650, \n",
      "Params:  tensor([  4.7307, -13.6985])\n",
      "Grad:  tensor([-0.1084,  0.6137])\n",
      "------\n",
      "------\n",
      "Epoch 931, Loss 4.066769, \n",
      "Params:  tensor([  4.7317, -13.7046])\n",
      "Grad:  tensor([-0.1082,  0.6126])\n",
      "------\n",
      "------\n",
      "Epoch 932, Loss 4.062900, \n",
      "Params:  tensor([  4.7328, -13.7107])\n",
      "Grad:  tensor([-0.1080,  0.6116])\n",
      "------\n",
      "------\n",
      "Epoch 933, Loss 4.059047, \n",
      "Params:  tensor([  4.7339, -13.7168])\n",
      "Grad:  tensor([-0.1079,  0.6105])\n",
      "------\n",
      "------\n",
      "Epoch 934, Loss 4.055204, \n",
      "Params:  tensor([  4.7350, -13.7229])\n",
      "Grad:  tensor([-0.1077,  0.6095])\n",
      "------\n",
      "------\n",
      "Epoch 935, Loss 4.051378, \n",
      "Params:  tensor([  4.7360, -13.7290])\n",
      "Grad:  tensor([-0.1075,  0.6085])\n",
      "------\n",
      "------\n",
      "Epoch 936, Loss 4.047564, \n",
      "Params:  tensor([  4.7371, -13.7351])\n",
      "Grad:  tensor([-0.1073,  0.6074])\n",
      "------\n",
      "------\n",
      "Epoch 937, Loss 4.043762, \n",
      "Params:  tensor([  4.7382, -13.7412])\n",
      "Grad:  tensor([-0.1071,  0.6064])\n",
      "------\n",
      "------\n",
      "Epoch 938, Loss 4.039972, \n",
      "Params:  tensor([  4.7393, -13.7472])\n",
      "Grad:  tensor([-0.1069,  0.6054])\n",
      "------\n",
      "------\n",
      "Epoch 939, Loss 4.036197, \n",
      "Params:  tensor([  4.7403, -13.7533])\n",
      "Grad:  tensor([-0.1068,  0.6043])\n",
      "------\n",
      "------\n",
      "Epoch 940, Loss 4.032433, \n",
      "Params:  tensor([  4.7414, -13.7593])\n",
      "Grad:  tensor([-0.1066,  0.6033])\n",
      "------\n",
      "------\n",
      "Epoch 941, Loss 4.028685, \n",
      "Params:  tensor([  4.7425, -13.7653])\n",
      "Grad:  tensor([-0.1064,  0.6023])\n",
      "------\n",
      "------\n",
      "Epoch 942, Loss 4.024947, \n",
      "Params:  tensor([  4.7435, -13.7713])\n",
      "Grad:  tensor([-0.1062,  0.6013])\n",
      "------\n",
      "------\n",
      "Epoch 943, Loss 4.021221, \n",
      "Params:  tensor([  4.7446, -13.7773])\n",
      "Grad:  tensor([-0.1060,  0.6003])\n",
      "------\n",
      "------\n",
      "Epoch 944, Loss 4.017508, \n",
      "Params:  tensor([  4.7456, -13.7833])\n",
      "Grad:  tensor([-0.1058,  0.5992])\n",
      "------\n",
      "------\n",
      "Epoch 945, Loss 4.013809, \n",
      "Params:  tensor([  4.7467, -13.7893])\n",
      "Grad:  tensor([-0.1057,  0.5982])\n",
      "------\n",
      "------\n",
      "Epoch 946, Loss 4.010123, \n",
      "Params:  tensor([  4.7478, -13.7953])\n",
      "Grad:  tensor([-0.1055,  0.5972])\n",
      "------\n",
      "------\n",
      "Epoch 947, Loss 4.006446, \n",
      "Params:  tensor([  4.7488, -13.8012])\n",
      "Grad:  tensor([-0.1053,  0.5962])\n",
      "------\n",
      "------\n",
      "Epoch 948, Loss 4.002786, \n",
      "Params:  tensor([  4.7499, -13.8072])\n",
      "Grad:  tensor([-0.1051,  0.5952])\n",
      "------\n",
      "------\n",
      "Epoch 949, Loss 3.999135, \n",
      "Params:  tensor([  4.7509, -13.8131])\n",
      "Grad:  tensor([-0.1050,  0.5942])\n",
      "------\n",
      "------\n",
      "Epoch 950, Loss 3.995498, \n",
      "Params:  tensor([  4.7520, -13.8191])\n",
      "Grad:  tensor([-0.1048,  0.5931])\n",
      "------\n",
      "------\n",
      "Epoch 951, Loss 3.991874, \n",
      "Params:  tensor([  4.7530, -13.8250])\n",
      "Grad:  tensor([-0.1046,  0.5921])\n",
      "------\n",
      "------\n",
      "Epoch 952, Loss 3.988261, \n",
      "Params:  tensor([  4.7540, -13.8309])\n",
      "Grad:  tensor([-0.1044,  0.5911])\n",
      "------\n",
      "------\n",
      "Epoch 953, Loss 3.984660, \n",
      "Params:  tensor([  4.7551, -13.8368])\n",
      "Grad:  tensor([-0.1042,  0.5901])\n",
      "------\n",
      "------\n",
      "Epoch 954, Loss 3.981073, \n",
      "Params:  tensor([  4.7561, -13.8427])\n",
      "Grad:  tensor([-0.1041,  0.5891])\n",
      "------\n",
      "------\n",
      "Epoch 955, Loss 3.977496, \n",
      "Params:  tensor([  4.7572, -13.8486])\n",
      "Grad:  tensor([-0.1039,  0.5881])\n",
      "------\n",
      "------\n",
      "Epoch 956, Loss 3.973931, \n",
      "Params:  tensor([  4.7582, -13.8544])\n",
      "Grad:  tensor([-0.1037,  0.5871])\n",
      "------\n",
      "------\n",
      "Epoch 957, Loss 3.970381, \n",
      "Params:  tensor([  4.7592, -13.8603])\n",
      "Grad:  tensor([-0.1035,  0.5861])\n",
      "------\n",
      "------\n",
      "Epoch 958, Loss 3.966841, \n",
      "Params:  tensor([  4.7603, -13.8661])\n",
      "Grad:  tensor([-0.1034,  0.5851])\n",
      "------\n",
      "------\n",
      "Epoch 959, Loss 3.963313, \n",
      "Params:  tensor([  4.7613, -13.8720])\n",
      "Grad:  tensor([-0.1032,  0.5841])\n",
      "------\n",
      "------\n",
      "Epoch 960, Loss 3.959796, \n",
      "Params:  tensor([  4.7623, -13.8778])\n",
      "Grad:  tensor([-0.1030,  0.5831])\n",
      "------\n",
      "------\n",
      "Epoch 961, Loss 3.956295, \n",
      "Params:  tensor([  4.7634, -13.8836])\n",
      "Grad:  tensor([-0.1028,  0.5822])\n",
      "------\n",
      "------\n",
      "Epoch 962, Loss 3.952801, \n",
      "Params:  tensor([  4.7644, -13.8895])\n",
      "Grad:  tensor([-0.1026,  0.5812])\n",
      "------\n",
      "------\n",
      "Epoch 963, Loss 3.949323, \n",
      "Params:  tensor([  4.7654, -13.8953])\n",
      "Grad:  tensor([-0.1025,  0.5802])\n",
      "------\n",
      "------\n",
      "Epoch 964, Loss 3.945855, \n",
      "Params:  tensor([  4.7664, -13.9010])\n",
      "Grad:  tensor([-0.1023,  0.5792])\n",
      "------\n",
      "------\n",
      "Epoch 965, Loss 3.942398, \n",
      "Params:  tensor([  4.7675, -13.9068])\n",
      "Grad:  tensor([-0.1021,  0.5782])\n",
      "------\n",
      "------\n",
      "Epoch 966, Loss 3.938954, \n",
      "Params:  tensor([  4.7685, -13.9126])\n",
      "Grad:  tensor([-0.1020,  0.5772])\n",
      "------\n",
      "------\n",
      "Epoch 967, Loss 3.935520, \n",
      "Params:  tensor([  4.7695, -13.9184])\n",
      "Grad:  tensor([-0.1018,  0.5762])\n",
      "------\n",
      "------\n",
      "Epoch 968, Loss 3.932096, \n",
      "Params:  tensor([  4.7705, -13.9241])\n",
      "Grad:  tensor([-0.1016,  0.5753])\n",
      "------\n",
      "------\n",
      "Epoch 969, Loss 3.928688, \n",
      "Params:  tensor([  4.7715, -13.9299])\n",
      "Grad:  tensor([-0.1015,  0.5743])\n",
      "------\n",
      "------\n",
      "Epoch 970, Loss 3.925292, \n",
      "Params:  tensor([  4.7725, -13.9356])\n",
      "Grad:  tensor([-0.1013,  0.5733])\n",
      "------\n",
      "------\n",
      "Epoch 971, Loss 3.921906, \n",
      "Params:  tensor([  4.7736, -13.9413])\n",
      "Grad:  tensor([-0.1011,  0.5723])\n",
      "------\n",
      "------\n",
      "Epoch 972, Loss 3.918527, \n",
      "Params:  tensor([  4.7746, -13.9470])\n",
      "Grad:  tensor([-0.1009,  0.5714])\n",
      "------\n",
      "------\n",
      "Epoch 973, Loss 3.915166, \n",
      "Params:  tensor([  4.7756, -13.9527])\n",
      "Grad:  tensor([-0.1008,  0.5704])\n",
      "------\n",
      "------\n",
      "Epoch 974, Loss 3.911815, \n",
      "Params:  tensor([  4.7766, -13.9584])\n",
      "Grad:  tensor([-0.1006,  0.5694])\n",
      "------\n",
      "------\n",
      "Epoch 975, Loss 3.908474, \n",
      "Params:  tensor([  4.7776, -13.9641])\n",
      "Grad:  tensor([-0.1004,  0.5685])\n",
      "------\n",
      "------\n",
      "Epoch 976, Loss 3.905143, \n",
      "Params:  tensor([  4.7786, -13.9698])\n",
      "Grad:  tensor([-0.1003,  0.5675])\n",
      "------\n",
      "------\n",
      "Epoch 977, Loss 3.901825, \n",
      "Params:  tensor([  4.7796, -13.9755])\n",
      "Grad:  tensor([-0.1001,  0.5665])\n",
      "------\n",
      "------\n",
      "Epoch 978, Loss 3.898517, \n",
      "Params:  tensor([  4.7806, -13.9811])\n",
      "Grad:  tensor([-0.0999,  0.5656])\n",
      "------\n",
      "------\n",
      "Epoch 979, Loss 3.895222, \n",
      "Params:  tensor([  4.7816, -13.9868])\n",
      "Grad:  tensor([-0.0997,  0.5646])\n",
      "------\n",
      "------\n",
      "Epoch 980, Loss 3.891935, \n",
      "Params:  tensor([  4.7826, -13.9924])\n",
      "Grad:  tensor([-0.0996,  0.5637])\n",
      "------\n",
      "------\n",
      "Epoch 981, Loss 3.888664, \n",
      "Params:  tensor([  4.7836, -13.9980])\n",
      "Grad:  tensor([-0.0994,  0.5627])\n",
      "------\n",
      "------\n",
      "Epoch 982, Loss 3.885401, \n",
      "Params:  tensor([  4.7846, -14.0036])\n",
      "Grad:  tensor([-0.0992,  0.5617])\n",
      "------\n",
      "------\n",
      "Epoch 983, Loss 3.882150, \n",
      "Params:  tensor([  4.7856, -14.0092])\n",
      "Grad:  tensor([-0.0991,  0.5608])\n",
      "------\n",
      "------\n",
      "Epoch 984, Loss 3.878910, \n",
      "Params:  tensor([  4.7865, -14.0148])\n",
      "Grad:  tensor([-0.0989,  0.5598])\n",
      "------\n",
      "------\n",
      "Epoch 985, Loss 3.875680, \n",
      "Params:  tensor([  4.7875, -14.0204])\n",
      "Grad:  tensor([-0.0987,  0.5589])\n",
      "------\n",
      "------\n",
      "Epoch 986, Loss 3.872463, \n",
      "Params:  tensor([  4.7885, -14.0260])\n",
      "Grad:  tensor([-0.0986,  0.5579])\n",
      "------\n",
      "------\n",
      "Epoch 987, Loss 3.869256, \n",
      "Params:  tensor([  4.7895, -14.0316])\n",
      "Grad:  tensor([-0.0984,  0.5570])\n",
      "------\n",
      "------\n",
      "Epoch 988, Loss 3.866060, \n",
      "Params:  tensor([  4.7905, -14.0371])\n",
      "Grad:  tensor([-0.0982,  0.5560])\n",
      "------\n",
      "------\n",
      "Epoch 989, Loss 3.862872, \n",
      "Params:  tensor([  4.7915, -14.0427])\n",
      "Grad:  tensor([-0.0981,  0.5551])\n",
      "------\n",
      "------\n",
      "Epoch 990, Loss 3.859699, \n",
      "Params:  tensor([  4.7924, -14.0482])\n",
      "Grad:  tensor([-0.0979,  0.5541])\n",
      "------\n",
      "------\n",
      "Epoch 991, Loss 3.856535, \n",
      "Params:  tensor([  4.7934, -14.0538])\n",
      "Grad:  tensor([-0.0978,  0.5532])\n",
      "------\n",
      "------\n",
      "Epoch 992, Loss 3.853381, \n",
      "Params:  tensor([  4.7944, -14.0593])\n",
      "Grad:  tensor([-0.0976,  0.5523])\n",
      "------\n",
      "------\n",
      "Epoch 993, Loss 3.850237, \n",
      "Params:  tensor([  4.7954, -14.0648])\n",
      "Grad:  tensor([-0.0974,  0.5513])\n",
      "------\n",
      "------\n",
      "Epoch 994, Loss 3.847109, \n",
      "Params:  tensor([  4.7963, -14.0703])\n",
      "Grad:  tensor([-0.0973,  0.5504])\n",
      "------\n",
      "------\n",
      "Epoch 995, Loss 3.843984, \n",
      "Params:  tensor([  4.7973, -14.0758])\n",
      "Grad:  tensor([-0.0971,  0.5495])\n",
      "------\n",
      "------\n",
      "Epoch 996, Loss 3.840876, \n",
      "Params:  tensor([  4.7983, -14.0813])\n",
      "Grad:  tensor([-0.0969,  0.5485])\n",
      "------\n",
      "------\n",
      "Epoch 997, Loss 3.837775, \n",
      "Params:  tensor([  4.7992, -14.0868])\n",
      "Grad:  tensor([-0.0967,  0.5476])\n",
      "------\n",
      "------\n",
      "Epoch 998, Loss 3.834686, \n",
      "Params:  tensor([  4.8002, -14.0922])\n",
      "Grad:  tensor([-0.0966,  0.5467])\n",
      "------\n",
      "------\n",
      "Epoch 999, Loss 3.831606, \n",
      "Params:  tensor([  4.8012, -14.0977])\n",
      "Grad:  tensor([-0.0964,  0.5457])\n",
      "------\n",
      "------\n",
      "Epoch 1000, Loss 3.828538, \n",
      "Params:  tensor([  4.8021, -14.1031])\n",
      "Grad:  tensor([-0.0962,  0.5448])\n",
      "------\n",
      "------\n",
      "Epoch 1001, Loss 3.825483, \n",
      "Params:  tensor([  4.8031, -14.1086])\n",
      "Grad:  tensor([-0.0961,  0.5439])\n",
      "------\n",
      "------\n",
      "Epoch 1002, Loss 3.822433, \n",
      "Params:  tensor([  4.8041, -14.1140])\n",
      "Grad:  tensor([-0.0959,  0.5430])\n",
      "------\n",
      "------\n",
      "Epoch 1003, Loss 3.819398, \n",
      "Params:  tensor([  4.8050, -14.1194])\n",
      "Grad:  tensor([-0.0957,  0.5420])\n",
      "------\n",
      "------\n",
      "Epoch 1004, Loss 3.816369, \n",
      "Params:  tensor([  4.8060, -14.1248])\n",
      "Grad:  tensor([-0.0956,  0.5411])\n",
      "------\n",
      "------\n",
      "Epoch 1005, Loss 3.813350, \n",
      "Params:  tensor([  4.8069, -14.1302])\n",
      "Grad:  tensor([-0.0954,  0.5402])\n",
      "------\n",
      "------\n",
      "Epoch 1006, Loss 3.810344, \n",
      "Params:  tensor([  4.8079, -14.1356])\n",
      "Grad:  tensor([-0.0953,  0.5393])\n",
      "------\n",
      "------\n",
      "Epoch 1007, Loss 3.807348, \n",
      "Params:  tensor([  4.8088, -14.1410])\n",
      "Grad:  tensor([-0.0951,  0.5384])\n",
      "------\n",
      "------\n",
      "Epoch 1008, Loss 3.804360, \n",
      "Params:  tensor([  4.8098, -14.1464])\n",
      "Grad:  tensor([-0.0949,  0.5375])\n",
      "------\n",
      "------\n",
      "Epoch 1009, Loss 3.801384, \n",
      "Params:  tensor([  4.8107, -14.1518])\n",
      "Grad:  tensor([-0.0948,  0.5365])\n",
      "------\n",
      "------\n",
      "Epoch 1010, Loss 3.798421, \n",
      "Params:  tensor([  4.8117, -14.1571])\n",
      "Grad:  tensor([-0.0946,  0.5356])\n",
      "------\n",
      "------\n",
      "Epoch 1011, Loss 3.795465, \n",
      "Params:  tensor([  4.8126, -14.1625])\n",
      "Grad:  tensor([-0.0945,  0.5347])\n",
      "------\n",
      "------\n",
      "Epoch 1012, Loss 3.792518, \n",
      "Params:  tensor([  4.8136, -14.1678])\n",
      "Grad:  tensor([-0.0943,  0.5338])\n",
      "------\n",
      "------\n",
      "Epoch 1013, Loss 3.789584, \n",
      "Params:  tensor([  4.8145, -14.1731])\n",
      "Grad:  tensor([-0.0942,  0.5329])\n",
      "------\n",
      "------\n",
      "Epoch 1014, Loss 3.786658, \n",
      "Params:  tensor([  4.8154, -14.1784])\n",
      "Grad:  tensor([-0.0940,  0.5320])\n",
      "------\n",
      "------\n",
      "Epoch 1015, Loss 3.783740, \n",
      "Params:  tensor([  4.8164, -14.1838])\n",
      "Grad:  tensor([-0.0938,  0.5311])\n",
      "------\n",
      "------\n",
      "Epoch 1016, Loss 3.780832, \n",
      "Params:  tensor([  4.8173, -14.1891])\n",
      "Grad:  tensor([-0.0937,  0.5302])\n",
      "------\n",
      "------\n",
      "Epoch 1017, Loss 3.777939, \n",
      "Params:  tensor([  4.8183, -14.1943])\n",
      "Grad:  tensor([-0.0935,  0.5293])\n",
      "------\n",
      "------\n",
      "Epoch 1018, Loss 3.775053, \n",
      "Params:  tensor([  4.8192, -14.1996])\n",
      "Grad:  tensor([-0.0933,  0.5284])\n",
      "------\n",
      "------\n",
      "Epoch 1019, Loss 3.772173, \n",
      "Params:  tensor([  4.8201, -14.2049])\n",
      "Grad:  tensor([-0.0932,  0.5275])\n",
      "------\n",
      "------\n",
      "Epoch 1020, Loss 3.769311, \n",
      "Params:  tensor([  4.8210, -14.2102])\n",
      "Grad:  tensor([-0.0930,  0.5266])\n",
      "------\n",
      "------\n",
      "Epoch 1021, Loss 3.766450, \n",
      "Params:  tensor([  4.8220, -14.2154])\n",
      "Grad:  tensor([-0.0929,  0.5257])\n",
      "------\n",
      "------\n",
      "Epoch 1022, Loss 3.763602, \n",
      "Params:  tensor([  4.8229, -14.2207])\n",
      "Grad:  tensor([-0.0927,  0.5248])\n",
      "------\n",
      "------\n",
      "Epoch 1023, Loss 3.760766, \n",
      "Params:  tensor([  4.8238, -14.2259])\n",
      "Grad:  tensor([-0.0926,  0.5239])\n",
      "------\n",
      "------\n",
      "Epoch 1024, Loss 3.757936, \n",
      "Params:  tensor([  4.8248, -14.2311])\n",
      "Grad:  tensor([-0.0924,  0.5230])\n",
      "------\n",
      "------\n",
      "Epoch 1025, Loss 3.755118, \n",
      "Params:  tensor([  4.8257, -14.2364])\n",
      "Grad:  tensor([-0.0922,  0.5221])\n",
      "------\n",
      "------\n",
      "Epoch 1026, Loss 3.752309, \n",
      "Params:  tensor([  4.8266, -14.2416])\n",
      "Grad:  tensor([-0.0921,  0.5213])\n",
      "------\n",
      "------\n",
      "Epoch 1027, Loss 3.749511, \n",
      "Params:  tensor([  4.8275, -14.2468])\n",
      "Grad:  tensor([-0.0919,  0.5204])\n",
      "------\n",
      "------\n",
      "Epoch 1028, Loss 3.746722, \n",
      "Params:  tensor([  4.8284, -14.2520])\n",
      "Grad:  tensor([-0.0918,  0.5195])\n",
      "------\n",
      "------\n",
      "Epoch 1029, Loss 3.743940, \n",
      "Params:  tensor([  4.8293, -14.2572])\n",
      "Grad:  tensor([-0.0916,  0.5186])\n",
      "------\n",
      "------\n",
      "Epoch 1030, Loss 3.741169, \n",
      "Params:  tensor([  4.8303, -14.2623])\n",
      "Grad:  tensor([-0.0915,  0.5177])\n",
      "------\n",
      "------\n",
      "Epoch 1031, Loss 3.738407, \n",
      "Params:  tensor([  4.8312, -14.2675])\n",
      "Grad:  tensor([-0.0913,  0.5168])\n",
      "------\n",
      "------\n",
      "Epoch 1032, Loss 3.735656, \n",
      "Params:  tensor([  4.8321, -14.2727])\n",
      "Grad:  tensor([-0.0912,  0.5160])\n",
      "------\n",
      "------\n",
      "Epoch 1033, Loss 3.732914, \n",
      "Params:  tensor([  4.8330, -14.2778])\n",
      "Grad:  tensor([-0.0910,  0.5151])\n",
      "------\n",
      "------\n",
      "Epoch 1034, Loss 3.730181, \n",
      "Params:  tensor([  4.8339, -14.2830])\n",
      "Grad:  tensor([-0.0908,  0.5142])\n",
      "------\n",
      "------\n",
      "Epoch 1035, Loss 3.727456, \n",
      "Params:  tensor([  4.8348, -14.2881])\n",
      "Grad:  tensor([-0.0907,  0.5133])\n",
      "------\n",
      "------\n",
      "Epoch 1036, Loss 3.724740, \n",
      "Params:  tensor([  4.8357, -14.2932])\n",
      "Grad:  tensor([-0.0905,  0.5125])\n",
      "------\n",
      "------\n",
      "Epoch 1037, Loss 3.722034, \n",
      "Params:  tensor([  4.8366, -14.2983])\n",
      "Grad:  tensor([-0.0904,  0.5116])\n",
      "------\n",
      "------\n",
      "Epoch 1038, Loss 3.719337, \n",
      "Params:  tensor([  4.8375, -14.3034])\n",
      "Grad:  tensor([-0.0902,  0.5107])\n",
      "------\n",
      "------\n",
      "Epoch 1039, Loss 3.716651, \n",
      "Params:  tensor([  4.8384, -14.3085])\n",
      "Grad:  tensor([-0.0901,  0.5099])\n",
      "------\n",
      "------\n",
      "Epoch 1040, Loss 3.713972, \n",
      "Params:  tensor([  4.8393, -14.3136])\n",
      "Grad:  tensor([-0.0899,  0.5090])\n",
      "------\n",
      "------\n",
      "Epoch 1041, Loss 3.711302, \n",
      "Params:  tensor([  4.8402, -14.3187])\n",
      "Grad:  tensor([-0.0898,  0.5081])\n",
      "------\n",
      "------\n",
      "Epoch 1042, Loss 3.708644, \n",
      "Params:  tensor([  4.8411, -14.3238])\n",
      "Grad:  tensor([-0.0896,  0.5073])\n",
      "------\n",
      "------\n",
      "Epoch 1043, Loss 3.705991, \n",
      "Params:  tensor([  4.8420, -14.3288])\n",
      "Grad:  tensor([-0.0895,  0.5064])\n",
      "------\n",
      "------\n",
      "Epoch 1044, Loss 3.703351, \n",
      "Params:  tensor([  4.8429, -14.3339])\n",
      "Grad:  tensor([-0.0893,  0.5055])\n",
      "------\n",
      "------\n",
      "Epoch 1045, Loss 3.700716, \n",
      "Params:  tensor([  4.8438, -14.3390])\n",
      "Grad:  tensor([-0.0892,  0.5047])\n",
      "------\n",
      "------\n",
      "Epoch 1046, Loss 3.698091, \n",
      "Params:  tensor([  4.8447, -14.3440])\n",
      "Grad:  tensor([-0.0890,  0.5038])\n",
      "------\n",
      "------\n",
      "Epoch 1047, Loss 3.695476, \n",
      "Params:  tensor([  4.8456, -14.3490])\n",
      "Grad:  tensor([-0.0888,  0.5030])\n",
      "------\n",
      "------\n",
      "Epoch 1048, Loss 3.692869, \n",
      "Params:  tensor([  4.8465, -14.3540])\n",
      "Grad:  tensor([-0.0887,  0.5021])\n",
      "------\n",
      "------\n",
      "Epoch 1049, Loss 3.690273, \n",
      "Params:  tensor([  4.8473, -14.3591])\n",
      "Grad:  tensor([-0.0886,  0.5013])\n",
      "------\n",
      "------\n",
      "Epoch 1050, Loss 3.687683, \n",
      "Params:  tensor([  4.8482, -14.3641])\n",
      "Grad:  tensor([-0.0884,  0.5004])\n",
      "------\n",
      "------\n",
      "Epoch 1051, Loss 3.685104, \n",
      "Params:  tensor([  4.8491, -14.3691])\n",
      "Grad:  tensor([-0.0882,  0.4996])\n",
      "------\n",
      "------\n",
      "Epoch 1052, Loss 3.682532, \n",
      "Params:  tensor([  4.8500, -14.3740])\n",
      "Grad:  tensor([-0.0881,  0.4987])\n",
      "------\n",
      "------\n",
      "Epoch 1053, Loss 3.679969, \n",
      "Params:  tensor([  4.8509, -14.3790])\n",
      "Grad:  tensor([-0.0879,  0.4979])\n",
      "------\n",
      "------\n",
      "Epoch 1054, Loss 3.677417, \n",
      "Params:  tensor([  4.8518, -14.3840])\n",
      "Grad:  tensor([-0.0878,  0.4970])\n",
      "------\n",
      "------\n",
      "Epoch 1055, Loss 3.674871, \n",
      "Params:  tensor([  4.8526, -14.3889])\n",
      "Grad:  tensor([-0.0877,  0.4962])\n",
      "------\n",
      "------\n",
      "Epoch 1056, Loss 3.672335, \n",
      "Params:  tensor([  4.8535, -14.3939])\n",
      "Grad:  tensor([-0.0875,  0.4953])\n",
      "------\n",
      "------\n",
      "Epoch 1057, Loss 3.669804, \n",
      "Params:  tensor([  4.8544, -14.3988])\n",
      "Grad:  tensor([-0.0873,  0.4945])\n",
      "------\n",
      "------\n",
      "Epoch 1058, Loss 3.667287, \n",
      "Params:  tensor([  4.8552, -14.4038])\n",
      "Grad:  tensor([-0.0872,  0.4936])\n",
      "------\n",
      "------\n",
      "Epoch 1059, Loss 3.664775, \n",
      "Params:  tensor([  4.8561, -14.4087])\n",
      "Grad:  tensor([-0.0870,  0.4928])\n",
      "------\n",
      "------\n",
      "Epoch 1060, Loss 3.662273, \n",
      "Params:  tensor([  4.8570, -14.4136])\n",
      "Grad:  tensor([-0.0869,  0.4920])\n",
      "------\n",
      "------\n",
      "Epoch 1061, Loss 3.659778, \n",
      "Params:  tensor([  4.8579, -14.4185])\n",
      "Grad:  tensor([-0.0868,  0.4911])\n",
      "------\n",
      "------\n",
      "Epoch 1062, Loss 3.657295, \n",
      "Params:  tensor([  4.8587, -14.4234])\n",
      "Grad:  tensor([-0.0866,  0.4903])\n",
      "------\n",
      "------\n",
      "Epoch 1063, Loss 3.654816, \n",
      "Params:  tensor([  4.8596, -14.4283])\n",
      "Grad:  tensor([-0.0865,  0.4895])\n",
      "------\n",
      "------\n",
      "Epoch 1064, Loss 3.652349, \n",
      "Params:  tensor([  4.8604, -14.4332])\n",
      "Grad:  tensor([-0.0863,  0.4886])\n",
      "------\n",
      "------\n",
      "Epoch 1065, Loss 3.649889, \n",
      "Params:  tensor([  4.8613, -14.4381])\n",
      "Grad:  tensor([-0.0862,  0.4878])\n",
      "------\n",
      "------\n",
      "Epoch 1066, Loss 3.647437, \n",
      "Params:  tensor([  4.8622, -14.4430])\n",
      "Grad:  tensor([-0.0860,  0.4870])\n",
      "------\n",
      "------\n",
      "Epoch 1067, Loss 3.644991, \n",
      "Params:  tensor([  4.8630, -14.4478])\n",
      "Grad:  tensor([-0.0859,  0.4862])\n",
      "------\n",
      "------\n",
      "Epoch 1068, Loss 3.642559, \n",
      "Params:  tensor([  4.8639, -14.4527])\n",
      "Grad:  tensor([-0.0857,  0.4853])\n",
      "------\n",
      "------\n",
      "Epoch 1069, Loss 3.640132, \n",
      "Params:  tensor([  4.8647, -14.4575])\n",
      "Grad:  tensor([-0.0856,  0.4845])\n",
      "------\n",
      "------\n",
      "Epoch 1070, Loss 3.637711, \n",
      "Params:  tensor([  4.8656, -14.4624])\n",
      "Grad:  tensor([-0.0854,  0.4837])\n",
      "------\n",
      "------\n",
      "Epoch 1071, Loss 3.635302, \n",
      "Params:  tensor([  4.8665, -14.4672])\n",
      "Grad:  tensor([-0.0853,  0.4829])\n",
      "------\n",
      "------\n",
      "Epoch 1072, Loss 3.632902, \n",
      "Params:  tensor([  4.8673, -14.4720])\n",
      "Grad:  tensor([-0.0851,  0.4820])\n",
      "------\n",
      "------\n",
      "Epoch 1073, Loss 3.630508, \n",
      "Params:  tensor([  4.8682, -14.4768])\n",
      "Grad:  tensor([-0.0850,  0.4812])\n",
      "------\n",
      "------\n",
      "Epoch 1074, Loss 3.628119, \n",
      "Params:  tensor([  4.8690, -14.4816])\n",
      "Grad:  tensor([-0.0849,  0.4804])\n",
      "------\n",
      "------\n",
      "Epoch 1075, Loss 3.625741, \n",
      "Params:  tensor([  4.8698, -14.4864])\n",
      "Grad:  tensor([-0.0847,  0.4796])\n",
      "------\n",
      "------\n",
      "Epoch 1076, Loss 3.623374, \n",
      "Params:  tensor([  4.8707, -14.4912])\n",
      "Grad:  tensor([-0.0846,  0.4788])\n",
      "------\n",
      "------\n",
      "Epoch 1077, Loss 3.621010, \n",
      "Params:  tensor([  4.8715, -14.4960])\n",
      "Grad:  tensor([-0.0844,  0.4780])\n",
      "------\n",
      "------\n",
      "Epoch 1078, Loss 3.618659, \n",
      "Params:  tensor([  4.8724, -14.5008])\n",
      "Grad:  tensor([-0.0843,  0.4771])\n",
      "------\n",
      "------\n",
      "Epoch 1079, Loss 3.616311, \n",
      "Params:  tensor([  4.8732, -14.5055])\n",
      "Grad:  tensor([-0.0841,  0.4763])\n",
      "------\n",
      "------\n",
      "Epoch 1080, Loss 3.613973, \n",
      "Params:  tensor([  4.8741, -14.5103])\n",
      "Grad:  tensor([-0.0840,  0.4755])\n",
      "------\n",
      "------\n",
      "Epoch 1081, Loss 3.611643, \n",
      "Params:  tensor([  4.8749, -14.5150])\n",
      "Grad:  tensor([-0.0839,  0.4747])\n",
      "------\n",
      "------\n",
      "Epoch 1082, Loss 3.609321, \n",
      "Params:  tensor([  4.8757, -14.5198])\n",
      "Grad:  tensor([-0.0837,  0.4739])\n",
      "------\n",
      "------\n",
      "Epoch 1083, Loss 3.607008, \n",
      "Params:  tensor([  4.8766, -14.5245])\n",
      "Grad:  tensor([-0.0836,  0.4731])\n",
      "------\n",
      "------\n",
      "Epoch 1084, Loss 3.604701, \n",
      "Params:  tensor([  4.8774, -14.5292])\n",
      "Grad:  tensor([-0.0834,  0.4723])\n",
      "------\n",
      "------\n",
      "Epoch 1085, Loss 3.602403, \n",
      "Params:  tensor([  4.8782, -14.5339])\n",
      "Grad:  tensor([-0.0833,  0.4715])\n",
      "------\n",
      "------\n",
      "Epoch 1086, Loss 3.600114, \n",
      "Params:  tensor([  4.8791, -14.5387])\n",
      "Grad:  tensor([-0.0832,  0.4707])\n",
      "------\n",
      "------\n",
      "Epoch 1087, Loss 3.597831, \n",
      "Params:  tensor([  4.8799, -14.5434])\n",
      "Grad:  tensor([-0.0830,  0.4699])\n",
      "------\n",
      "------\n",
      "Epoch 1088, Loss 3.595553, \n",
      "Params:  tensor([  4.8807, -14.5480])\n",
      "Grad:  tensor([-0.0829,  0.4691])\n",
      "------\n",
      "------\n",
      "Epoch 1089, Loss 3.593287, \n",
      "Params:  tensor([  4.8816, -14.5527])\n",
      "Grad:  tensor([-0.0827,  0.4683])\n",
      "------\n",
      "------\n",
      "Epoch 1090, Loss 3.591030, \n",
      "Params:  tensor([  4.8824, -14.5574])\n",
      "Grad:  tensor([-0.0826,  0.4675])\n",
      "------\n",
      "------\n",
      "Epoch 1091, Loss 3.588776, \n",
      "Params:  tensor([  4.8832, -14.5621])\n",
      "Grad:  tensor([-0.0824,  0.4667])\n",
      "------\n",
      "------\n",
      "Epoch 1092, Loss 3.586534, \n",
      "Params:  tensor([  4.8840, -14.5667])\n",
      "Grad:  tensor([-0.0823,  0.4659])\n",
      "------\n",
      "------\n",
      "Epoch 1093, Loss 3.584294, \n",
      "Params:  tensor([  4.8849, -14.5714])\n",
      "Grad:  tensor([-0.0822,  0.4651])\n",
      "------\n",
      "------\n",
      "Epoch 1094, Loss 3.582067, \n",
      "Params:  tensor([  4.8857, -14.5760])\n",
      "Grad:  tensor([-0.0820,  0.4643])\n",
      "------\n",
      "------\n",
      "Epoch 1095, Loss 3.579845, \n",
      "Params:  tensor([  4.8865, -14.5807])\n",
      "Grad:  tensor([-0.0819,  0.4636])\n",
      "------\n",
      "------\n",
      "Epoch 1096, Loss 3.577631, \n",
      "Params:  tensor([  4.8873, -14.5853])\n",
      "Grad:  tensor([-0.0818,  0.4628])\n",
      "------\n",
      "------\n",
      "Epoch 1097, Loss 3.575424, \n",
      "Params:  tensor([  4.8881, -14.5899])\n",
      "Grad:  tensor([-0.0816,  0.4620])\n",
      "------\n",
      "------\n",
      "Epoch 1098, Loss 3.573225, \n",
      "Params:  tensor([  4.8889, -14.5945])\n",
      "Grad:  tensor([-0.0815,  0.4612])\n",
      "------\n",
      "------\n",
      "Epoch 1099, Loss 3.571035, \n",
      "Params:  tensor([  4.8898, -14.5991])\n",
      "Grad:  tensor([-0.0813,  0.4604])\n",
      "------\n",
      "------\n",
      "Epoch 1100, Loss 3.568848, \n",
      "Params:  tensor([  4.8906, -14.6037])\n",
      "Grad:  tensor([-0.0812,  0.4596])\n",
      "------\n",
      "------\n",
      "Epoch 1101, Loss 3.566673, \n",
      "Params:  tensor([  4.8914, -14.6083])\n",
      "Grad:  tensor([-0.0810,  0.4588])\n",
      "------\n",
      "------\n",
      "Epoch 1102, Loss 3.564506, \n",
      "Params:  tensor([  4.8922, -14.6129])\n",
      "Grad:  tensor([-0.0809,  0.4581])\n",
      "------\n",
      "------\n",
      "Epoch 1103, Loss 3.562341, \n",
      "Params:  tensor([  4.8930, -14.6175])\n",
      "Grad:  tensor([-0.0808,  0.4573])\n",
      "------\n",
      "------\n",
      "Epoch 1104, Loss 3.560185, \n",
      "Params:  tensor([  4.8938, -14.6220])\n",
      "Grad:  tensor([-0.0806,  0.4565])\n",
      "------\n",
      "------\n",
      "Epoch 1105, Loss 3.558040, \n",
      "Params:  tensor([  4.8946, -14.6266])\n",
      "Grad:  tensor([-0.0805,  0.4557])\n",
      "------\n",
      "------\n",
      "Epoch 1106, Loss 3.555901, \n",
      "Params:  tensor([  4.8954, -14.6311])\n",
      "Grad:  tensor([-0.0804,  0.4550])\n",
      "------\n",
      "------\n",
      "Epoch 1107, Loss 3.553767, \n",
      "Params:  tensor([  4.8962, -14.6357])\n",
      "Grad:  tensor([-0.0802,  0.4542])\n",
      "------\n",
      "------\n",
      "Epoch 1108, Loss 3.551641, \n",
      "Params:  tensor([  4.8970, -14.6402])\n",
      "Grad:  tensor([-0.0801,  0.4534])\n",
      "------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Epoch 1109, Loss 3.549524, \n",
      "Params:  tensor([  4.8978, -14.6447])\n",
      "Grad:  tensor([-0.0799,  0.4527])\n",
      "------\n",
      "------\n",
      "Epoch 1110, Loss 3.547411, \n",
      "Params:  tensor([  4.8986, -14.6493])\n",
      "Grad:  tensor([-0.0798,  0.4519])\n",
      "------\n",
      "------\n",
      "Epoch 1111, Loss 3.545309, \n",
      "Params:  tensor([  4.8994, -14.6538])\n",
      "Grad:  tensor([-0.0797,  0.4511])\n",
      "------\n",
      "------\n",
      "Epoch 1112, Loss 3.543211, \n",
      "Params:  tensor([  4.9002, -14.6583])\n",
      "Grad:  tensor([-0.0796,  0.4503])\n",
      "------\n",
      "------\n",
      "Epoch 1113, Loss 3.541124, \n",
      "Params:  tensor([  4.9010, -14.6628])\n",
      "Grad:  tensor([-0.0794,  0.4496])\n",
      "------\n",
      "------\n",
      "Epoch 1114, Loss 3.539041, \n",
      "Params:  tensor([  4.9018, -14.6673])\n",
      "Grad:  tensor([-0.0793,  0.4488])\n",
      "------\n",
      "------\n",
      "Epoch 1115, Loss 3.536967, \n",
      "Params:  tensor([  4.9026, -14.6717])\n",
      "Grad:  tensor([-0.0791,  0.4481])\n",
      "------\n",
      "------\n",
      "Epoch 1116, Loss 3.534896, \n",
      "Params:  tensor([  4.9034, -14.6762])\n",
      "Grad:  tensor([-0.0790,  0.4473])\n",
      "------\n",
      "------\n",
      "Epoch 1117, Loss 3.532835, \n",
      "Params:  tensor([  4.9042, -14.6807])\n",
      "Grad:  tensor([-0.0789,  0.4465])\n",
      "------\n",
      "------\n",
      "Epoch 1118, Loss 3.530781, \n",
      "Params:  tensor([  4.9049, -14.6851])\n",
      "Grad:  tensor([-0.0787,  0.4458])\n",
      "------\n",
      "------\n",
      "Epoch 1119, Loss 3.528734, \n",
      "Params:  tensor([  4.9057, -14.6896])\n",
      "Grad:  tensor([-0.0786,  0.4450])\n",
      "------\n",
      "------\n",
      "Epoch 1120, Loss 3.526694, \n",
      "Params:  tensor([  4.9065, -14.6940])\n",
      "Grad:  tensor([-0.0785,  0.4443])\n",
      "------\n",
      "------\n",
      "Epoch 1121, Loss 3.524662, \n",
      "Params:  tensor([  4.9073, -14.6985])\n",
      "Grad:  tensor([-0.0784,  0.4435])\n",
      "------\n",
      "------\n",
      "Epoch 1122, Loss 3.522633, \n",
      "Params:  tensor([  4.9081, -14.7029])\n",
      "Grad:  tensor([-0.0782,  0.4428])\n",
      "------\n",
      "------\n",
      "Epoch 1123, Loss 3.520614, \n",
      "Params:  tensor([  4.9089, -14.7073])\n",
      "Grad:  tensor([-0.0781,  0.4420])\n",
      "------\n",
      "------\n",
      "Epoch 1124, Loss 3.518601, \n",
      "Params:  tensor([  4.9096, -14.7117])\n",
      "Grad:  tensor([-0.0779,  0.4413])\n",
      "------\n",
      "------\n",
      "Epoch 1125, Loss 3.516594, \n",
      "Params:  tensor([  4.9104, -14.7161])\n",
      "Grad:  tensor([-0.0778,  0.4405])\n",
      "------\n",
      "------\n",
      "Epoch 1126, Loss 3.514594, \n",
      "Params:  tensor([  4.9112, -14.7205])\n",
      "Grad:  tensor([-0.0777,  0.4398])\n",
      "------\n",
      "------\n",
      "Epoch 1127, Loss 3.512602, \n",
      "Params:  tensor([  4.9120, -14.7249])\n",
      "Grad:  tensor([-0.0775,  0.4390])\n",
      "------\n",
      "------\n",
      "Epoch 1128, Loss 3.510619, \n",
      "Params:  tensor([  4.9128, -14.7293])\n",
      "Grad:  tensor([-0.0774,  0.4383])\n",
      "------\n",
      "------\n",
      "Epoch 1129, Loss 3.508637, \n",
      "Params:  tensor([  4.9135, -14.7337])\n",
      "Grad:  tensor([-0.0773,  0.4375])\n",
      "------\n",
      "------\n",
      "Epoch 1130, Loss 3.506665, \n",
      "Params:  tensor([  4.9143, -14.7380])\n",
      "Grad:  tensor([-0.0772,  0.4368])\n",
      "------\n",
      "------\n",
      "Epoch 1131, Loss 3.504699, \n",
      "Params:  tensor([  4.9151, -14.7424])\n",
      "Grad:  tensor([-0.0770,  0.4360])\n",
      "------\n",
      "------\n",
      "Epoch 1132, Loss 3.502741, \n",
      "Params:  tensor([  4.9158, -14.7467])\n",
      "Grad:  tensor([-0.0769,  0.4353])\n",
      "------\n",
      "------\n",
      "Epoch 1133, Loss 3.500789, \n",
      "Params:  tensor([  4.9166, -14.7511])\n",
      "Grad:  tensor([-0.0767,  0.4346])\n",
      "------\n",
      "------\n",
      "Epoch 1134, Loss 3.498843, \n",
      "Params:  tensor([  4.9174, -14.7554])\n",
      "Grad:  tensor([-0.0766,  0.4338])\n",
      "------\n",
      "------\n",
      "Epoch 1135, Loss 3.496905, \n",
      "Params:  tensor([  4.9181, -14.7598])\n",
      "Grad:  tensor([-0.0765,  0.4331])\n",
      "------\n",
      "------\n",
      "Epoch 1136, Loss 3.494972, \n",
      "Params:  tensor([  4.9189, -14.7641])\n",
      "Grad:  tensor([-0.0764,  0.4323])\n",
      "------\n",
      "------\n",
      "Epoch 1137, Loss 3.493046, \n",
      "Params:  tensor([  4.9197, -14.7684])\n",
      "Grad:  tensor([-0.0763,  0.4316])\n",
      "------\n",
      "------\n",
      "Epoch 1138, Loss 3.491127, \n",
      "Params:  tensor([  4.9204, -14.7727])\n",
      "Grad:  tensor([-0.0761,  0.4309])\n",
      "------\n",
      "------\n",
      "Epoch 1139, Loss 3.489214, \n",
      "Params:  tensor([  4.9212, -14.7770])\n",
      "Grad:  tensor([-0.0760,  0.4301])\n",
      "------\n",
      "------\n",
      "Epoch 1140, Loss 3.487308, \n",
      "Params:  tensor([  4.9219, -14.7813])\n",
      "Grad:  tensor([-0.0759,  0.4294])\n",
      "------\n",
      "------\n",
      "Epoch 1141, Loss 3.485410, \n",
      "Params:  tensor([  4.9227, -14.7856])\n",
      "Grad:  tensor([-0.0757,  0.4287])\n",
      "------\n",
      "------\n",
      "Epoch 1142, Loss 3.483515, \n",
      "Params:  tensor([  4.9235, -14.7899])\n",
      "Grad:  tensor([-0.0756,  0.4280])\n",
      "------\n",
      "------\n",
      "Epoch 1143, Loss 3.481627, \n",
      "Params:  tensor([  4.9242, -14.7941])\n",
      "Grad:  tensor([-0.0755,  0.4272])\n",
      "------\n",
      "------\n",
      "Epoch 1144, Loss 3.479746, \n",
      "Params:  tensor([  4.9250, -14.7984])\n",
      "Grad:  tensor([-0.0753,  0.4265])\n",
      "------\n",
      "------\n",
      "Epoch 1145, Loss 3.477872, \n",
      "Params:  tensor([  4.9257, -14.8027])\n",
      "Grad:  tensor([-0.0752,  0.4258])\n",
      "------\n",
      "------\n",
      "Epoch 1146, Loss 3.476005, \n",
      "Params:  tensor([  4.9265, -14.8069])\n",
      "Grad:  tensor([-0.0751,  0.4250])\n",
      "------\n",
      "------\n",
      "Epoch 1147, Loss 3.474143, \n",
      "Params:  tensor([  4.9272, -14.8112])\n",
      "Grad:  tensor([-0.0750,  0.4243])\n",
      "------\n",
      "------\n",
      "Epoch 1148, Loss 3.472288, \n",
      "Params:  tensor([  4.9280, -14.8154])\n",
      "Grad:  tensor([-0.0748,  0.4236])\n",
      "------\n",
      "------\n",
      "Epoch 1149, Loss 3.470441, \n",
      "Params:  tensor([  4.9287, -14.8196])\n",
      "Grad:  tensor([-0.0747,  0.4229])\n",
      "------\n",
      "------\n",
      "Epoch 1150, Loss 3.468597, \n",
      "Params:  tensor([  4.9295, -14.8238])\n",
      "Grad:  tensor([-0.0746,  0.4222])\n",
      "------\n",
      "------\n",
      "Epoch 1151, Loss 3.466762, \n",
      "Params:  tensor([  4.9302, -14.8281])\n",
      "Grad:  tensor([-0.0745,  0.4215])\n",
      "------\n",
      "------\n",
      "Epoch 1152, Loss 3.464930, \n",
      "Params:  tensor([  4.9309, -14.8323])\n",
      "Grad:  tensor([-0.0743,  0.4207])\n",
      "------\n",
      "------\n",
      "Epoch 1153, Loss 3.463105, \n",
      "Params:  tensor([  4.9317, -14.8365])\n",
      "Grad:  tensor([-0.0742,  0.4200])\n",
      "------\n",
      "------\n",
      "Epoch 1154, Loss 3.461290, \n",
      "Params:  tensor([  4.9324, -14.8407])\n",
      "Grad:  tensor([-0.0741,  0.4193])\n",
      "------\n",
      "------\n",
      "Epoch 1155, Loss 3.459477, \n",
      "Params:  tensor([  4.9332, -14.8448])\n",
      "Grad:  tensor([-0.0739,  0.4186])\n",
      "------\n",
      "------\n",
      "Epoch 1156, Loss 3.457671, \n",
      "Params:  tensor([  4.9339, -14.8490])\n",
      "Grad:  tensor([-0.0738,  0.4179])\n",
      "------\n",
      "------\n",
      "Epoch 1157, Loss 3.455873, \n",
      "Params:  tensor([  4.9346, -14.8532])\n",
      "Grad:  tensor([-0.0737,  0.4172])\n",
      "------\n",
      "------\n",
      "Epoch 1158, Loss 3.454080, \n",
      "Params:  tensor([  4.9354, -14.8574])\n",
      "Grad:  tensor([-0.0736,  0.4165])\n",
      "------\n",
      "------\n",
      "Epoch 1159, Loss 3.452293, \n",
      "Params:  tensor([  4.9361, -14.8615])\n",
      "Grad:  tensor([-0.0734,  0.4158])\n",
      "------\n",
      "------\n",
      "Epoch 1160, Loss 3.450513, \n",
      "Params:  tensor([  4.9368, -14.8657])\n",
      "Grad:  tensor([-0.0733,  0.4151])\n",
      "------\n",
      "------\n",
      "Epoch 1161, Loss 3.448736, \n",
      "Params:  tensor([  4.9376, -14.8698])\n",
      "Grad:  tensor([-0.0732,  0.4143])\n",
      "------\n",
      "------\n",
      "Epoch 1162, Loss 3.446968, \n",
      "Params:  tensor([  4.9383, -14.8739])\n",
      "Grad:  tensor([-0.0731,  0.4136])\n",
      "------\n",
      "------\n",
      "Epoch 1163, Loss 3.445203, \n",
      "Params:  tensor([  4.9390, -14.8781])\n",
      "Grad:  tensor([-0.0730,  0.4129])\n",
      "------\n",
      "------\n",
      "Epoch 1164, Loss 3.443449, \n",
      "Params:  tensor([  4.9398, -14.8822])\n",
      "Grad:  tensor([-0.0728,  0.4122])\n",
      "------\n",
      "------\n",
      "Epoch 1165, Loss 3.441697, \n",
      "Params:  tensor([  4.9405, -14.8863])\n",
      "Grad:  tensor([-0.0727,  0.4115])\n",
      "------\n",
      "------\n",
      "Epoch 1166, Loss 3.439952, \n",
      "Params:  tensor([  4.9412, -14.8904])\n",
      "Grad:  tensor([-0.0726,  0.4108])\n",
      "------\n",
      "------\n",
      "Epoch 1167, Loss 3.438210, \n",
      "Params:  tensor([  4.9419, -14.8945])\n",
      "Grad:  tensor([-0.0725,  0.4101])\n",
      "------\n",
      "------\n",
      "Epoch 1168, Loss 3.436479, \n",
      "Params:  tensor([  4.9427, -14.8986])\n",
      "Grad:  tensor([-0.0723,  0.4094])\n",
      "------\n",
      "------\n",
      "Epoch 1169, Loss 3.434753, \n",
      "Params:  tensor([  4.9434, -14.9027])\n",
      "Grad:  tensor([-0.0722,  0.4087])\n",
      "------\n",
      "------\n",
      "Epoch 1170, Loss 3.433030, \n",
      "Params:  tensor([  4.9441, -14.9068])\n",
      "Grad:  tensor([-0.0721,  0.4081])\n",
      "------\n",
      "------\n",
      "Epoch 1171, Loss 3.431314, \n",
      "Params:  tensor([  4.9448, -14.9109])\n",
      "Grad:  tensor([-0.0720,  0.4074])\n",
      "------\n",
      "------\n",
      "Epoch 1172, Loss 3.429607, \n",
      "Params:  tensor([  4.9455, -14.9149])\n",
      "Grad:  tensor([-0.0719,  0.4067])\n",
      "------\n",
      "------\n",
      "Epoch 1173, Loss 3.427903, \n",
      "Params:  tensor([  4.9463, -14.9190])\n",
      "Grad:  tensor([-0.0717,  0.4060])\n",
      "------\n",
      "------\n",
      "Epoch 1174, Loss 3.426204, \n",
      "Params:  tensor([  4.9470, -14.9230])\n",
      "Grad:  tensor([-0.0716,  0.4053])\n",
      "------\n",
      "------\n",
      "Epoch 1175, Loss 3.424510, \n",
      "Params:  tensor([  4.9477, -14.9271])\n",
      "Grad:  tensor([-0.0715,  0.4046])\n",
      "------\n",
      "------\n",
      "Epoch 1176, Loss 3.422823, \n",
      "Params:  tensor([  4.9484, -14.9311])\n",
      "Grad:  tensor([-0.0714,  0.4039])\n",
      "------\n",
      "------\n",
      "Epoch 1177, Loss 3.421144, \n",
      "Params:  tensor([  4.9491, -14.9352])\n",
      "Grad:  tensor([-0.0712,  0.4032])\n",
      "------\n",
      "------\n",
      "Epoch 1178, Loss 3.419468, \n",
      "Params:  tensor([  4.9498, -14.9392])\n",
      "Grad:  tensor([-0.0711,  0.4025])\n",
      "------\n",
      "------\n",
      "Epoch 1179, Loss 3.417798, \n",
      "Params:  tensor([  4.9505, -14.9432])\n",
      "Grad:  tensor([-0.0710,  0.4019])\n",
      "------\n",
      "------\n",
      "Epoch 1180, Loss 3.416134, \n",
      "Params:  tensor([  4.9512, -14.9472])\n",
      "Grad:  tensor([-0.0709,  0.4012])\n",
      "------\n",
      "------\n",
      "Epoch 1181, Loss 3.414476, \n",
      "Params:  tensor([  4.9520, -14.9512])\n",
      "Grad:  tensor([-0.0708,  0.4005])\n",
      "------\n",
      "------\n",
      "Epoch 1182, Loss 3.412824, \n",
      "Params:  tensor([  4.9527, -14.9552])\n",
      "Grad:  tensor([-0.0706,  0.3998])\n",
      "------\n",
      "------\n",
      "Epoch 1183, Loss 3.411176, \n",
      "Params:  tensor([  4.9534, -14.9592])\n",
      "Grad:  tensor([-0.0705,  0.3991])\n",
      "------\n",
      "------\n",
      "Epoch 1184, Loss 3.409534, \n",
      "Params:  tensor([  4.9541, -14.9632])\n",
      "Grad:  tensor([-0.0704,  0.3985])\n",
      "------\n",
      "------\n",
      "Epoch 1185, Loss 3.407900, \n",
      "Params:  tensor([  4.9548, -14.9672])\n",
      "Grad:  tensor([-0.0703,  0.3978])\n",
      "------\n",
      "------\n",
      "Epoch 1186, Loss 3.406271, \n",
      "Params:  tensor([  4.9555, -14.9711])\n",
      "Grad:  tensor([-0.0701,  0.3971])\n",
      "------\n",
      "------\n",
      "Epoch 1187, Loss 3.404645, \n",
      "Params:  tensor([  4.9562, -14.9751])\n",
      "Grad:  tensor([-0.0700,  0.3964])\n",
      "------\n",
      "------\n",
      "Epoch 1188, Loss 3.403024, \n",
      "Params:  tensor([  4.9569, -14.9791])\n",
      "Grad:  tensor([-0.0699,  0.3958])\n",
      "------\n",
      "------\n",
      "Epoch 1189, Loss 3.401413, \n",
      "Params:  tensor([  4.9576, -14.9830])\n",
      "Grad:  tensor([-0.0698,  0.3951])\n",
      "------\n",
      "------\n",
      "Epoch 1190, Loss 3.399802, \n",
      "Params:  tensor([  4.9583, -14.9870])\n",
      "Grad:  tensor([-0.0697,  0.3944])\n",
      "------\n",
      "------\n",
      "Epoch 1191, Loss 3.398200, \n",
      "Params:  tensor([  4.9590, -14.9909])\n",
      "Grad:  tensor([-0.0696,  0.3937])\n",
      "------\n",
      "------\n",
      "Epoch 1192, Loss 3.396602, \n",
      "Params:  tensor([  4.9597, -14.9948])\n",
      "Grad:  tensor([-0.0694,  0.3931])\n",
      "------\n",
      "------\n",
      "Epoch 1193, Loss 3.395011, \n",
      "Params:  tensor([  4.9604, -14.9988])\n",
      "Grad:  tensor([-0.0693,  0.3924])\n",
      "------\n",
      "------\n",
      "Epoch 1194, Loss 3.393425, \n",
      "Params:  tensor([  4.9610, -15.0027])\n",
      "Grad:  tensor([-0.0692,  0.3917])\n",
      "------\n",
      "------\n",
      "Epoch 1195, Loss 3.391844, \n",
      "Params:  tensor([  4.9617, -15.0066])\n",
      "Grad:  tensor([-0.0691,  0.3911])\n",
      "------\n",
      "------\n",
      "Epoch 1196, Loss 3.390266, \n",
      "Params:  tensor([  4.9624, -15.0105])\n",
      "Grad:  tensor([-0.0690,  0.3904])\n",
      "------\n",
      "------\n",
      "Epoch 1197, Loss 3.388697, \n",
      "Params:  tensor([  4.9631, -15.0144])\n",
      "Grad:  tensor([-0.0689,  0.3897])\n",
      "------\n",
      "------\n",
      "Epoch 1198, Loss 3.387131, \n",
      "Params:  tensor([  4.9638, -15.0183])\n",
      "Grad:  tensor([-0.0687,  0.3891])\n",
      "------\n",
      "------\n",
      "Epoch 1199, Loss 3.385571, \n",
      "Params:  tensor([  4.9645, -15.0222])\n",
      "Grad:  tensor([-0.0686,  0.3884])\n",
      "------\n",
      "------\n",
      "Epoch 1200, Loss 3.384018, \n",
      "Params:  tensor([  4.9652, -15.0260])\n",
      "Grad:  tensor([-0.0685,  0.3878])\n",
      "------\n",
      "------\n",
      "Epoch 1201, Loss 3.382467, \n",
      "Params:  tensor([  4.9659, -15.0299])\n",
      "Grad:  tensor([-0.0684,  0.3871])\n",
      "------\n",
      "------\n",
      "Epoch 1202, Loss 3.380925, \n",
      "Params:  tensor([  4.9665, -15.0338])\n",
      "Grad:  tensor([-0.0683,  0.3864])\n",
      "------\n",
      "------\n",
      "Epoch 1203, Loss 3.379385, \n",
      "Params:  tensor([  4.9672, -15.0376])\n",
      "Grad:  tensor([-0.0681,  0.3858])\n",
      "------\n",
      "------\n",
      "Epoch 1204, Loss 3.377851, \n",
      "Params:  tensor([  4.9679, -15.0415])\n",
      "Grad:  tensor([-0.0680,  0.3851])\n",
      "------\n",
      "------\n",
      "Epoch 1205, Loss 3.376323, \n",
      "Params:  tensor([  4.9686, -15.0453])\n",
      "Grad:  tensor([-0.0679,  0.3845])\n",
      "------\n",
      "------\n",
      "Epoch 1206, Loss 3.374800, \n",
      "Params:  tensor([  4.9693, -15.0492])\n",
      "Grad:  tensor([-0.0678,  0.3838])\n",
      "------\n",
      "------\n",
      "Epoch 1207, Loss 3.373284, \n",
      "Params:  tensor([  4.9699, -15.0530])\n",
      "Grad:  tensor([-0.0677,  0.3832])\n",
      "------\n",
      "------\n",
      "Epoch 1208, Loss 3.371769, \n",
      "Params:  tensor([  4.9706, -15.0568])\n",
      "Grad:  tensor([-0.0676,  0.3825])\n",
      "------\n",
      "------\n",
      "Epoch 1209, Loss 3.370261, \n",
      "Params:  tensor([  4.9713, -15.0606])\n",
      "Grad:  tensor([-0.0675,  0.3819])\n",
      "------\n",
      "------\n",
      "Epoch 1210, Loss 3.368760, \n",
      "Params:  tensor([  4.9720, -15.0645])\n",
      "Grad:  tensor([-0.0673,  0.3812])\n",
      "------\n",
      "------\n",
      "Epoch 1211, Loss 3.367262, \n",
      "Params:  tensor([  4.9726, -15.0683])\n",
      "Grad:  tensor([-0.0672,  0.3806])\n",
      "------\n",
      "------\n",
      "Epoch 1212, Loss 3.365771, \n",
      "Params:  tensor([  4.9733, -15.0721])\n",
      "Grad:  tensor([-0.0671,  0.3799])\n",
      "------\n",
      "------\n",
      "Epoch 1213, Loss 3.364282, \n",
      "Params:  tensor([  4.9740, -15.0758])\n",
      "Grad:  tensor([-0.0670,  0.3793])\n",
      "------\n",
      "------\n",
      "Epoch 1214, Loss 3.362800, \n",
      "Params:  tensor([  4.9746, -15.0796])\n",
      "Grad:  tensor([-0.0669,  0.3786])\n",
      "------\n",
      "------\n",
      "Epoch 1215, Loss 3.361324, \n",
      "Params:  tensor([  4.9753, -15.0834])\n",
      "Grad:  tensor([-0.0668,  0.3780])\n",
      "------\n",
      "------\n",
      "Epoch 1216, Loss 3.359850, \n",
      "Params:  tensor([  4.9760, -15.0872])\n",
      "Grad:  tensor([-0.0667,  0.3774])\n",
      "------\n",
      "------\n",
      "Epoch 1217, Loss 3.358384, \n",
      "Params:  tensor([  4.9766, -15.0910])\n",
      "Grad:  tensor([-0.0665,  0.3767])\n",
      "------\n",
      "------\n",
      "Epoch 1218, Loss 3.356921, \n",
      "Params:  tensor([  4.9773, -15.0947])\n",
      "Grad:  tensor([-0.0664,  0.3761])\n",
      "------\n",
      "------\n",
      "Epoch 1219, Loss 3.355464, \n",
      "Params:  tensor([  4.9780, -15.0985])\n",
      "Grad:  tensor([-0.0663,  0.3754])\n",
      "------\n",
      "------\n",
      "Epoch 1220, Loss 3.354012, \n",
      "Params:  tensor([  4.9786, -15.1022])\n",
      "Grad:  tensor([-0.0662,  0.3748])\n",
      "------\n",
      "------\n",
      "Epoch 1221, Loss 3.352564, \n",
      "Params:  tensor([  4.9793, -15.1060])\n",
      "Grad:  tensor([-0.0661,  0.3742])\n",
      "------\n",
      "------\n",
      "Epoch 1222, Loss 3.351122, \n",
      "Params:  tensor([  4.9799, -15.1097])\n",
      "Grad:  tensor([-0.0660,  0.3735])\n",
      "------\n",
      "------\n",
      "Epoch 1223, Loss 3.349685, \n",
      "Params:  tensor([  4.9806, -15.1134])\n",
      "Grad:  tensor([-0.0659,  0.3729])\n",
      "------\n",
      "------\n",
      "Epoch 1224, Loss 3.348251, \n",
      "Params:  tensor([  4.9813, -15.1171])\n",
      "Grad:  tensor([-0.0657,  0.3723])\n",
      "------\n",
      "------\n",
      "Epoch 1225, Loss 3.346824, \n",
      "Params:  tensor([  4.9819, -15.1209])\n",
      "Grad:  tensor([-0.0656,  0.3716])\n",
      "------\n",
      "------\n",
      "Epoch 1226, Loss 3.345403, \n",
      "Params:  tensor([  4.9826, -15.1246])\n",
      "Grad:  tensor([-0.0655,  0.3710])\n",
      "------\n",
      "------\n",
      "Epoch 1227, Loss 3.343982, \n",
      "Params:  tensor([  4.9832, -15.1283])\n",
      "Grad:  tensor([-0.0654,  0.3704])\n",
      "------\n",
      "------\n",
      "Epoch 1228, Loss 3.342571, \n",
      "Params:  tensor([  4.9839, -15.1320])\n",
      "Grad:  tensor([-0.0653,  0.3697])\n",
      "------\n",
      "------\n",
      "Epoch 1229, Loss 3.341160, \n",
      "Params:  tensor([  4.9845, -15.1357])\n",
      "Grad:  tensor([-0.0652,  0.3691])\n",
      "------\n",
      "------\n",
      "Epoch 1230, Loss 3.339758, \n",
      "Params:  tensor([  4.9852, -15.1393])\n",
      "Grad:  tensor([-0.0651,  0.3685])\n",
      "------\n",
      "------\n",
      "Epoch 1231, Loss 3.338359, \n",
      "Params:  tensor([  4.9858, -15.1430])\n",
      "Grad:  tensor([-0.0650,  0.3679])\n",
      "------\n",
      "------\n",
      "Epoch 1232, Loss 3.336965, \n",
      "Params:  tensor([  4.9865, -15.1467])\n",
      "Grad:  tensor([-0.0649,  0.3672])\n",
      "------\n",
      "------\n",
      "Epoch 1233, Loss 3.335577, \n",
      "Params:  tensor([  4.9871, -15.1504])\n",
      "Grad:  tensor([-0.0648,  0.3666])\n",
      "------\n",
      "------\n",
      "Epoch 1234, Loss 3.334192, \n",
      "Params:  tensor([  4.9878, -15.1540])\n",
      "Grad:  tensor([-0.0646,  0.3660])\n",
      "------\n",
      "------\n",
      "Epoch 1235, Loss 3.332811, \n",
      "Params:  tensor([  4.9884, -15.1577])\n",
      "Grad:  tensor([-0.0645,  0.3654])\n",
      "------\n",
      "------\n",
      "Epoch 1236, Loss 3.331436, \n",
      "Params:  tensor([  4.9891, -15.1613])\n",
      "Grad:  tensor([-0.0644,  0.3647])\n",
      "------\n",
      "------\n",
      "Epoch 1237, Loss 3.330065, \n",
      "Params:  tensor([  4.9897, -15.1650])\n",
      "Grad:  tensor([-0.0643,  0.3641])\n",
      "------\n",
      "------\n",
      "Epoch 1238, Loss 3.328699, \n",
      "Params:  tensor([  4.9904, -15.1686])\n",
      "Grad:  tensor([-0.0642,  0.3635])\n",
      "------\n",
      "------\n",
      "Epoch 1239, Loss 3.327339, \n",
      "Params:  tensor([  4.9910, -15.1722])\n",
      "Grad:  tensor([-0.0641,  0.3629])\n",
      "------\n",
      "------\n",
      "Epoch 1240, Loss 3.325980, \n",
      "Params:  tensor([  4.9916, -15.1759])\n",
      "Grad:  tensor([-0.0640,  0.3623])\n",
      "------\n",
      "------\n",
      "Epoch 1241, Loss 3.324628, \n",
      "Params:  tensor([  4.9923, -15.1795])\n",
      "Grad:  tensor([-0.0639,  0.3617])\n",
      "------\n",
      "------\n",
      "Epoch 1242, Loss 3.323279, \n",
      "Params:  tensor([  4.9929, -15.1831])\n",
      "Grad:  tensor([-0.0638,  0.3610])\n",
      "------\n",
      "------\n",
      "Epoch 1243, Loss 3.321935, \n",
      "Params:  tensor([  4.9936, -15.1867])\n",
      "Grad:  tensor([-0.0637,  0.3604])\n",
      "------\n",
      "------\n",
      "Epoch 1244, Loss 3.320600, \n",
      "Params:  tensor([  4.9942, -15.1903])\n",
      "Grad:  tensor([-0.0636,  0.3598])\n",
      "------\n",
      "------\n",
      "Epoch 1245, Loss 3.319264, \n",
      "Params:  tensor([  4.9948, -15.1939])\n",
      "Grad:  tensor([-0.0635,  0.3592])\n",
      "------\n",
      "------\n",
      "Epoch 1246, Loss 3.317935, \n",
      "Params:  tensor([  4.9955, -15.1975])\n",
      "Grad:  tensor([-0.0633,  0.3586])\n",
      "------\n",
      "------\n",
      "Epoch 1247, Loss 3.316611, \n",
      "Params:  tensor([  4.9961, -15.2010])\n",
      "Grad:  tensor([-0.0633,  0.3580])\n",
      "------\n",
      "------\n",
      "Epoch 1248, Loss 3.315289, \n",
      "Params:  tensor([  4.9967, -15.2046])\n",
      "Grad:  tensor([-0.0631,  0.3574])\n",
      "------\n",
      "------\n",
      "Epoch 1249, Loss 3.313973, \n",
      "Params:  tensor([  4.9973, -15.2082])\n",
      "Grad:  tensor([-0.0630,  0.3568])\n",
      "------\n",
      "------\n",
      "Epoch 1250, Loss 3.312663, \n",
      "Params:  tensor([  4.9980, -15.2117])\n",
      "Grad:  tensor([-0.0629,  0.3562])\n",
      "------\n",
      "------\n",
      "Epoch 1251, Loss 3.311353, \n",
      "Params:  tensor([  4.9986, -15.2153])\n",
      "Grad:  tensor([-0.0628,  0.3556])\n",
      "------\n",
      "------\n",
      "Epoch 1252, Loss 3.310053, \n",
      "Params:  tensor([  4.9992, -15.2189])\n",
      "Grad:  tensor([-0.0627,  0.3550])\n",
      "------\n",
      "------\n",
      "Epoch 1253, Loss 3.308756, \n",
      "Params:  tensor([  4.9999, -15.2224])\n",
      "Grad:  tensor([-0.0626,  0.3543])\n",
      "------\n",
      "------\n",
      "Epoch 1254, Loss 3.307463, \n",
      "Params:  tensor([  5.0005, -15.2259])\n",
      "Grad:  tensor([-0.0625,  0.3537])\n",
      "------\n",
      "------\n",
      "Epoch 1255, Loss 3.306170, \n",
      "Params:  tensor([  5.0011, -15.2295])\n",
      "Grad:  tensor([-0.0624,  0.3531])\n",
      "------\n",
      "------\n",
      "Epoch 1256, Loss 3.304887, \n",
      "Params:  tensor([  5.0017, -15.2330])\n",
      "Grad:  tensor([-0.0623,  0.3525])\n",
      "------\n",
      "------\n",
      "Epoch 1257, Loss 3.303605, \n",
      "Params:  tensor([  5.0024, -15.2365])\n",
      "Grad:  tensor([-0.0622,  0.3519])\n",
      "------\n",
      "------\n",
      "Epoch 1258, Loss 3.302329, \n",
      "Params:  tensor([  5.0030, -15.2400])\n",
      "Grad:  tensor([-0.0620,  0.3514])\n",
      "------\n",
      "------\n",
      "Epoch 1259, Loss 3.301057, \n",
      "Params:  tensor([  5.0036, -15.2435])\n",
      "Grad:  tensor([-0.0620,  0.3508])\n",
      "------\n",
      "------\n",
      "Epoch 1260, Loss 3.299791, \n",
      "Params:  tensor([  5.0042, -15.2470])\n",
      "Grad:  tensor([-0.0619,  0.3502])\n",
      "------\n",
      "------\n",
      "Epoch 1261, Loss 3.298528, \n",
      "Params:  tensor([  5.0048, -15.2505])\n",
      "Grad:  tensor([-0.0618,  0.3496])\n",
      "------\n",
      "------\n",
      "Epoch 1262, Loss 3.297267, \n",
      "Params:  tensor([  5.0054, -15.2540])\n",
      "Grad:  tensor([-0.0616,  0.3490])\n",
      "------\n",
      "------\n",
      "Epoch 1263, Loss 3.296014, \n",
      "Params:  tensor([  5.0061, -15.2575])\n",
      "Grad:  tensor([-0.0615,  0.3484])\n",
      "------\n",
      "------\n",
      "Epoch 1264, Loss 3.294762, \n",
      "Params:  tensor([  5.0067, -15.2610])\n",
      "Grad:  tensor([-0.0614,  0.3478])\n",
      "------\n",
      "------\n",
      "Epoch 1265, Loss 3.293517, \n",
      "Params:  tensor([  5.0073, -15.2645])\n",
      "Grad:  tensor([-0.0613,  0.3472])\n",
      "------\n",
      "------\n",
      "Epoch 1266, Loss 3.292276, \n",
      "Params:  tensor([  5.0079, -15.2679])\n",
      "Grad:  tensor([-0.0612,  0.3466])\n",
      "------\n",
      "------\n",
      "Epoch 1267, Loss 3.291036, \n",
      "Params:  tensor([  5.0085, -15.2714])\n",
      "Grad:  tensor([-0.0611,  0.3460])\n",
      "------\n",
      "------\n",
      "Epoch 1268, Loss 3.289804, \n",
      "Params:  tensor([  5.0091, -15.2748])\n",
      "Grad:  tensor([-0.0610,  0.3454])\n",
      "------\n",
      "------\n",
      "Epoch 1269, Loss 3.288573, \n",
      "Params:  tensor([  5.0097, -15.2783])\n",
      "Grad:  tensor([-0.0609,  0.3448])\n",
      "------\n",
      "------\n",
      "Epoch 1270, Loss 3.287347, \n",
      "Params:  tensor([  5.0103, -15.2817])\n",
      "Grad:  tensor([-0.0608,  0.3443])\n",
      "------\n",
      "------\n",
      "Epoch 1271, Loss 3.286129, \n",
      "Params:  tensor([  5.0109, -15.2852])\n",
      "Grad:  tensor([-0.0607,  0.3437])\n",
      "------\n",
      "------\n",
      "Epoch 1272, Loss 3.284911, \n",
      "Params:  tensor([  5.0116, -15.2886])\n",
      "Grad:  tensor([-0.0606,  0.3431])\n",
      "------\n",
      "------\n",
      "Epoch 1273, Loss 3.283698, \n",
      "Params:  tensor([  5.0122, -15.2920])\n",
      "Grad:  tensor([-0.0605,  0.3425])\n",
      "------\n",
      "------\n",
      "Epoch 1274, Loss 3.282488, \n",
      "Params:  tensor([  5.0128, -15.2954])\n",
      "Grad:  tensor([-0.0604,  0.3419])\n",
      "------\n",
      "------\n",
      "Epoch 1275, Loss 3.281284, \n",
      "Params:  tensor([  5.0134, -15.2988])\n",
      "Grad:  tensor([-0.0603,  0.3413])\n",
      "------\n",
      "------\n",
      "Epoch 1276, Loss 3.280085, \n",
      "Params:  tensor([  5.0140, -15.3023])\n",
      "Grad:  tensor([-0.0602,  0.3408])\n",
      "------\n",
      "------\n",
      "Epoch 1277, Loss 3.278888, \n",
      "Params:  tensor([  5.0146, -15.3057])\n",
      "Grad:  tensor([-0.0601,  0.3402])\n",
      "------\n",
      "------\n",
      "Epoch 1278, Loss 3.277696, \n",
      "Params:  tensor([  5.0152, -15.3091])\n",
      "Grad:  tensor([-0.0600,  0.3396])\n",
      "------\n",
      "------\n",
      "Epoch 1279, Loss 3.276506, \n",
      "Params:  tensor([  5.0158, -15.3124])\n",
      "Grad:  tensor([-0.0599,  0.3390])\n",
      "------\n",
      "------\n",
      "Epoch 1280, Loss 3.275322, \n",
      "Params:  tensor([  5.0164, -15.3158])\n",
      "Grad:  tensor([-0.0598,  0.3384])\n",
      "------\n",
      "------\n",
      "Epoch 1281, Loss 3.274142, \n",
      "Params:  tensor([  5.0170, -15.3192])\n",
      "Grad:  tensor([-0.0597,  0.3379])\n",
      "------\n",
      "------\n",
      "Epoch 1282, Loss 3.272968, \n",
      "Params:  tensor([  5.0176, -15.3226])\n",
      "Grad:  tensor([-0.0596,  0.3373])\n",
      "------\n",
      "------\n",
      "Epoch 1283, Loss 3.271793, \n",
      "Params:  tensor([  5.0182, -15.3259])\n",
      "Grad:  tensor([-0.0595,  0.3367])\n",
      "------\n",
      "------\n",
      "Epoch 1284, Loss 3.270625, \n",
      "Params:  tensor([  5.0187, -15.3293])\n",
      "Grad:  tensor([-0.0594,  0.3362])\n",
      "------\n",
      "------\n",
      "Epoch 1285, Loss 3.269460, \n",
      "Params:  tensor([  5.0193, -15.3327])\n",
      "Grad:  tensor([-0.0593,  0.3356])\n",
      "------\n",
      "------\n",
      "Epoch 1286, Loss 3.268301, \n",
      "Params:  tensor([  5.0199, -15.3360])\n",
      "Grad:  tensor([-0.0592,  0.3350])\n",
      "------\n",
      "------\n",
      "Epoch 1287, Loss 3.267143, \n",
      "Params:  tensor([  5.0205, -15.3394])\n",
      "Grad:  tensor([-0.0591,  0.3344])\n",
      "------\n",
      "------\n",
      "Epoch 1288, Loss 3.265991, \n",
      "Params:  tensor([  5.0211, -15.3427])\n",
      "Grad:  tensor([-0.0590,  0.3339])\n",
      "------\n",
      "------\n",
      "Epoch 1289, Loss 3.264842, \n",
      "Params:  tensor([  5.0217, -15.3460])\n",
      "Grad:  tensor([-0.0589,  0.3333])\n",
      "------\n",
      "------\n",
      "Epoch 1290, Loss 3.263700, \n",
      "Params:  tensor([  5.0223, -15.3494])\n",
      "Grad:  tensor([-0.0588,  0.3327])\n",
      "------\n",
      "------\n",
      "Epoch 1291, Loss 3.262556, \n",
      "Params:  tensor([  5.0229, -15.3527])\n",
      "Grad:  tensor([-0.0587,  0.3322])\n",
      "------\n",
      "------\n",
      "Epoch 1292, Loss 3.261421, \n",
      "Params:  tensor([  5.0235, -15.3560])\n",
      "Grad:  tensor([-0.0586,  0.3316])\n",
      "------\n",
      "------\n",
      "Epoch 1293, Loss 3.260287, \n",
      "Params:  tensor([  5.0240, -15.3593])\n",
      "Grad:  tensor([-0.0585,  0.3311])\n",
      "------\n",
      "------\n",
      "Epoch 1294, Loss 3.259160, \n",
      "Params:  tensor([  5.0246, -15.3626])\n",
      "Grad:  tensor([-0.0584,  0.3305])\n",
      "------\n",
      "------\n",
      "Epoch 1295, Loss 3.258033, \n",
      "Params:  tensor([  5.0252, -15.3659])\n",
      "Grad:  tensor([-0.0583,  0.3299])\n",
      "------\n",
      "------\n",
      "Epoch 1296, Loss 3.256912, \n",
      "Params:  tensor([  5.0258, -15.3692])\n",
      "Grad:  tensor([-0.0582,  0.3294])\n",
      "------\n",
      "------\n",
      "Epoch 1297, Loss 3.255795, \n",
      "Params:  tensor([  5.0264, -15.3725])\n",
      "Grad:  tensor([-0.0581,  0.3288])\n",
      "------\n",
      "------\n",
      "Epoch 1298, Loss 3.254681, \n",
      "Params:  tensor([  5.0270, -15.3758])\n",
      "Grad:  tensor([-0.0580,  0.3282])\n",
      "------\n",
      "------\n",
      "Epoch 1299, Loss 3.253569, \n",
      "Params:  tensor([  5.0275, -15.3791])\n",
      "Grad:  tensor([-0.0579,  0.3277])\n",
      "------\n",
      "------\n",
      "Epoch 1300, Loss 3.252462, \n",
      "Params:  tensor([  5.0281, -15.3823])\n",
      "Grad:  tensor([-0.0578,  0.3271])\n",
      "------\n",
      "------\n",
      "Epoch 1301, Loss 3.251362, \n",
      "Params:  tensor([  5.0287, -15.3856])\n",
      "Grad:  tensor([-0.0577,  0.3266])\n",
      "------\n",
      "------\n",
      "Epoch 1302, Loss 3.250263, \n",
      "Params:  tensor([  5.0293, -15.3888])\n",
      "Grad:  tensor([-0.0576,  0.3260])\n",
      "------\n",
      "------\n",
      "Epoch 1303, Loss 3.249168, \n",
      "Params:  tensor([  5.0298, -15.3921])\n",
      "Grad:  tensor([-0.0575,  0.3255])\n",
      "------\n",
      "------\n",
      "Epoch 1304, Loss 3.248077, \n",
      "Params:  tensor([  5.0304, -15.3954])\n",
      "Grad:  tensor([-0.0574,  0.3249])\n",
      "------\n",
      "------\n",
      "Epoch 1305, Loss 3.246988, \n",
      "Params:  tensor([  5.0310, -15.3986])\n",
      "Grad:  tensor([-0.0573,  0.3244])\n",
      "------\n",
      "------\n",
      "Epoch 1306, Loss 3.245904, \n",
      "Params:  tensor([  5.0316, -15.4018])\n",
      "Grad:  tensor([-0.0572,  0.3238])\n",
      "------\n",
      "------\n",
      "Epoch 1307, Loss 3.244824, \n",
      "Params:  tensor([  5.0321, -15.4051])\n",
      "Grad:  tensor([-0.0571,  0.3233])\n",
      "------\n",
      "------\n",
      "Epoch 1308, Loss 3.243747, \n",
      "Params:  tensor([  5.0327, -15.4083])\n",
      "Grad:  tensor([-0.0570,  0.3227])\n",
      "------\n",
      "------\n",
      "Epoch 1309, Loss 3.242674, \n",
      "Params:  tensor([  5.0333, -15.4115])\n",
      "Grad:  tensor([-0.0569,  0.3222])\n",
      "------\n",
      "------\n",
      "Epoch 1310, Loss 3.241606, \n",
      "Params:  tensor([  5.0338, -15.4147])\n",
      "Grad:  tensor([-0.0568,  0.3216])\n",
      "------\n",
      "------\n",
      "Epoch 1311, Loss 3.240538, \n",
      "Params:  tensor([  5.0344, -15.4179])\n",
      "Grad:  tensor([-0.0567,  0.3211])\n",
      "------\n",
      "------\n",
      "Epoch 1312, Loss 3.239475, \n",
      "Params:  tensor([  5.0350, -15.4211])\n",
      "Grad:  tensor([-0.0566,  0.3205])\n",
      "------\n",
      "------\n",
      "Epoch 1313, Loss 3.238419, \n",
      "Params:  tensor([  5.0355, -15.4243])\n",
      "Grad:  tensor([-0.0565,  0.3200])\n",
      "------\n",
      "------\n",
      "Epoch 1314, Loss 3.237363, \n",
      "Params:  tensor([  5.0361, -15.4275])\n",
      "Grad:  tensor([-0.0564,  0.3194])\n",
      "------\n",
      "------\n",
      "Epoch 1315, Loss 3.236314, \n",
      "Params:  tensor([  5.0367, -15.4307])\n",
      "Grad:  tensor([-0.0563,  0.3189])\n",
      "------\n",
      "------\n",
      "Epoch 1316, Loss 3.235265, \n",
      "Params:  tensor([  5.0372, -15.4339])\n",
      "Grad:  tensor([-0.0562,  0.3184])\n",
      "------\n",
      "------\n",
      "Epoch 1317, Loss 3.234218, \n",
      "Params:  tensor([  5.0378, -15.4371])\n",
      "Grad:  tensor([-0.0561,  0.3178])\n",
      "------\n",
      "------\n",
      "Epoch 1318, Loss 3.233179, \n",
      "Params:  tensor([  5.0383, -15.4403])\n",
      "Grad:  tensor([-0.0561,  0.3173])\n",
      "------\n",
      "------\n",
      "Epoch 1319, Loss 3.232143, \n",
      "Params:  tensor([  5.0389, -15.4434])\n",
      "Grad:  tensor([-0.0560,  0.3167])\n",
      "------\n",
      "------\n",
      "Epoch 1320, Loss 3.231109, \n",
      "Params:  tensor([  5.0395, -15.4466])\n",
      "Grad:  tensor([-0.0558,  0.3162])\n",
      "------\n",
      "------\n",
      "Epoch 1321, Loss 3.230078, \n",
      "Params:  tensor([  5.0400, -15.4498])\n",
      "Grad:  tensor([-0.0558,  0.3157])\n",
      "------\n",
      "------\n",
      "Epoch 1322, Loss 3.229051, \n",
      "Params:  tensor([  5.0406, -15.4529])\n",
      "Grad:  tensor([-0.0557,  0.3151])\n",
      "------\n",
      "------\n",
      "Epoch 1323, Loss 3.228027, \n",
      "Params:  tensor([  5.0411, -15.4560])\n",
      "Grad:  tensor([-0.0556,  0.3146])\n",
      "------\n",
      "------\n",
      "Epoch 1324, Loss 3.227010, \n",
      "Params:  tensor([  5.0417, -15.4592])\n",
      "Grad:  tensor([-0.0555,  0.3141])\n",
      "------\n",
      "------\n",
      "Epoch 1325, Loss 3.225992, \n",
      "Params:  tensor([  5.0422, -15.4623])\n",
      "Grad:  tensor([-0.0554,  0.3135])\n",
      "------\n",
      "------\n",
      "Epoch 1326, Loss 3.224979, \n",
      "Params:  tensor([  5.0428, -15.4655])\n",
      "Grad:  tensor([-0.0553,  0.3130])\n",
      "------\n",
      "------\n",
      "Epoch 1327, Loss 3.223971, \n",
      "Params:  tensor([  5.0433, -15.4686])\n",
      "Grad:  tensor([-0.0552,  0.3125])\n",
      "------\n",
      "------\n",
      "Epoch 1328, Loss 3.222965, \n",
      "Params:  tensor([  5.0439, -15.4717])\n",
      "Grad:  tensor([-0.0551,  0.3119])\n",
      "------\n",
      "------\n",
      "Epoch 1329, Loss 3.221960, \n",
      "Params:  tensor([  5.0444, -15.4748])\n",
      "Grad:  tensor([-0.0550,  0.3114])\n",
      "------\n",
      "------\n",
      "Epoch 1330, Loss 3.220962, \n",
      "Params:  tensor([  5.0450, -15.4779])\n",
      "Grad:  tensor([-0.0549,  0.3109])\n",
      "------\n",
      "------\n",
      "Epoch 1331, Loss 3.219967, \n",
      "Params:  tensor([  5.0455, -15.4810])\n",
      "Grad:  tensor([-0.0548,  0.3103])\n",
      "------\n",
      "------\n",
      "Epoch 1332, Loss 3.218975, \n",
      "Params:  tensor([  5.0461, -15.4841])\n",
      "Grad:  tensor([-0.0547,  0.3098])\n",
      "------\n",
      "------\n",
      "Epoch 1333, Loss 3.217986, \n",
      "Params:  tensor([  5.0466, -15.4872])\n",
      "Grad:  tensor([-0.0546,  0.3093])\n",
      "------\n",
      "------\n",
      "Epoch 1334, Loss 3.217000, \n",
      "Params:  tensor([  5.0472, -15.4903])\n",
      "Grad:  tensor([-0.0545,  0.3088])\n",
      "------\n",
      "------\n",
      "Epoch 1335, Loss 3.216017, \n",
      "Params:  tensor([  5.0477, -15.4934])\n",
      "Grad:  tensor([-0.0544,  0.3082])\n",
      "------\n",
      "------\n",
      "Epoch 1336, Loss 3.215038, \n",
      "Params:  tensor([  5.0483, -15.4965])\n",
      "Grad:  tensor([-0.0543,  0.3077])\n",
      "------\n",
      "------\n",
      "Epoch 1337, Loss 3.214062, \n",
      "Params:  tensor([  5.0488, -15.4995])\n",
      "Grad:  tensor([-0.0543,  0.3072])\n",
      "------\n",
      "------\n",
      "Epoch 1338, Loss 3.213092, \n",
      "Params:  tensor([  5.0494, -15.5026])\n",
      "Grad:  tensor([-0.0542,  0.3067])\n",
      "------\n",
      "------\n",
      "Epoch 1339, Loss 3.212122, \n",
      "Params:  tensor([  5.0499, -15.5057])\n",
      "Grad:  tensor([-0.0541,  0.3061])\n",
      "------\n",
      "------\n",
      "Epoch 1340, Loss 3.211157, \n",
      "Params:  tensor([  5.0504, -15.5087])\n",
      "Grad:  tensor([-0.0540,  0.3056])\n",
      "------\n",
      "------\n",
      "Epoch 1341, Loss 3.210192, \n",
      "Params:  tensor([  5.0510, -15.5118])\n",
      "Grad:  tensor([-0.0539,  0.3051])\n",
      "------\n",
      "------\n",
      "Epoch 1342, Loss 3.209235, \n",
      "Params:  tensor([  5.0515, -15.5148])\n",
      "Grad:  tensor([-0.0538,  0.3046])\n",
      "------\n",
      "------\n",
      "Epoch 1343, Loss 3.208279, \n",
      "Params:  tensor([  5.0521, -15.5179])\n",
      "Grad:  tensor([-0.0537,  0.3041])\n",
      "------\n",
      "------\n",
      "Epoch 1344, Loss 3.207326, \n",
      "Params:  tensor([  5.0526, -15.5209])\n",
      "Grad:  tensor([-0.0536,  0.3036])\n",
      "------\n",
      "------\n",
      "Epoch 1345, Loss 3.206376, \n",
      "Params:  tensor([  5.0531, -15.5239])\n",
      "Grad:  tensor([-0.0535,  0.3030])\n",
      "------\n",
      "------\n",
      "Epoch 1346, Loss 3.205430, \n",
      "Params:  tensor([  5.0537, -15.5269])\n",
      "Grad:  tensor([-0.0534,  0.3025])\n",
      "------\n",
      "------\n",
      "Epoch 1347, Loss 3.204488, \n",
      "Params:  tensor([  5.0542, -15.5300])\n",
      "Grad:  tensor([-0.0533,  0.3020])\n",
      "------\n",
      "------\n",
      "Epoch 1348, Loss 3.203547, \n",
      "Params:  tensor([  5.0547, -15.5330])\n",
      "Grad:  tensor([-0.0532,  0.3015])\n",
      "------\n",
      "------\n",
      "Epoch 1349, Loss 3.202610, \n",
      "Params:  tensor([  5.0553, -15.5360])\n",
      "Grad:  tensor([-0.0532,  0.3010])\n",
      "------\n",
      "------\n",
      "Epoch 1350, Loss 3.201678, \n",
      "Params:  tensor([  5.0558, -15.5390])\n",
      "Grad:  tensor([-0.0531,  0.3005])\n",
      "------\n",
      "------\n",
      "Epoch 1351, Loss 3.200747, \n",
      "Params:  tensor([  5.0563, -15.5420])\n",
      "Grad:  tensor([-0.0530,  0.3000])\n",
      "------\n",
      "------\n",
      "Epoch 1352, Loss 3.199820, \n",
      "Params:  tensor([  5.0568, -15.5450])\n",
      "Grad:  tensor([-0.0529,  0.2995])\n",
      "------\n",
      "------\n",
      "Epoch 1353, Loss 3.198897, \n",
      "Params:  tensor([  5.0574, -15.5480])\n",
      "Grad:  tensor([-0.0528,  0.2989])\n",
      "------\n",
      "------\n",
      "Epoch 1354, Loss 3.197976, \n",
      "Params:  tensor([  5.0579, -15.5510])\n",
      "Grad:  tensor([-0.0527,  0.2984])\n",
      "------\n",
      "------\n",
      "Epoch 1355, Loss 3.197060, \n",
      "Params:  tensor([  5.0584, -15.5539])\n",
      "Grad:  tensor([-0.0526,  0.2979])\n",
      "------\n",
      "------\n",
      "Epoch 1356, Loss 3.196143, \n",
      "Params:  tensor([  5.0590, -15.5569])\n",
      "Grad:  tensor([-0.0525,  0.2974])\n",
      "------\n",
      "------\n",
      "Epoch 1357, Loss 3.195231, \n",
      "Params:  tensor([  5.0595, -15.5599])\n",
      "Grad:  tensor([-0.0524,  0.2969])\n",
      "------\n",
      "------\n",
      "Epoch 1358, Loss 3.194324, \n",
      "Params:  tensor([  5.0600, -15.5629])\n",
      "Grad:  tensor([-0.0524,  0.2964])\n",
      "------\n",
      "------\n",
      "Epoch 1359, Loss 3.193420, \n",
      "Params:  tensor([  5.0605, -15.5658])\n",
      "Grad:  tensor([-0.0523,  0.2959])\n",
      "------\n",
      "------\n",
      "Epoch 1360, Loss 3.192517, \n",
      "Params:  tensor([  5.0610, -15.5688])\n",
      "Grad:  tensor([-0.0522,  0.2954])\n",
      "------\n",
      "------\n",
      "Epoch 1361, Loss 3.191616, \n",
      "Params:  tensor([  5.0616, -15.5717])\n",
      "Grad:  tensor([-0.0521,  0.2949])\n",
      "------\n",
      "------\n",
      "Epoch 1362, Loss 3.190720, \n",
      "Params:  tensor([  5.0621, -15.5747])\n",
      "Grad:  tensor([-0.0520,  0.2944])\n",
      "------\n",
      "------\n",
      "Epoch 1363, Loss 3.189829, \n",
      "Params:  tensor([  5.0626, -15.5776])\n",
      "Grad:  tensor([-0.0519,  0.2939])\n",
      "------\n",
      "------\n",
      "Epoch 1364, Loss 3.188938, \n",
      "Params:  tensor([  5.0631, -15.5805])\n",
      "Grad:  tensor([-0.0518,  0.2934])\n",
      "------\n",
      "------\n",
      "Epoch 1365, Loss 3.188051, \n",
      "Params:  tensor([  5.0636, -15.5835])\n",
      "Grad:  tensor([-0.0517,  0.2929])\n",
      "------\n",
      "------\n",
      "Epoch 1366, Loss 3.187166, \n",
      "Params:  tensor([  5.0642, -15.5864])\n",
      "Grad:  tensor([-0.0516,  0.2924])\n",
      "------\n",
      "------\n",
      "Epoch 1367, Loss 3.186287, \n",
      "Params:  tensor([  5.0647, -15.5893])\n",
      "Grad:  tensor([-0.0516,  0.2919])\n",
      "------\n",
      "------\n",
      "Epoch 1368, Loss 3.185409, \n",
      "Params:  tensor([  5.0652, -15.5922])\n",
      "Grad:  tensor([-0.0515,  0.2914])\n",
      "------\n",
      "------\n",
      "Epoch 1369, Loss 3.184534, \n",
      "Params:  tensor([  5.0657, -15.5951])\n",
      "Grad:  tensor([-0.0514,  0.2909])\n",
      "------\n",
      "------\n",
      "Epoch 1370, Loss 3.183662, \n",
      "Params:  tensor([  5.0662, -15.5980])\n",
      "Grad:  tensor([-0.0513,  0.2904])\n",
      "------\n",
      "------\n",
      "Epoch 1371, Loss 3.182792, \n",
      "Params:  tensor([  5.0667, -15.6009])\n",
      "Grad:  tensor([-0.0512,  0.2899])\n",
      "------\n",
      "------\n",
      "Epoch 1372, Loss 3.181925, \n",
      "Params:  tensor([  5.0672, -15.6038])\n",
      "Grad:  tensor([-0.0511,  0.2894])\n",
      "------\n",
      "------\n",
      "Epoch 1373, Loss 3.181063, \n",
      "Params:  tensor([  5.0678, -15.6067])\n",
      "Grad:  tensor([-0.0510,  0.2890])\n",
      "------\n",
      "------\n",
      "Epoch 1374, Loss 3.180201, \n",
      "Params:  tensor([  5.0683, -15.6096])\n",
      "Grad:  tensor([-0.0509,  0.2885])\n",
      "------\n",
      "------\n",
      "Epoch 1375, Loss 3.179347, \n",
      "Params:  tensor([  5.0688, -15.6125])\n",
      "Grad:  tensor([-0.0509,  0.2880])\n",
      "------\n",
      "------\n",
      "Epoch 1376, Loss 3.178490, \n",
      "Params:  tensor([  5.0693, -15.6154])\n",
      "Grad:  tensor([-0.0508,  0.2875])\n",
      "------\n",
      "------\n",
      "Epoch 1377, Loss 3.177638, \n",
      "Params:  tensor([  5.0698, -15.6182])\n",
      "Grad:  tensor([-0.0507,  0.2870])\n",
      "------\n",
      "------\n",
      "Epoch 1378, Loss 3.176789, \n",
      "Params:  tensor([  5.0703, -15.6211])\n",
      "Grad:  tensor([-0.0506,  0.2865])\n",
      "------\n",
      "------\n",
      "Epoch 1379, Loss 3.175945, \n",
      "Params:  tensor([  5.0708, -15.6240])\n",
      "Grad:  tensor([-0.0505,  0.2860])\n",
      "------\n",
      "------\n",
      "Epoch 1380, Loss 3.175101, \n",
      "Params:  tensor([  5.0713, -15.6268])\n",
      "Grad:  tensor([-0.0504,  0.2855])\n",
      "------\n",
      "------\n",
      "Epoch 1381, Loss 3.174262, \n",
      "Params:  tensor([  5.0718, -15.6297])\n",
      "Grad:  tensor([-0.0504,  0.2850])\n",
      "------\n",
      "------\n",
      "Epoch 1382, Loss 3.173425, \n",
      "Params:  tensor([  5.0723, -15.6325])\n",
      "Grad:  tensor([-0.0503,  0.2846])\n",
      "------\n",
      "------\n",
      "Epoch 1383, Loss 3.172590, \n",
      "Params:  tensor([  5.0728, -15.6353])\n",
      "Grad:  tensor([-0.0502,  0.2841])\n",
      "------\n",
      "------\n",
      "Epoch 1384, Loss 3.171759, \n",
      "Params:  tensor([  5.0733, -15.6382])\n",
      "Grad:  tensor([-0.0501,  0.2836])\n",
      "------\n",
      "------\n",
      "Epoch 1385, Loss 3.170929, \n",
      "Params:  tensor([  5.0738, -15.6410])\n",
      "Grad:  tensor([-0.0500,  0.2831])\n",
      "------\n",
      "------\n",
      "Epoch 1386, Loss 3.170103, \n",
      "Params:  tensor([  5.0743, -15.6438])\n",
      "Grad:  tensor([-0.0499,  0.2826])\n",
      "------\n",
      "------\n",
      "Epoch 1387, Loss 3.169280, \n",
      "Params:  tensor([  5.0748, -15.6467])\n",
      "Grad:  tensor([-0.0498,  0.2822])\n",
      "------\n",
      "------\n",
      "Epoch 1388, Loss 3.168462, \n",
      "Params:  tensor([  5.0753, -15.6495])\n",
      "Grad:  tensor([-0.0498,  0.2817])\n",
      "------\n",
      "------\n",
      "Epoch 1389, Loss 3.167644, \n",
      "Params:  tensor([  5.0758, -15.6523])\n",
      "Grad:  tensor([-0.0497,  0.2812])\n",
      "------\n",
      "------\n",
      "Epoch 1390, Loss 3.166827, \n",
      "Params:  tensor([  5.0763, -15.6551])\n",
      "Grad:  tensor([-0.0496,  0.2807])\n",
      "------\n",
      "------\n",
      "Epoch 1391, Loss 3.166017, \n",
      "Params:  tensor([  5.0768, -15.6579])\n",
      "Grad:  tensor([-0.0495,  0.2802])\n",
      "------\n",
      "------\n",
      "Epoch 1392, Loss 3.165207, \n",
      "Params:  tensor([  5.0773, -15.6607])\n",
      "Grad:  tensor([-0.0494,  0.2798])\n",
      "------\n",
      "------\n",
      "Epoch 1393, Loss 3.164401, \n",
      "Params:  tensor([  5.0778, -15.6635])\n",
      "Grad:  tensor([-0.0493,  0.2793])\n",
      "------\n",
      "------\n",
      "Epoch 1394, Loss 3.163594, \n",
      "Params:  tensor([  5.0783, -15.6663])\n",
      "Grad:  tensor([-0.0492,  0.2788])\n",
      "------\n",
      "------\n",
      "Epoch 1395, Loss 3.162795, \n",
      "Params:  tensor([  5.0788, -15.6691])\n",
      "Grad:  tensor([-0.0492,  0.2783])\n",
      "------\n",
      "------\n",
      "Epoch 1396, Loss 3.161996, \n",
      "Params:  tensor([  5.0793, -15.6718])\n",
      "Grad:  tensor([-0.0491,  0.2779])\n",
      "------\n",
      "------\n",
      "Epoch 1397, Loss 3.161201, \n",
      "Params:  tensor([  5.0797, -15.6746])\n",
      "Grad:  tensor([-0.0490,  0.2774])\n",
      "------\n",
      "------\n",
      "Epoch 1398, Loss 3.160410, \n",
      "Params:  tensor([  5.0802, -15.6774])\n",
      "Grad:  tensor([-0.0489,  0.2769])\n",
      "------\n",
      "------\n",
      "Epoch 1399, Loss 3.159618, \n",
      "Params:  tensor([  5.0807, -15.6802])\n",
      "Grad:  tensor([-0.0488,  0.2765])\n",
      "------\n",
      "------\n",
      "Epoch 1400, Loss 3.158830, \n",
      "Params:  tensor([  5.0812, -15.6829])\n",
      "Grad:  tensor([-0.0488,  0.2760])\n",
      "------\n",
      "------\n",
      "Epoch 1401, Loss 3.158046, \n",
      "Params:  tensor([  5.0817, -15.6857])\n",
      "Grad:  tensor([-0.0487,  0.2755])\n",
      "------\n",
      "------\n",
      "Epoch 1402, Loss 3.157263, \n",
      "Params:  tensor([  5.0822, -15.6884])\n",
      "Grad:  tensor([-0.0486,  0.2751])\n",
      "------\n",
      "------\n",
      "Epoch 1403, Loss 3.156484, \n",
      "Params:  tensor([  5.0827, -15.6912])\n",
      "Grad:  tensor([-0.0485,  0.2746])\n",
      "------\n",
      "------\n",
      "Epoch 1404, Loss 3.155708, \n",
      "Params:  tensor([  5.0832, -15.6939])\n",
      "Grad:  tensor([-0.0484,  0.2741])\n",
      "------\n",
      "------\n",
      "Epoch 1405, Loss 3.154933, \n",
      "Params:  tensor([  5.0836, -15.6966])\n",
      "Grad:  tensor([-0.0483,  0.2736])\n",
      "------\n",
      "------\n",
      "Epoch 1406, Loss 3.154162, \n",
      "Params:  tensor([  5.0841, -15.6994])\n",
      "Grad:  tensor([-0.0483,  0.2732])\n",
      "------\n",
      "------\n",
      "Epoch 1407, Loss 3.153393, \n",
      "Params:  tensor([  5.0846, -15.7021])\n",
      "Grad:  tensor([-0.0482,  0.2727])\n",
      "------\n",
      "------\n",
      "Epoch 1408, Loss 3.152628, \n",
      "Params:  tensor([  5.0851, -15.7048])\n",
      "Grad:  tensor([-0.0481,  0.2723])\n",
      "------\n",
      "------\n",
      "Epoch 1409, Loss 3.151865, \n",
      "Params:  tensor([  5.0856, -15.7075])\n",
      "Grad:  tensor([-0.0480,  0.2718])\n",
      "------\n",
      "------\n",
      "Epoch 1410, Loss 3.151101, \n",
      "Params:  tensor([  5.0860, -15.7103])\n",
      "Grad:  tensor([-0.0479,  0.2713])\n",
      "------\n",
      "------\n",
      "Epoch 1411, Loss 3.150343, \n",
      "Params:  tensor([  5.0865, -15.7130])\n",
      "Grad:  tensor([-0.0479,  0.2709])\n",
      "------\n",
      "------\n",
      "Epoch 1412, Loss 3.149587, \n",
      "Params:  tensor([  5.0870, -15.7157])\n",
      "Grad:  tensor([-0.0478,  0.2704])\n",
      "------\n",
      "------\n",
      "Epoch 1413, Loss 3.148833, \n",
      "Params:  tensor([  5.0875, -15.7184])\n",
      "Grad:  tensor([-0.0477,  0.2700])\n",
      "------\n",
      "------\n",
      "Epoch 1414, Loss 3.148083, \n",
      "Params:  tensor([  5.0879, -15.7211])\n",
      "Grad:  tensor([-0.0476,  0.2695])\n",
      "------\n",
      "------\n",
      "Epoch 1415, Loss 3.147335, \n",
      "Params:  tensor([  5.0884, -15.7238])\n",
      "Grad:  tensor([-0.0475,  0.2690])\n",
      "------\n",
      "------\n",
      "Epoch 1416, Loss 3.146588, \n",
      "Params:  tensor([  5.0889, -15.7264])\n",
      "Grad:  tensor([-0.0474,  0.2686])\n",
      "------\n",
      "------\n",
      "Epoch 1417, Loss 3.145845, \n",
      "Params:  tensor([  5.0894, -15.7291])\n",
      "Grad:  tensor([-0.0474,  0.2681])\n",
      "------\n",
      "------\n",
      "Epoch 1418, Loss 3.145105, \n",
      "Params:  tensor([  5.0898, -15.7318])\n",
      "Grad:  tensor([-0.0473,  0.2677])\n",
      "------\n",
      "------\n",
      "Epoch 1419, Loss 3.144367, \n",
      "Params:  tensor([  5.0903, -15.7345])\n",
      "Grad:  tensor([-0.0472,  0.2672])\n",
      "------\n",
      "------\n",
      "Epoch 1420, Loss 3.143630, \n",
      "Params:  tensor([  5.0908, -15.7371])\n",
      "Grad:  tensor([-0.0471,  0.2668])\n",
      "------\n",
      "------\n",
      "Epoch 1421, Loss 3.142899, \n",
      "Params:  tensor([  5.0913, -15.7398])\n",
      "Grad:  tensor([-0.0470,  0.2663])\n",
      "------\n",
      "------\n",
      "Epoch 1422, Loss 3.142166, \n",
      "Params:  tensor([  5.0917, -15.7425])\n",
      "Grad:  tensor([-0.0469,  0.2659])\n",
      "------\n",
      "------\n",
      "Epoch 1423, Loss 3.141439, \n",
      "Params:  tensor([  5.0922, -15.7451])\n",
      "Grad:  tensor([-0.0469,  0.2654])\n",
      "------\n",
      "------\n",
      "Epoch 1424, Loss 3.140712, \n",
      "Params:  tensor([  5.0927, -15.7478])\n",
      "Grad:  tensor([-0.0468,  0.2649])\n",
      "------\n",
      "------\n",
      "Epoch 1425, Loss 3.139989, \n",
      "Params:  tensor([  5.0931, -15.7504])\n",
      "Grad:  tensor([-0.0467,  0.2645])\n",
      "------\n",
      "------\n",
      "Epoch 1426, Loss 3.139271, \n",
      "Params:  tensor([  5.0936, -15.7530])\n",
      "Grad:  tensor([-0.0466,  0.2641])\n",
      "------\n",
      "------\n",
      "Epoch 1427, Loss 3.138551, \n",
      "Params:  tensor([  5.0941, -15.7557])\n",
      "Grad:  tensor([-0.0466,  0.2636])\n",
      "------\n",
      "------\n",
      "Epoch 1428, Loss 3.137835, \n",
      "Params:  tensor([  5.0945, -15.7583])\n",
      "Grad:  tensor([-0.0465,  0.2632])\n",
      "------\n",
      "------\n",
      "Epoch 1429, Loss 3.137121, \n",
      "Params:  tensor([  5.0950, -15.7609])\n",
      "Grad:  tensor([-0.0464,  0.2627])\n",
      "------\n",
      "------\n",
      "Epoch 1430, Loss 3.136409, \n",
      "Params:  tensor([  5.0955, -15.7636])\n",
      "Grad:  tensor([-0.0463,  0.2623])\n",
      "------\n",
      "------\n",
      "Epoch 1431, Loss 3.135702, \n",
      "Params:  tensor([  5.0959, -15.7662])\n",
      "Grad:  tensor([-0.0462,  0.2618])\n",
      "------\n",
      "------\n",
      "Epoch 1432, Loss 3.134994, \n",
      "Params:  tensor([  5.0964, -15.7688])\n",
      "Grad:  tensor([-0.0461,  0.2614])\n",
      "------\n",
      "------\n",
      "Epoch 1433, Loss 3.134292, \n",
      "Params:  tensor([  5.0968, -15.7714])\n",
      "Grad:  tensor([-0.0461,  0.2609])\n",
      "------\n",
      "------\n",
      "Epoch 1434, Loss 3.133590, \n",
      "Params:  tensor([  5.0973, -15.7740])\n",
      "Grad:  tensor([-0.0460,  0.2605])\n",
      "------\n",
      "------\n",
      "Epoch 1435, Loss 3.132889, \n",
      "Params:  tensor([  5.0978, -15.7766])\n",
      "Grad:  tensor([-0.0459,  0.2600])\n",
      "------\n",
      "------\n",
      "Epoch 1436, Loss 3.132194, \n",
      "Params:  tensor([  5.0982, -15.7792])\n",
      "Grad:  tensor([-0.0459,  0.2596])\n",
      "------\n",
      "------\n",
      "Epoch 1437, Loss 3.131500, \n",
      "Params:  tensor([  5.0987, -15.7818])\n",
      "Grad:  tensor([-0.0458,  0.2592])\n",
      "------\n",
      "------\n",
      "Epoch 1438, Loss 3.130810, \n",
      "Params:  tensor([  5.0991, -15.7844])\n",
      "Grad:  tensor([-0.0457,  0.2587])\n",
      "------\n",
      "------\n",
      "Epoch 1439, Loss 3.130119, \n",
      "Params:  tensor([  5.0996, -15.7870])\n",
      "Grad:  tensor([-0.0456,  0.2583])\n",
      "------\n",
      "------\n",
      "Epoch 1440, Loss 3.129432, \n",
      "Params:  tensor([  5.1000, -15.7895])\n",
      "Grad:  tensor([-0.0455,  0.2578])\n",
      "------\n",
      "------\n",
      "Epoch 1441, Loss 3.128746, \n",
      "Params:  tensor([  5.1005, -15.7921])\n",
      "Grad:  tensor([-0.0455,  0.2574])\n",
      "------\n",
      "------\n",
      "Epoch 1442, Loss 3.128064, \n",
      "Params:  tensor([  5.1010, -15.7947])\n",
      "Grad:  tensor([-0.0454,  0.2570])\n",
      "------\n",
      "------\n",
      "Epoch 1443, Loss 3.127382, \n",
      "Params:  tensor([  5.1014, -15.7973])\n",
      "Grad:  tensor([-0.0453,  0.2565])\n",
      "------\n",
      "------\n",
      "Epoch 1444, Loss 3.126705, \n",
      "Params:  tensor([  5.1019, -15.7998])\n",
      "Grad:  tensor([-0.0453,  0.2561])\n",
      "------\n",
      "------\n",
      "Epoch 1445, Loss 3.126030, \n",
      "Params:  tensor([  5.1023, -15.8024])\n",
      "Grad:  tensor([-0.0452,  0.2557])\n",
      "------\n",
      "------\n",
      "Epoch 1446, Loss 3.125356, \n",
      "Params:  tensor([  5.1028, -15.8049])\n",
      "Grad:  tensor([-0.0451,  0.2552])\n",
      "------\n",
      "------\n",
      "Epoch 1447, Loss 3.124683, \n",
      "Params:  tensor([  5.1032, -15.8075])\n",
      "Grad:  tensor([-0.0450,  0.2548])\n",
      "------\n",
      "------\n",
      "Epoch 1448, Loss 3.124016, \n",
      "Params:  tensor([  5.1037, -15.8100])\n",
      "Grad:  tensor([-0.0449,  0.2544])\n",
      "------\n",
      "------\n",
      "Epoch 1449, Loss 3.123349, \n",
      "Params:  tensor([  5.1041, -15.8126])\n",
      "Grad:  tensor([-0.0449,  0.2539])\n",
      "------\n",
      "------\n",
      "Epoch 1450, Loss 3.122686, \n",
      "Params:  tensor([  5.1046, -15.8151])\n",
      "Grad:  tensor([-0.0448,  0.2535])\n",
      "------\n",
      "------\n",
      "Epoch 1451, Loss 3.122022, \n",
      "Params:  tensor([  5.1050, -15.8176])\n",
      "Grad:  tensor([-0.0447,  0.2531])\n",
      "------\n",
      "------\n",
      "Epoch 1452, Loss 3.121362, \n",
      "Params:  tensor([  5.1055, -15.8201])\n",
      "Grad:  tensor([-0.0446,  0.2526])\n",
      "------\n",
      "------\n",
      "Epoch 1453, Loss 3.120707, \n",
      "Params:  tensor([  5.1059, -15.8227])\n",
      "Grad:  tensor([-0.0445,  0.2522])\n",
      "------\n",
      "------\n",
      "Epoch 1454, Loss 3.120049, \n",
      "Params:  tensor([  5.1063, -15.8252])\n",
      "Grad:  tensor([-0.0445,  0.2518])\n",
      "------\n",
      "------\n",
      "Epoch 1455, Loss 3.119397, \n",
      "Params:  tensor([  5.1068, -15.8277])\n",
      "Grad:  tensor([-0.0444,  0.2513])\n",
      "------\n",
      "------\n",
      "Epoch 1456, Loss 3.118746, \n",
      "Params:  tensor([  5.1072, -15.8302])\n",
      "Grad:  tensor([-0.0443,  0.2509])\n",
      "------\n",
      "------\n",
      "Epoch 1457, Loss 3.118098, \n",
      "Params:  tensor([  5.1077, -15.8327])\n",
      "Grad:  tensor([-0.0442,  0.2505])\n",
      "------\n",
      "------\n",
      "Epoch 1458, Loss 3.117451, \n",
      "Params:  tensor([  5.1081, -15.8352])\n",
      "Grad:  tensor([-0.0442,  0.2501])\n",
      "------\n",
      "------\n",
      "Epoch 1459, Loss 3.116805, \n",
      "Params:  tensor([  5.1086, -15.8377])\n",
      "Grad:  tensor([-0.0441,  0.2496])\n",
      "------\n",
      "------\n",
      "Epoch 1460, Loss 3.116164, \n",
      "Params:  tensor([  5.1090, -15.8402])\n",
      "Grad:  tensor([-0.0440,  0.2492])\n",
      "------\n",
      "------\n",
      "Epoch 1461, Loss 3.115525, \n",
      "Params:  tensor([  5.1094, -15.8427])\n",
      "Grad:  tensor([-0.0439,  0.2488])\n",
      "------\n",
      "------\n",
      "Epoch 1462, Loss 3.114886, \n",
      "Params:  tensor([  5.1099, -15.8452])\n",
      "Grad:  tensor([-0.0439,  0.2484])\n",
      "------\n",
      "------\n",
      "Epoch 1463, Loss 3.114251, \n",
      "Params:  tensor([  5.1103, -15.8477])\n",
      "Grad:  tensor([-0.0438,  0.2480])\n",
      "------\n",
      "------\n",
      "Epoch 1464, Loss 3.113617, \n",
      "Params:  tensor([  5.1107, -15.8501])\n",
      "Grad:  tensor([-0.0437,  0.2475])\n",
      "------\n",
      "------\n",
      "Epoch 1465, Loss 3.112985, \n",
      "Params:  tensor([  5.1112, -15.8526])\n",
      "Grad:  tensor([-0.0437,  0.2471])\n",
      "------\n",
      "------\n",
      "Epoch 1466, Loss 3.112358, \n",
      "Params:  tensor([  5.1116, -15.8551])\n",
      "Grad:  tensor([-0.0436,  0.2467])\n",
      "------\n",
      "------\n",
      "Epoch 1467, Loss 3.111731, \n",
      "Params:  tensor([  5.1121, -15.8575])\n",
      "Grad:  tensor([-0.0435,  0.2463])\n",
      "------\n",
      "------\n",
      "Epoch 1468, Loss 3.111103, \n",
      "Params:  tensor([  5.1125, -15.8600])\n",
      "Grad:  tensor([-0.0434,  0.2459])\n",
      "------\n",
      "------\n",
      "Epoch 1469, Loss 3.110484, \n",
      "Params:  tensor([  5.1129, -15.8624])\n",
      "Grad:  tensor([-0.0433,  0.2454])\n",
      "------\n",
      "------\n",
      "Epoch 1470, Loss 3.109859, \n",
      "Params:  tensor([  5.1134, -15.8649])\n",
      "Grad:  tensor([-0.0433,  0.2450])\n",
      "------\n",
      "------\n",
      "Epoch 1471, Loss 3.109243, \n",
      "Params:  tensor([  5.1138, -15.8673])\n",
      "Grad:  tensor([-0.0432,  0.2446])\n",
      "------\n",
      "------\n",
      "Epoch 1472, Loss 3.108627, \n",
      "Params:  tensor([  5.1142, -15.8698])\n",
      "Grad:  tensor([-0.0431,  0.2442])\n",
      "------\n",
      "------\n",
      "Epoch 1473, Loss 3.108011, \n",
      "Params:  tensor([  5.1147, -15.8722])\n",
      "Grad:  tensor([-0.0430,  0.2438])\n",
      "------\n",
      "------\n",
      "Epoch 1474, Loss 3.107401, \n",
      "Params:  tensor([  5.1151, -15.8747])\n",
      "Grad:  tensor([-0.0430,  0.2434])\n",
      "------\n",
      "------\n",
      "Epoch 1475, Loss 3.106791, \n",
      "Params:  tensor([  5.1155, -15.8771])\n",
      "Grad:  tensor([-0.0429,  0.2429])\n",
      "------\n",
      "------\n",
      "Epoch 1476, Loss 3.106180, \n",
      "Params:  tensor([  5.1159, -15.8795])\n",
      "Grad:  tensor([-0.0428,  0.2425])\n",
      "------\n",
      "------\n",
      "Epoch 1477, Loss 3.105575, \n",
      "Params:  tensor([  5.1164, -15.8819])\n",
      "Grad:  tensor([-0.0428,  0.2421])\n",
      "------\n",
      "------\n",
      "Epoch 1478, Loss 3.104972, \n",
      "Params:  tensor([  5.1168, -15.8843])\n",
      "Grad:  tensor([-0.0427,  0.2417])\n",
      "------\n",
      "------\n",
      "Epoch 1479, Loss 3.104370, \n",
      "Params:  tensor([  5.1172, -15.8868])\n",
      "Grad:  tensor([-0.0426,  0.2413])\n",
      "------\n",
      "------\n",
      "Epoch 1480, Loss 3.103770, \n",
      "Params:  tensor([  5.1176, -15.8892])\n",
      "Grad:  tensor([-0.0425,  0.2409])\n",
      "------\n",
      "------\n",
      "Epoch 1481, Loss 3.103172, \n",
      "Params:  tensor([  5.1181, -15.8916])\n",
      "Grad:  tensor([-0.0425,  0.2405])\n",
      "------\n",
      "------\n",
      "Epoch 1482, Loss 3.102576, \n",
      "Params:  tensor([  5.1185, -15.8940])\n",
      "Grad:  tensor([-0.0424,  0.2401])\n",
      "------\n",
      "------\n",
      "Epoch 1483, Loss 3.101982, \n",
      "Params:  tensor([  5.1189, -15.8964])\n",
      "Grad:  tensor([-0.0423,  0.2397])\n",
      "------\n",
      "------\n",
      "Epoch 1484, Loss 3.101390, \n",
      "Params:  tensor([  5.1193, -15.8988])\n",
      "Grad:  tensor([-0.0423,  0.2393])\n",
      "------\n",
      "------\n",
      "Epoch 1485, Loss 3.100802, \n",
      "Params:  tensor([  5.1198, -15.9011])\n",
      "Grad:  tensor([-0.0422,  0.2388])\n",
      "------\n",
      "------\n",
      "Epoch 1486, Loss 3.100213, \n",
      "Params:  tensor([  5.1202, -15.9035])\n",
      "Grad:  tensor([-0.0421,  0.2384])\n",
      "------\n",
      "------\n",
      "Epoch 1487, Loss 3.099627, \n",
      "Params:  tensor([  5.1206, -15.9059])\n",
      "Grad:  tensor([-0.0421,  0.2380])\n",
      "------\n",
      "------\n",
      "Epoch 1488, Loss 3.099044, \n",
      "Params:  tensor([  5.1210, -15.9083])\n",
      "Grad:  tensor([-0.0420,  0.2376])\n",
      "------\n",
      "------\n",
      "Epoch 1489, Loss 3.098463, \n",
      "Params:  tensor([  5.1214, -15.9107])\n",
      "Grad:  tensor([-0.0419,  0.2372])\n",
      "------\n",
      "------\n",
      "Epoch 1490, Loss 3.097883, \n",
      "Params:  tensor([  5.1219, -15.9130])\n",
      "Grad:  tensor([-0.0418,  0.2368])\n",
      "------\n",
      "------\n",
      "Epoch 1491, Loss 3.097302, \n",
      "Params:  tensor([  5.1223, -15.9154])\n",
      "Grad:  tensor([-0.0418,  0.2364])\n",
      "------\n",
      "------\n",
      "Epoch 1492, Loss 3.096727, \n",
      "Params:  tensor([  5.1227, -15.9178])\n",
      "Grad:  tensor([-0.0417,  0.2360])\n",
      "------\n",
      "------\n",
      "Epoch 1493, Loss 3.096153, \n",
      "Params:  tensor([  5.1231, -15.9201])\n",
      "Grad:  tensor([-0.0416,  0.2356])\n",
      "------\n",
      "------\n",
      "Epoch 1494, Loss 3.095583, \n",
      "Params:  tensor([  5.1235, -15.9225])\n",
      "Grad:  tensor([-0.0416,  0.2352])\n",
      "------\n",
      "------\n",
      "Epoch 1495, Loss 3.095011, \n",
      "Params:  tensor([  5.1239, -15.9248])\n",
      "Grad:  tensor([-0.0415,  0.2348])\n",
      "------\n",
      "------\n",
      "Epoch 1496, Loss 3.094444, \n",
      "Params:  tensor([  5.1244, -15.9272])\n",
      "Grad:  tensor([-0.0414,  0.2344])\n",
      "------\n",
      "------\n",
      "Epoch 1497, Loss 3.093877, \n",
      "Params:  tensor([  5.1248, -15.9295])\n",
      "Grad:  tensor([-0.0413,  0.2340])\n",
      "------\n",
      "------\n",
      "Epoch 1498, Loss 3.093314, \n",
      "Params:  tensor([  5.1252, -15.9318])\n",
      "Grad:  tensor([-0.0413,  0.2336])\n",
      "------\n",
      "------\n",
      "Epoch 1499, Loss 3.092751, \n",
      "Params:  tensor([  5.1256, -15.9342])\n",
      "Grad:  tensor([-0.0412,  0.2332])\n",
      "------\n",
      "------\n",
      "Epoch 1500, Loss 3.092191, \n",
      "Params:  tensor([  5.1260, -15.9365])\n",
      "Grad:  tensor([-0.0411,  0.2328])\n",
      "------\n",
      "------\n",
      "Epoch 1501, Loss 3.091630, \n",
      "Params:  tensor([  5.1264, -15.9388])\n",
      "Grad:  tensor([-0.0411,  0.2324])\n",
      "------\n",
      "------\n",
      "Epoch 1502, Loss 3.091074, \n",
      "Params:  tensor([  5.1268, -15.9411])\n",
      "Grad:  tensor([-0.0410,  0.2320])\n",
      "------\n",
      "------\n",
      "Epoch 1503, Loss 3.090520, \n",
      "Params:  tensor([  5.1272, -15.9435])\n",
      "Grad:  tensor([-0.0409,  0.2317])\n",
      "------\n",
      "------\n",
      "Epoch 1504, Loss 3.089969, \n",
      "Params:  tensor([  5.1276, -15.9458])\n",
      "Grad:  tensor([-0.0408,  0.2313])\n",
      "------\n",
      "------\n",
      "Epoch 1505, Loss 3.089417, \n",
      "Params:  tensor([  5.1281, -15.9481])\n",
      "Grad:  tensor([-0.0408,  0.2309])\n",
      "------\n",
      "------\n",
      "Epoch 1506, Loss 3.088867, \n",
      "Params:  tensor([  5.1285, -15.9504])\n",
      "Grad:  tensor([-0.0407,  0.2305])\n",
      "------\n",
      "------\n",
      "Epoch 1507, Loss 3.088320, \n",
      "Params:  tensor([  5.1289, -15.9527])\n",
      "Grad:  tensor([-0.0406,  0.2301])\n",
      "------\n",
      "------\n",
      "Epoch 1508, Loss 3.087775, \n",
      "Params:  tensor([  5.1293, -15.9550])\n",
      "Grad:  tensor([-0.0406,  0.2297])\n",
      "------\n",
      "------\n",
      "Epoch 1509, Loss 3.087232, \n",
      "Params:  tensor([  5.1297, -15.9573])\n",
      "Grad:  tensor([-0.0405,  0.2293])\n",
      "------\n",
      "------\n",
      "Epoch 1510, Loss 3.086690, \n",
      "Params:  tensor([  5.1301, -15.9596])\n",
      "Grad:  tensor([-0.0404,  0.2289])\n",
      "------\n",
      "------\n",
      "Epoch 1511, Loss 3.086150, \n",
      "Params:  tensor([  5.1305, -15.9618])\n",
      "Grad:  tensor([-0.0404,  0.2285])\n",
      "------\n",
      "------\n",
      "Epoch 1512, Loss 3.085612, \n",
      "Params:  tensor([  5.1309, -15.9641])\n",
      "Grad:  tensor([-0.0403,  0.2281])\n",
      "------\n",
      "------\n",
      "Epoch 1513, Loss 3.085075, \n",
      "Params:  tensor([  5.1313, -15.9664])\n",
      "Grad:  tensor([-0.0402,  0.2277])\n",
      "------\n",
      "------\n",
      "Epoch 1514, Loss 3.084542, \n",
      "Params:  tensor([  5.1317, -15.9687])\n",
      "Grad:  tensor([-0.0402,  0.2274])\n",
      "------\n",
      "------\n",
      "Epoch 1515, Loss 3.084009, \n",
      "Params:  tensor([  5.1321, -15.9709])\n",
      "Grad:  tensor([-0.0401,  0.2270])\n",
      "------\n",
      "------\n",
      "Epoch 1516, Loss 3.083478, \n",
      "Params:  tensor([  5.1325, -15.9732])\n",
      "Grad:  tensor([-0.0400,  0.2266])\n",
      "------\n",
      "------\n",
      "Epoch 1517, Loss 3.082948, \n",
      "Params:  tensor([  5.1329, -15.9755])\n",
      "Grad:  tensor([-0.0400,  0.2262])\n",
      "------\n",
      "------\n",
      "Epoch 1518, Loss 3.082422, \n",
      "Params:  tensor([  5.1333, -15.9777])\n",
      "Grad:  tensor([-0.0399,  0.2258])\n",
      "------\n",
      "------\n",
      "Epoch 1519, Loss 3.081897, \n",
      "Params:  tensor([  5.1337, -15.9800])\n",
      "Grad:  tensor([-0.0398,  0.2254])\n",
      "------\n",
      "------\n",
      "Epoch 1520, Loss 3.081373, \n",
      "Params:  tensor([  5.1341, -15.9822])\n",
      "Grad:  tensor([-0.0398,  0.2250])\n",
      "------\n",
      "------\n",
      "Epoch 1521, Loss 3.080850, \n",
      "Params:  tensor([  5.1345, -15.9845])\n",
      "Grad:  tensor([-0.0397,  0.2247])\n",
      "------\n",
      "------\n",
      "Epoch 1522, Loss 3.080331, \n",
      "Params:  tensor([  5.1349, -15.9867])\n",
      "Grad:  tensor([-0.0396,  0.2243])\n",
      "------\n",
      "------\n",
      "Epoch 1523, Loss 3.079811, \n",
      "Params:  tensor([  5.1353, -15.9890])\n",
      "Grad:  tensor([-0.0396,  0.2239])\n",
      "------\n",
      "------\n",
      "Epoch 1524, Loss 3.079296, \n",
      "Params:  tensor([  5.1357, -15.9912])\n",
      "Grad:  tensor([-0.0395,  0.2235])\n",
      "------\n",
      "------\n",
      "Epoch 1525, Loss 3.078781, \n",
      "Params:  tensor([  5.1361, -15.9934])\n",
      "Grad:  tensor([-0.0394,  0.2231])\n",
      "------\n",
      "------\n",
      "Epoch 1526, Loss 3.078268, \n",
      "Params:  tensor([  5.1365, -15.9957])\n",
      "Grad:  tensor([-0.0394,  0.2228])\n",
      "------\n",
      "------\n",
      "Epoch 1527, Loss 3.077758, \n",
      "Params:  tensor([  5.1369, -15.9979])\n",
      "Grad:  tensor([-0.0393,  0.2224])\n",
      "------\n",
      "------\n",
      "Epoch 1528, Loss 3.077248, \n",
      "Params:  tensor([  5.1372, -16.0001])\n",
      "Grad:  tensor([-0.0392,  0.2220])\n",
      "------\n",
      "------\n",
      "Epoch 1529, Loss 3.076739, \n",
      "Params:  tensor([  5.1376, -16.0023])\n",
      "Grad:  tensor([-0.0391,  0.2216])\n",
      "------\n",
      "------\n",
      "Epoch 1530, Loss 3.076232, \n",
      "Params:  tensor([  5.1380, -16.0045])\n",
      "Grad:  tensor([-0.0391,  0.2213])\n",
      "------\n",
      "------\n",
      "Epoch 1531, Loss 3.075729, \n",
      "Params:  tensor([  5.1384, -16.0067])\n",
      "Grad:  tensor([-0.0390,  0.2209])\n",
      "------\n",
      "------\n",
      "Epoch 1532, Loss 3.075225, \n",
      "Params:  tensor([  5.1388, -16.0089])\n",
      "Grad:  tensor([-0.0390,  0.2205])\n",
      "------\n",
      "------\n",
      "Epoch 1533, Loss 3.074724, \n",
      "Params:  tensor([  5.1392, -16.0111])\n",
      "Grad:  tensor([-0.0389,  0.2201])\n",
      "------\n",
      "------\n",
      "Epoch 1534, Loss 3.074227, \n",
      "Params:  tensor([  5.1396, -16.0133])\n",
      "Grad:  tensor([-0.0388,  0.2198])\n",
      "------\n",
      "------\n",
      "Epoch 1535, Loss 3.073726, \n",
      "Params:  tensor([  5.1400, -16.0155])\n",
      "Grad:  tensor([-0.0387,  0.2194])\n",
      "------\n",
      "------\n",
      "Epoch 1536, Loss 3.073232, \n",
      "Params:  tensor([  5.1404, -16.0177])\n",
      "Grad:  tensor([-0.0387,  0.2190])\n",
      "------\n",
      "------\n",
      "Epoch 1537, Loss 3.072739, \n",
      "Params:  tensor([  5.1407, -16.0199])\n",
      "Grad:  tensor([-0.0386,  0.2186])\n",
      "------\n",
      "------\n",
      "Epoch 1538, Loss 3.072245, \n",
      "Params:  tensor([  5.1411, -16.0221])\n",
      "Grad:  tensor([-0.0385,  0.2183])\n",
      "------\n",
      "------\n",
      "Epoch 1539, Loss 3.071753, \n",
      "Params:  tensor([  5.1415, -16.0243])\n",
      "Grad:  tensor([-0.0385,  0.2179])\n",
      "------\n",
      "------\n",
      "Epoch 1540, Loss 3.071265, \n",
      "Params:  tensor([  5.1419, -16.0264])\n",
      "Grad:  tensor([-0.0384,  0.2175])\n",
      "------\n",
      "------\n",
      "Epoch 1541, Loss 3.070778, \n",
      "Params:  tensor([  5.1423, -16.0286])\n",
      "Grad:  tensor([-0.0383,  0.2172])\n",
      "------\n",
      "------\n",
      "Epoch 1542, Loss 3.070293, \n",
      "Params:  tensor([  5.1427, -16.0308])\n",
      "Grad:  tensor([-0.0383,  0.2168])\n",
      "------\n",
      "------\n",
      "Epoch 1543, Loss 3.069808, \n",
      "Params:  tensor([  5.1430, -16.0330])\n",
      "Grad:  tensor([-0.0382,  0.2164])\n",
      "------\n",
      "------\n",
      "Epoch 1544, Loss 3.069326, \n",
      "Params:  tensor([  5.1434, -16.0351])\n",
      "Grad:  tensor([-0.0382,  0.2161])\n",
      "------\n",
      "------\n",
      "Epoch 1545, Loss 3.068845, \n",
      "Params:  tensor([  5.1438, -16.0373])\n",
      "Grad:  tensor([-0.0381,  0.2157])\n",
      "------\n",
      "------\n",
      "Epoch 1546, Loss 3.068366, \n",
      "Params:  tensor([  5.1442, -16.0394])\n",
      "Grad:  tensor([-0.0380,  0.2153])\n",
      "------\n",
      "------\n",
      "Epoch 1547, Loss 3.067887, \n",
      "Params:  tensor([  5.1446, -16.0416])\n",
      "Grad:  tensor([-0.0380,  0.2150])\n",
      "------\n",
      "------\n",
      "Epoch 1548, Loss 3.067412, \n",
      "Params:  tensor([  5.1449, -16.0437])\n",
      "Grad:  tensor([-0.0379,  0.2146])\n",
      "------\n",
      "------\n",
      "Epoch 1549, Loss 3.066937, \n",
      "Params:  tensor([  5.1453, -16.0459])\n",
      "Grad:  tensor([-0.0378,  0.2142])\n",
      "------\n",
      "------\n",
      "Epoch 1550, Loss 3.066463, \n",
      "Params:  tensor([  5.1457, -16.0480])\n",
      "Grad:  tensor([-0.0378,  0.2139])\n",
      "------\n",
      "------\n",
      "Epoch 1551, Loss 3.065993, \n",
      "Params:  tensor([  5.1461, -16.0501])\n",
      "Grad:  tensor([-0.0377,  0.2135])\n",
      "------\n",
      "------\n",
      "Epoch 1552, Loss 3.065524, \n",
      "Params:  tensor([  5.1465, -16.0523])\n",
      "Grad:  tensor([-0.0376,  0.2131])\n",
      "------\n",
      "------\n",
      "Epoch 1553, Loss 3.065055, \n",
      "Params:  tensor([  5.1468, -16.0544])\n",
      "Grad:  tensor([-0.0376,  0.2128])\n",
      "------\n",
      "------\n",
      "Epoch 1554, Loss 3.064588, \n",
      "Params:  tensor([  5.1472, -16.0565])\n",
      "Grad:  tensor([-0.0375,  0.2124])\n",
      "------\n",
      "------\n",
      "Epoch 1555, Loss 3.064123, \n",
      "Params:  tensor([  5.1476, -16.0586])\n",
      "Grad:  tensor([-0.0375,  0.2120])\n",
      "------\n",
      "------\n",
      "Epoch 1556, Loss 3.063660, \n",
      "Params:  tensor([  5.1480, -16.0608])\n",
      "Grad:  tensor([-0.0374,  0.2117])\n",
      "------\n",
      "------\n",
      "Epoch 1557, Loss 3.063199, \n",
      "Params:  tensor([  5.1483, -16.0629])\n",
      "Grad:  tensor([-0.0373,  0.2113])\n",
      "------\n",
      "------\n",
      "Epoch 1558, Loss 3.062738, \n",
      "Params:  tensor([  5.1487, -16.0650])\n",
      "Grad:  tensor([-0.0373,  0.2110])\n",
      "------\n",
      "------\n",
      "Epoch 1559, Loss 3.062280, \n",
      "Params:  tensor([  5.1491, -16.0671])\n",
      "Grad:  tensor([-0.0372,  0.2106])\n",
      "------\n",
      "------\n",
      "Epoch 1560, Loss 3.061822, \n",
      "Params:  tensor([  5.1494, -16.0692])\n",
      "Grad:  tensor([-0.0371,  0.2103])\n",
      "------\n",
      "------\n",
      "Epoch 1561, Loss 3.061367, \n",
      "Params:  tensor([  5.1498, -16.0713])\n",
      "Grad:  tensor([-0.0371,  0.2099])\n",
      "------\n",
      "------\n",
      "Epoch 1562, Loss 3.060913, \n",
      "Params:  tensor([  5.1502, -16.0734])\n",
      "Grad:  tensor([-0.0370,  0.2095])\n",
      "------\n",
      "------\n",
      "Epoch 1563, Loss 3.060462, \n",
      "Params:  tensor([  5.1506, -16.0755])\n",
      "Grad:  tensor([-0.0370,  0.2092])\n",
      "------\n",
      "------\n",
      "Epoch 1564, Loss 3.060011, \n",
      "Params:  tensor([  5.1509, -16.0776])\n",
      "Grad:  tensor([-0.0369,  0.2088])\n",
      "------\n",
      "------\n",
      "Epoch 1565, Loss 3.059561, \n",
      "Params:  tensor([  5.1513, -16.0796])\n",
      "Grad:  tensor([-0.0368,  0.2085])\n",
      "------\n",
      "------\n",
      "Epoch 1566, Loss 3.059114, \n",
      "Params:  tensor([  5.1517, -16.0817])\n",
      "Grad:  tensor([-0.0368,  0.2081])\n",
      "------\n",
      "------\n",
      "Epoch 1567, Loss 3.058668, \n",
      "Params:  tensor([  5.1520, -16.0838])\n",
      "Grad:  tensor([-0.0367,  0.2078])\n",
      "------\n",
      "------\n",
      "Epoch 1568, Loss 3.058221, \n",
      "Params:  tensor([  5.1524, -16.0859])\n",
      "Grad:  tensor([-0.0366,  0.2074])\n",
      "------\n",
      "------\n",
      "Epoch 1569, Loss 3.057781, \n",
      "Params:  tensor([  5.1528, -16.0880])\n",
      "Grad:  tensor([-0.0366,  0.2071])\n",
      "------\n",
      "------\n",
      "Epoch 1570, Loss 3.057338, \n",
      "Params:  tensor([  5.1531, -16.0900])\n",
      "Grad:  tensor([-0.0365,  0.2067])\n",
      "------\n",
      "------\n",
      "Epoch 1571, Loss 3.056898, \n",
      "Params:  tensor([  5.1535, -16.0921])\n",
      "Grad:  tensor([-0.0364,  0.2064])\n",
      "------\n",
      "------\n",
      "Epoch 1572, Loss 3.056458, \n",
      "Params:  tensor([  5.1539, -16.0941])\n",
      "Grad:  tensor([-0.0364,  0.2060])\n",
      "------\n",
      "------\n",
      "Epoch 1573, Loss 3.056019, \n",
      "Params:  tensor([  5.1542, -16.0962])\n",
      "Grad:  tensor([-0.0363,  0.2057])\n",
      "------\n",
      "------\n",
      "Epoch 1574, Loss 3.055585, \n",
      "Params:  tensor([  5.1546, -16.0983])\n",
      "Grad:  tensor([-0.0363,  0.2053])\n",
      "------\n",
      "------\n",
      "Epoch 1575, Loss 3.055151, \n",
      "Params:  tensor([  5.1549, -16.1003])\n",
      "Grad:  tensor([-0.0362,  0.2050])\n",
      "------\n",
      "------\n",
      "Epoch 1576, Loss 3.054717, \n",
      "Params:  tensor([  5.1553, -16.1023])\n",
      "Grad:  tensor([-0.0361,  0.2046])\n",
      "------\n",
      "------\n",
      "Epoch 1577, Loss 3.054286, \n",
      "Params:  tensor([  5.1557, -16.1044])\n",
      "Grad:  tensor([-0.0361,  0.2043])\n",
      "------\n",
      "------\n",
      "Epoch 1578, Loss 3.053857, \n",
      "Params:  tensor([  5.1560, -16.1064])\n",
      "Grad:  tensor([-0.0360,  0.2039])\n",
      "------\n",
      "------\n",
      "Epoch 1579, Loss 3.053427, \n",
      "Params:  tensor([  5.1564, -16.1085])\n",
      "Grad:  tensor([-0.0360,  0.2036])\n",
      "------\n",
      "------\n",
      "Epoch 1580, Loss 3.053000, \n",
      "Params:  tensor([  5.1567, -16.1105])\n",
      "Grad:  tensor([-0.0359,  0.2032])\n",
      "------\n",
      "------\n",
      "Epoch 1581, Loss 3.052576, \n",
      "Params:  tensor([  5.1571, -16.1125])\n",
      "Grad:  tensor([-0.0358,  0.2029])\n",
      "------\n",
      "------\n",
      "Epoch 1582, Loss 3.052152, \n",
      "Params:  tensor([  5.1575, -16.1146])\n",
      "Grad:  tensor([-0.0358,  0.2025])\n",
      "------\n",
      "------\n",
      "Epoch 1583, Loss 3.051730, \n",
      "Params:  tensor([  5.1578, -16.1166])\n",
      "Grad:  tensor([-0.0357,  0.2022])\n",
      "------\n",
      "------\n",
      "Epoch 1584, Loss 3.051306, \n",
      "Params:  tensor([  5.1582, -16.1186])\n",
      "Grad:  tensor([-0.0357,  0.2018])\n",
      "------\n",
      "------\n",
      "Epoch 1585, Loss 3.050888, \n",
      "Params:  tensor([  5.1585, -16.1206])\n",
      "Grad:  tensor([-0.0356,  0.2015])\n",
      "------\n",
      "------\n",
      "Epoch 1586, Loss 3.050471, \n",
      "Params:  tensor([  5.1589, -16.1226])\n",
      "Grad:  tensor([-0.0355,  0.2012])\n",
      "------\n",
      "------\n",
      "Epoch 1587, Loss 3.050052, \n",
      "Params:  tensor([  5.1592, -16.1246])\n",
      "Grad:  tensor([-0.0355,  0.2008])\n",
      "------\n",
      "------\n",
      "Epoch 1588, Loss 3.049639, \n",
      "Params:  tensor([  5.1596, -16.1266])\n",
      "Grad:  tensor([-0.0354,  0.2005])\n",
      "------\n",
      "------\n",
      "Epoch 1589, Loss 3.049223, \n",
      "Params:  tensor([  5.1599, -16.1286])\n",
      "Grad:  tensor([-0.0354,  0.2001])\n",
      "------\n",
      "------\n",
      "Epoch 1590, Loss 3.048811, \n",
      "Params:  tensor([  5.1603, -16.1306])\n",
      "Grad:  tensor([-0.0353,  0.1998])\n",
      "------\n",
      "------\n",
      "Epoch 1591, Loss 3.048398, \n",
      "Params:  tensor([  5.1607, -16.1326])\n",
      "Grad:  tensor([-0.0353,  0.1995])\n",
      "------\n",
      "------\n",
      "Epoch 1592, Loss 3.047991, \n",
      "Params:  tensor([  5.1610, -16.1346])\n",
      "Grad:  tensor([-0.0352,  0.1991])\n",
      "------\n",
      "------\n",
      "Epoch 1593, Loss 3.047581, \n",
      "Params:  tensor([  5.1614, -16.1366])\n",
      "Grad:  tensor([-0.0351,  0.1988])\n",
      "------\n",
      "------\n",
      "Epoch 1594, Loss 3.047173, \n",
      "Params:  tensor([  5.1617, -16.1386])\n",
      "Grad:  tensor([-0.0351,  0.1984])\n",
      "------\n",
      "------\n",
      "Epoch 1595, Loss 3.046768, \n",
      "Params:  tensor([  5.1621, -16.1406])\n",
      "Grad:  tensor([-0.0350,  0.1981])\n",
      "------\n",
      "------\n",
      "Epoch 1596, Loss 3.046362, \n",
      "Params:  tensor([  5.1624, -16.1425])\n",
      "Grad:  tensor([-0.0349,  0.1978])\n",
      "------\n",
      "------\n",
      "Epoch 1597, Loss 3.045960, \n",
      "Params:  tensor([  5.1628, -16.1445])\n",
      "Grad:  tensor([-0.0349,  0.1974])\n",
      "------\n",
      "------\n",
      "Epoch 1598, Loss 3.045559, \n",
      "Params:  tensor([  5.1631, -16.1465])\n",
      "Grad:  tensor([-0.0348,  0.1971])\n",
      "------\n",
      "------\n",
      "Epoch 1599, Loss 3.045160, \n",
      "Params:  tensor([  5.1635, -16.1485])\n",
      "Grad:  tensor([-0.0348,  0.1968])\n",
      "------\n",
      "------\n",
      "Epoch 1600, Loss 3.044759, \n",
      "Params:  tensor([  5.1638, -16.1504])\n",
      "Grad:  tensor([-0.0347,  0.1964])\n",
      "------\n",
      "------\n",
      "Epoch 1601, Loss 3.044361, \n",
      "Params:  tensor([  5.1641, -16.1524])\n",
      "Grad:  tensor([-0.0346,  0.1961])\n",
      "------\n",
      "------\n",
      "Epoch 1602, Loss 3.043966, \n",
      "Params:  tensor([  5.1645, -16.1543])\n",
      "Grad:  tensor([-0.0346,  0.1958])\n",
      "------\n",
      "------\n",
      "Epoch 1603, Loss 3.043571, \n",
      "Params:  tensor([  5.1648, -16.1563])\n",
      "Grad:  tensor([-0.0345,  0.1954])\n",
      "------\n",
      "------\n",
      "Epoch 1604, Loss 3.043176, \n",
      "Params:  tensor([  5.1652, -16.1582])\n",
      "Grad:  tensor([-0.0345,  0.1951])\n",
      "------\n",
      "------\n",
      "Epoch 1605, Loss 3.042785, \n",
      "Params:  tensor([  5.1655, -16.1602])\n",
      "Grad:  tensor([-0.0344,  0.1948])\n",
      "------\n",
      "------\n",
      "Epoch 1606, Loss 3.042395, \n",
      "Params:  tensor([  5.1659, -16.1621])\n",
      "Grad:  tensor([-0.0343,  0.1944])\n",
      "------\n",
      "------\n",
      "Epoch 1607, Loss 3.042005, \n",
      "Params:  tensor([  5.1662, -16.1641])\n",
      "Grad:  tensor([-0.0343,  0.1941])\n",
      "------\n",
      "------\n",
      "Epoch 1608, Loss 3.041615, \n",
      "Params:  tensor([  5.1666, -16.1660])\n",
      "Grad:  tensor([-0.0342,  0.1938])\n",
      "------\n",
      "------\n",
      "Epoch 1609, Loss 3.041230, \n",
      "Params:  tensor([  5.1669, -16.1680])\n",
      "Grad:  tensor([-0.0342,  0.1934])\n",
      "------\n",
      "------\n",
      "Epoch 1610, Loss 3.040844, \n",
      "Params:  tensor([  5.1672, -16.1699])\n",
      "Grad:  tensor([-0.0341,  0.1931])\n",
      "------\n",
      "------\n",
      "Epoch 1611, Loss 3.040461, \n",
      "Params:  tensor([  5.1676, -16.1718])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.0341,  0.1928])\n",
      "------\n",
      "------\n",
      "Epoch 1612, Loss 3.040077, \n",
      "Params:  tensor([  5.1679, -16.1737])\n",
      "Grad:  tensor([-0.0340,  0.1925])\n",
      "------\n",
      "------\n",
      "Epoch 1613, Loss 3.039695, \n",
      "Params:  tensor([  5.1683, -16.1757])\n",
      "Grad:  tensor([-0.0339,  0.1921])\n",
      "------\n",
      "------\n",
      "Epoch 1614, Loss 3.039314, \n",
      "Params:  tensor([  5.1686, -16.1776])\n",
      "Grad:  tensor([-0.0339,  0.1918])\n",
      "------\n",
      "------\n",
      "Epoch 1615, Loss 3.038934, \n",
      "Params:  tensor([  5.1689, -16.1795])\n",
      "Grad:  tensor([-0.0338,  0.1915])\n",
      "------\n",
      "------\n",
      "Epoch 1616, Loss 3.038557, \n",
      "Params:  tensor([  5.1693, -16.1814])\n",
      "Grad:  tensor([-0.0338,  0.1912])\n",
      "------\n",
      "------\n",
      "Epoch 1617, Loss 3.038181, \n",
      "Params:  tensor([  5.1696, -16.1833])\n",
      "Grad:  tensor([-0.0337,  0.1908])\n",
      "------\n",
      "------\n",
      "Epoch 1618, Loss 3.037805, \n",
      "Params:  tensor([  5.1699, -16.1852])\n",
      "Grad:  tensor([-0.0337,  0.1905])\n",
      "------\n",
      "------\n",
      "Epoch 1619, Loss 3.037432, \n",
      "Params:  tensor([  5.1703, -16.1871])\n",
      "Grad:  tensor([-0.0336,  0.1902])\n",
      "------\n",
      "------\n",
      "Epoch 1620, Loss 3.037059, \n",
      "Params:  tensor([  5.1706, -16.1890])\n",
      "Grad:  tensor([-0.0335,  0.1899])\n",
      "------\n",
      "------\n",
      "Epoch 1621, Loss 3.036689, \n",
      "Params:  tensor([  5.1710, -16.1909])\n",
      "Grad:  tensor([-0.0335,  0.1895])\n",
      "------\n",
      "------\n",
      "Epoch 1622, Loss 3.036319, \n",
      "Params:  tensor([  5.1713, -16.1928])\n",
      "Grad:  tensor([-0.0334,  0.1892])\n",
      "------\n",
      "------\n",
      "Epoch 1623, Loss 3.035949, \n",
      "Params:  tensor([  5.1716, -16.1947])\n",
      "Grad:  tensor([-0.0334,  0.1889])\n",
      "------\n",
      "------\n",
      "Epoch 1624, Loss 3.035583, \n",
      "Params:  tensor([  5.1720, -16.1966])\n",
      "Grad:  tensor([-0.0333,  0.1886])\n",
      "------\n",
      "------\n",
      "Epoch 1625, Loss 3.035216, \n",
      "Params:  tensor([  5.1723, -16.1985])\n",
      "Grad:  tensor([-0.0333,  0.1883])\n",
      "------\n",
      "------\n",
      "Epoch 1626, Loss 3.034849, \n",
      "Params:  tensor([  5.1726, -16.2003])\n",
      "Grad:  tensor([-0.0332,  0.1879])\n",
      "------\n",
      "------\n",
      "Epoch 1627, Loss 3.034485, \n",
      "Params:  tensor([  5.1729, -16.2022])\n",
      "Grad:  tensor([-0.0331,  0.1876])\n",
      "------\n",
      "------\n",
      "Epoch 1628, Loss 3.034123, \n",
      "Params:  tensor([  5.1733, -16.2041])\n",
      "Grad:  tensor([-0.0331,  0.1873])\n",
      "------\n",
      "------\n",
      "Epoch 1629, Loss 3.033762, \n",
      "Params:  tensor([  5.1736, -16.2060])\n",
      "Grad:  tensor([-0.0330,  0.1870])\n",
      "------\n",
      "------\n",
      "Epoch 1630, Loss 3.033402, \n",
      "Params:  tensor([  5.1739, -16.2078])\n",
      "Grad:  tensor([-0.0330,  0.1867])\n",
      "------\n",
      "------\n",
      "Epoch 1631, Loss 3.033041, \n",
      "Params:  tensor([  5.1743, -16.2097])\n",
      "Grad:  tensor([-0.0329,  0.1863])\n",
      "------\n",
      "------\n",
      "Epoch 1632, Loss 3.032685, \n",
      "Params:  tensor([  5.1746, -16.2116])\n",
      "Grad:  tensor([-0.0329,  0.1860])\n",
      "------\n",
      "------\n",
      "Epoch 1633, Loss 3.032329, \n",
      "Params:  tensor([  5.1749, -16.2134])\n",
      "Grad:  tensor([-0.0328,  0.1857])\n",
      "------\n",
      "------\n",
      "Epoch 1634, Loss 3.031973, \n",
      "Params:  tensor([  5.1753, -16.2153])\n",
      "Grad:  tensor([-0.0327,  0.1854])\n",
      "------\n",
      "------\n",
      "Epoch 1635, Loss 3.031619, \n",
      "Params:  tensor([  5.1756, -16.2171])\n",
      "Grad:  tensor([-0.0327,  0.1851])\n",
      "------\n",
      "------\n",
      "Epoch 1636, Loss 3.031265, \n",
      "Params:  tensor([  5.1759, -16.2190])\n",
      "Grad:  tensor([-0.0326,  0.1848])\n",
      "------\n",
      "------\n",
      "Epoch 1637, Loss 3.030913, \n",
      "Params:  tensor([  5.1762, -16.2208])\n",
      "Grad:  tensor([-0.0326,  0.1845])\n",
      "------\n",
      "------\n",
      "Epoch 1638, Loss 3.030564, \n",
      "Params:  tensor([  5.1766, -16.2226])\n",
      "Grad:  tensor([-0.0325,  0.1841])\n",
      "------\n",
      "------\n",
      "Epoch 1639, Loss 3.030215, \n",
      "Params:  tensor([  5.1769, -16.2245])\n",
      "Grad:  tensor([-0.0325,  0.1838])\n",
      "------\n",
      "------\n",
      "Epoch 1640, Loss 3.029867, \n",
      "Params:  tensor([  5.1772, -16.2263])\n",
      "Grad:  tensor([-0.0324,  0.1835])\n",
      "------\n",
      "------\n",
      "Epoch 1641, Loss 3.029518, \n",
      "Params:  tensor([  5.1775, -16.2282])\n",
      "Grad:  tensor([-0.0324,  0.1832])\n",
      "------\n",
      "------\n",
      "Epoch 1642, Loss 3.029173, \n",
      "Params:  tensor([  5.1779, -16.2300])\n",
      "Grad:  tensor([-0.0323,  0.1829])\n",
      "------\n",
      "------\n",
      "Epoch 1643, Loss 3.028828, \n",
      "Params:  tensor([  5.1782, -16.2318])\n",
      "Grad:  tensor([-0.0323,  0.1826])\n",
      "------\n",
      "------\n",
      "Epoch 1644, Loss 3.028486, \n",
      "Params:  tensor([  5.1785, -16.2336])\n",
      "Grad:  tensor([-0.0322,  0.1823])\n",
      "------\n",
      "------\n",
      "Epoch 1645, Loss 3.028142, \n",
      "Params:  tensor([  5.1788, -16.2355])\n",
      "Grad:  tensor([-0.0321,  0.1820])\n",
      "------\n",
      "------\n",
      "Epoch 1646, Loss 3.027802, \n",
      "Params:  tensor([  5.1791, -16.2373])\n",
      "Grad:  tensor([-0.0321,  0.1817])\n",
      "------\n",
      "------\n",
      "Epoch 1647, Loss 3.027463, \n",
      "Params:  tensor([  5.1795, -16.2391])\n",
      "Grad:  tensor([-0.0320,  0.1813])\n",
      "------\n",
      "------\n",
      "Epoch 1648, Loss 3.027122, \n",
      "Params:  tensor([  5.1798, -16.2409])\n",
      "Grad:  tensor([-0.0320,  0.1810])\n",
      "------\n",
      "------\n",
      "Epoch 1649, Loss 3.026784, \n",
      "Params:  tensor([  5.1801, -16.2427])\n",
      "Grad:  tensor([-0.0319,  0.1807])\n",
      "------\n",
      "------\n",
      "Epoch 1650, Loss 3.026447, \n",
      "Params:  tensor([  5.1804, -16.2445])\n",
      "Grad:  tensor([-0.0319,  0.1804])\n",
      "------\n",
      "------\n",
      "Epoch 1651, Loss 3.026111, \n",
      "Params:  tensor([  5.1807, -16.2463])\n",
      "Grad:  tensor([-0.0318,  0.1801])\n",
      "------\n",
      "------\n",
      "Epoch 1652, Loss 3.025780, \n",
      "Params:  tensor([  5.1811, -16.2481])\n",
      "Grad:  tensor([-0.0318,  0.1798])\n",
      "------\n",
      "------\n",
      "Epoch 1653, Loss 3.025447, \n",
      "Params:  tensor([  5.1814, -16.2499])\n",
      "Grad:  tensor([-0.0317,  0.1795])\n",
      "------\n",
      "------\n",
      "Epoch 1654, Loss 3.025114, \n",
      "Params:  tensor([  5.1817, -16.2517])\n",
      "Grad:  tensor([-0.0317,  0.1792])\n",
      "------\n",
      "------\n",
      "Epoch 1655, Loss 3.024782, \n",
      "Params:  tensor([  5.1820, -16.2535])\n",
      "Grad:  tensor([-0.0316,  0.1789])\n",
      "------\n",
      "------\n",
      "Epoch 1656, Loss 3.024452, \n",
      "Params:  tensor([  5.1823, -16.2553])\n",
      "Grad:  tensor([-0.0316,  0.1786])\n",
      "------\n",
      "------\n",
      "Epoch 1657, Loss 3.024125, \n",
      "Params:  tensor([  5.1826, -16.2570])\n",
      "Grad:  tensor([-0.0315,  0.1783])\n",
      "------\n",
      "------\n",
      "Epoch 1658, Loss 3.023796, \n",
      "Params:  tensor([  5.1829, -16.2588])\n",
      "Grad:  tensor([-0.0315,  0.1780])\n",
      "------\n",
      "------\n",
      "Epoch 1659, Loss 3.023471, \n",
      "Params:  tensor([  5.1833, -16.2606])\n",
      "Grad:  tensor([-0.0314,  0.1777])\n",
      "------\n",
      "------\n",
      "Epoch 1660, Loss 3.023145, \n",
      "Params:  tensor([  5.1836, -16.2624])\n",
      "Grad:  tensor([-0.0313,  0.1774])\n",
      "------\n",
      "------\n",
      "Epoch 1661, Loss 3.022820, \n",
      "Params:  tensor([  5.1839, -16.2641])\n",
      "Grad:  tensor([-0.0313,  0.1771])\n",
      "------\n",
      "------\n",
      "Epoch 1662, Loss 3.022498, \n",
      "Params:  tensor([  5.1842, -16.2659])\n",
      "Grad:  tensor([-0.0312,  0.1768])\n",
      "------\n",
      "------\n",
      "Epoch 1663, Loss 3.022177, \n",
      "Params:  tensor([  5.1845, -16.2677])\n",
      "Grad:  tensor([-0.0312,  0.1765])\n",
      "------\n",
      "------\n",
      "Epoch 1664, Loss 3.021855, \n",
      "Params:  tensor([  5.1848, -16.2694])\n",
      "Grad:  tensor([-0.0311,  0.1762])\n",
      "------\n",
      "------\n",
      "Epoch 1665, Loss 3.021534, \n",
      "Params:  tensor([  5.1851, -16.2712])\n",
      "Grad:  tensor([-0.0311,  0.1759])\n",
      "------\n",
      "------\n",
      "Epoch 1666, Loss 3.021217, \n",
      "Params:  tensor([  5.1854, -16.2730])\n",
      "Grad:  tensor([-0.0310,  0.1756])\n",
      "------\n",
      "------\n",
      "Epoch 1667, Loss 3.020898, \n",
      "Params:  tensor([  5.1858, -16.2747])\n",
      "Grad:  tensor([-0.0310,  0.1753])\n",
      "------\n",
      "------\n",
      "Epoch 1668, Loss 3.020582, \n",
      "Params:  tensor([  5.1861, -16.2765])\n",
      "Grad:  tensor([-0.0309,  0.1750])\n",
      "------\n",
      "------\n",
      "Epoch 1669, Loss 3.020265, \n",
      "Params:  tensor([  5.1864, -16.2782])\n",
      "Grad:  tensor([-0.0309,  0.1747])\n",
      "------\n",
      "------\n",
      "Epoch 1670, Loss 3.019952, \n",
      "Params:  tensor([  5.1867, -16.2800])\n",
      "Grad:  tensor([-0.0308,  0.1744])\n",
      "------\n",
      "------\n",
      "Epoch 1671, Loss 3.019639, \n",
      "Params:  tensor([  5.1870, -16.2817])\n",
      "Grad:  tensor([-0.0308,  0.1741])\n",
      "------\n",
      "------\n",
      "Epoch 1672, Loss 3.019325, \n",
      "Params:  tensor([  5.1873, -16.2834])\n",
      "Grad:  tensor([-0.0307,  0.1738])\n",
      "------\n",
      "------\n",
      "Epoch 1673, Loss 3.019016, \n",
      "Params:  tensor([  5.1876, -16.2852])\n",
      "Grad:  tensor([-0.0307,  0.1735])\n",
      "------\n",
      "------\n",
      "Epoch 1674, Loss 3.018706, \n",
      "Params:  tensor([  5.1879, -16.2869])\n",
      "Grad:  tensor([-0.0306,  0.1732])\n",
      "------\n",
      "------\n",
      "Epoch 1675, Loss 3.018395, \n",
      "Params:  tensor([  5.1882, -16.2886])\n",
      "Grad:  tensor([-0.0305,  0.1729])\n",
      "------\n",
      "------\n",
      "Epoch 1676, Loss 3.018089, \n",
      "Params:  tensor([  5.1885, -16.2904])\n",
      "Grad:  tensor([-0.0305,  0.1726])\n",
      "------\n",
      "------\n",
      "Epoch 1677, Loss 3.017780, \n",
      "Params:  tensor([  5.1888, -16.2921])\n",
      "Grad:  tensor([-0.0304,  0.1723])\n",
      "------\n",
      "------\n",
      "Epoch 1678, Loss 3.017475, \n",
      "Params:  tensor([  5.1891, -16.2938])\n",
      "Grad:  tensor([-0.0304,  0.1720])\n",
      "------\n",
      "------\n",
      "Epoch 1679, Loss 3.017170, \n",
      "Params:  tensor([  5.1894, -16.2955])\n",
      "Grad:  tensor([-0.0303,  0.1717])\n",
      "------\n",
      "------\n",
      "Epoch 1680, Loss 3.016867, \n",
      "Params:  tensor([  5.1897, -16.2972])\n",
      "Grad:  tensor([-0.0303,  0.1715])\n",
      "------\n",
      "------\n",
      "Epoch 1681, Loss 3.016564, \n",
      "Params:  tensor([  5.1900, -16.2989])\n",
      "Grad:  tensor([-0.0302,  0.1712])\n",
      "------\n",
      "------\n",
      "Epoch 1682, Loss 3.016262, \n",
      "Params:  tensor([  5.1903, -16.3006])\n",
      "Grad:  tensor([-0.0302,  0.1709])\n",
      "------\n",
      "------\n",
      "Epoch 1683, Loss 3.015959, \n",
      "Params:  tensor([  5.1906, -16.3024])\n",
      "Grad:  tensor([-0.0301,  0.1706])\n",
      "------\n",
      "------\n",
      "Epoch 1684, Loss 3.015662, \n",
      "Params:  tensor([  5.1909, -16.3041])\n",
      "Grad:  tensor([-0.0301,  0.1703])\n",
      "------\n",
      "------\n",
      "Epoch 1685, Loss 3.015361, \n",
      "Params:  tensor([  5.1912, -16.3058])\n",
      "Grad:  tensor([-0.0300,  0.1700])\n",
      "------\n",
      "------\n",
      "Epoch 1686, Loss 3.015064, \n",
      "Params:  tensor([  5.1915, -16.3075])\n",
      "Grad:  tensor([-0.0300,  0.1697])\n",
      "------\n",
      "------\n",
      "Epoch 1687, Loss 3.014768, \n",
      "Params:  tensor([  5.1918, -16.3091])\n",
      "Grad:  tensor([-0.0299,  0.1694])\n",
      "------\n",
      "------\n",
      "Epoch 1688, Loss 3.014472, \n",
      "Params:  tensor([  5.1921, -16.3108])\n",
      "Grad:  tensor([-0.0299,  0.1691])\n",
      "------\n",
      "------\n",
      "Epoch 1689, Loss 3.014179, \n",
      "Params:  tensor([  5.1924, -16.3125])\n",
      "Grad:  tensor([-0.0298,  0.1688])\n",
      "------\n",
      "------\n",
      "Epoch 1690, Loss 3.013884, \n",
      "Params:  tensor([  5.1927, -16.3142])\n",
      "Grad:  tensor([-0.0298,  0.1686])\n",
      "------\n",
      "------\n",
      "Epoch 1691, Loss 3.013591, \n",
      "Params:  tensor([  5.1930, -16.3159])\n",
      "Grad:  tensor([-0.0297,  0.1683])\n",
      "------\n",
      "------\n",
      "Epoch 1692, Loss 3.013299, \n",
      "Params:  tensor([  5.1933, -16.3176])\n",
      "Grad:  tensor([-0.0297,  0.1680])\n",
      "------\n",
      "------\n",
      "Epoch 1693, Loss 3.013008, \n",
      "Params:  tensor([  5.1936, -16.3193])\n",
      "Grad:  tensor([-0.0296,  0.1677])\n",
      "------\n",
      "------\n",
      "Epoch 1694, Loss 3.012719, \n",
      "Params:  tensor([  5.1939, -16.3209])\n",
      "Grad:  tensor([-0.0296,  0.1674])\n",
      "------\n",
      "------\n",
      "Epoch 1695, Loss 3.012431, \n",
      "Params:  tensor([  5.1942, -16.3226])\n",
      "Grad:  tensor([-0.0295,  0.1671])\n",
      "------\n",
      "------\n",
      "Epoch 1696, Loss 3.012141, \n",
      "Params:  tensor([  5.1945, -16.3243])\n",
      "Grad:  tensor([-0.0295,  0.1668])\n",
      "------\n",
      "------\n",
      "Epoch 1697, Loss 3.011855, \n",
      "Params:  tensor([  5.1948, -16.3259])\n",
      "Grad:  tensor([-0.0294,  0.1666])\n",
      "------\n",
      "------\n",
      "Epoch 1698, Loss 3.011570, \n",
      "Params:  tensor([  5.1951, -16.3276])\n",
      "Grad:  tensor([-0.0294,  0.1663])\n",
      "------\n",
      "------\n",
      "Epoch 1699, Loss 3.011284, \n",
      "Params:  tensor([  5.1954, -16.3293])\n",
      "Grad:  tensor([-0.0293,  0.1660])\n",
      "------\n",
      "------\n",
      "Epoch 1700, Loss 3.011001, \n",
      "Params:  tensor([  5.1957, -16.3309])\n",
      "Grad:  tensor([-0.0293,  0.1657])\n",
      "------\n",
      "------\n",
      "Epoch 1701, Loss 3.010718, \n",
      "Params:  tensor([  5.1960, -16.3326])\n",
      "Grad:  tensor([-0.0292,  0.1654])\n",
      "------\n",
      "------\n",
      "Epoch 1702, Loss 3.010436, \n",
      "Params:  tensor([  5.1963, -16.3342])\n",
      "Grad:  tensor([-0.0292,  0.1652])\n",
      "------\n",
      "------\n",
      "Epoch 1703, Loss 3.010156, \n",
      "Params:  tensor([  5.1966, -16.3359])\n",
      "Grad:  tensor([-0.0291,  0.1649])\n",
      "------\n",
      "------\n",
      "Epoch 1704, Loss 3.009876, \n",
      "Params:  tensor([  5.1968, -16.3375])\n",
      "Grad:  tensor([-0.0291,  0.1646])\n",
      "------\n",
      "------\n",
      "Epoch 1705, Loss 3.009595, \n",
      "Params:  tensor([  5.1971, -16.3392])\n",
      "Grad:  tensor([-0.0290,  0.1643])\n",
      "------\n",
      "------\n",
      "Epoch 1706, Loss 3.009319, \n",
      "Params:  tensor([  5.1974, -16.3408])\n",
      "Grad:  tensor([-0.0290,  0.1640])\n",
      "------\n",
      "------\n",
      "Epoch 1707, Loss 3.009040, \n",
      "Params:  tensor([  5.1977, -16.3424])\n",
      "Grad:  tensor([-0.0289,  0.1638])\n",
      "------\n",
      "------\n",
      "Epoch 1708, Loss 3.008763, \n",
      "Params:  tensor([  5.1980, -16.3441])\n",
      "Grad:  tensor([-0.0289,  0.1635])\n",
      "------\n",
      "------\n",
      "Epoch 1709, Loss 3.008487, \n",
      "Params:  tensor([  5.1983, -16.3457])\n",
      "Grad:  tensor([-0.0288,  0.1632])\n",
      "------\n",
      "------\n",
      "Epoch 1710, Loss 3.008215, \n",
      "Params:  tensor([  5.1986, -16.3473])\n",
      "Grad:  tensor([-0.0288,  0.1629])\n",
      "------\n",
      "------\n",
      "Epoch 1711, Loss 3.007941, \n",
      "Params:  tensor([  5.1989, -16.3490])\n",
      "Grad:  tensor([-0.0287,  0.1626])\n",
      "------\n",
      "------\n",
      "Epoch 1712, Loss 3.007668, \n",
      "Params:  tensor([  5.1992, -16.3506])\n",
      "Grad:  tensor([-0.0287,  0.1624])\n",
      "------\n",
      "------\n",
      "Epoch 1713, Loss 3.007397, \n",
      "Params:  tensor([  5.1994, -16.3522])\n",
      "Grad:  tensor([-0.0286,  0.1621])\n",
      "------\n",
      "------\n",
      "Epoch 1714, Loss 3.007126, \n",
      "Params:  tensor([  5.1997, -16.3538])\n",
      "Grad:  tensor([-0.0286,  0.1618])\n",
      "------\n",
      "------\n",
      "Epoch 1715, Loss 3.006857, \n",
      "Params:  tensor([  5.2000, -16.3554])\n",
      "Grad:  tensor([-0.0285,  0.1615])\n",
      "------\n",
      "------\n",
      "Epoch 1716, Loss 3.006586, \n",
      "Params:  tensor([  5.2003, -16.3570])\n",
      "Grad:  tensor([-0.0285,  0.1613])\n",
      "------\n",
      "------\n",
      "Epoch 1717, Loss 3.006318, \n",
      "Params:  tensor([  5.2006, -16.3587])\n",
      "Grad:  tensor([-0.0284,  0.1610])\n",
      "------\n",
      "------\n",
      "Epoch 1718, Loss 3.006052, \n",
      "Params:  tensor([  5.2009, -16.3603])\n",
      "Grad:  tensor([-0.0284,  0.1607])\n",
      "------\n",
      "------\n",
      "Epoch 1719, Loss 3.005785, \n",
      "Params:  tensor([  5.2012, -16.3619])\n",
      "Grad:  tensor([-0.0284,  0.1604])\n",
      "------\n",
      "------\n",
      "Epoch 1720, Loss 3.005521, \n",
      "Params:  tensor([  5.2014, -16.3635])\n",
      "Grad:  tensor([-0.0283,  0.1602])\n",
      "------\n",
      "------\n",
      "Epoch 1721, Loss 3.005256, \n",
      "Params:  tensor([  5.2017, -16.3651])\n",
      "Grad:  tensor([-0.0283,  0.1599])\n",
      "------\n",
      "------\n",
      "Epoch 1722, Loss 3.004993, \n",
      "Params:  tensor([  5.2020, -16.3667])\n",
      "Grad:  tensor([-0.0282,  0.1596])\n",
      "------\n",
      "------\n",
      "Epoch 1723, Loss 3.004729, \n",
      "Params:  tensor([  5.2023, -16.3683])\n",
      "Grad:  tensor([-0.0281,  0.1594])\n",
      "------\n",
      "------\n",
      "Epoch 1724, Loss 3.004467, \n",
      "Params:  tensor([  5.2026, -16.3699])\n",
      "Grad:  tensor([-0.0281,  0.1591])\n",
      "------\n",
      "------\n",
      "Epoch 1725, Loss 3.004207, \n",
      "Params:  tensor([  5.2028, -16.3714])\n",
      "Grad:  tensor([-0.0280,  0.1588])\n",
      "------\n",
      "------\n",
      "Epoch 1726, Loss 3.003947, \n",
      "Params:  tensor([  5.2031, -16.3730])\n",
      "Grad:  tensor([-0.0280,  0.1586])\n",
      "------\n",
      "------\n",
      "Epoch 1727, Loss 3.003690, \n",
      "Params:  tensor([  5.2034, -16.3746])\n",
      "Grad:  tensor([-0.0280,  0.1583])\n",
      "------\n",
      "------\n",
      "Epoch 1728, Loss 3.003431, \n",
      "Params:  tensor([  5.2037, -16.3762])\n",
      "Grad:  tensor([-0.0279,  0.1580])\n",
      "------\n",
      "------\n",
      "Epoch 1729, Loss 3.003174, \n",
      "Params:  tensor([  5.2040, -16.3778])\n",
      "Grad:  tensor([-0.0279,  0.1577])\n",
      "------\n",
      "------\n",
      "Epoch 1730, Loss 3.002918, \n",
      "Params:  tensor([  5.2042, -16.3793])\n",
      "Grad:  tensor([-0.0278,  0.1575])\n",
      "------\n",
      "------\n",
      "Epoch 1731, Loss 3.002661, \n",
      "Params:  tensor([  5.2045, -16.3809])\n",
      "Grad:  tensor([-0.0278,  0.1572])\n",
      "------\n",
      "------\n",
      "Epoch 1732, Loss 3.002406, \n",
      "Params:  tensor([  5.2048, -16.3825])\n",
      "Grad:  tensor([-0.0277,  0.1569])\n",
      "------\n",
      "------\n",
      "Epoch 1733, Loss 3.002152, \n",
      "Params:  tensor([  5.2051, -16.3840])\n",
      "Grad:  tensor([-0.0277,  0.1567])\n",
      "------\n",
      "------\n",
      "Epoch 1734, Loss 3.001901, \n",
      "Params:  tensor([  5.2053, -16.3856])\n",
      "Grad:  tensor([-0.0276,  0.1564])\n",
      "------\n",
      "------\n",
      "Epoch 1735, Loss 3.001649, \n",
      "Params:  tensor([  5.2056, -16.3872])\n",
      "Grad:  tensor([-0.0276,  0.1561])\n",
      "------\n",
      "------\n",
      "Epoch 1736, Loss 3.001395, \n",
      "Params:  tensor([  5.2059, -16.3887])\n",
      "Grad:  tensor([-0.0275,  0.1559])\n",
      "------\n",
      "------\n",
      "Epoch 1737, Loss 3.001145, \n",
      "Params:  tensor([  5.2062, -16.3903])\n",
      "Grad:  tensor([-0.0275,  0.1556])\n",
      "------\n",
      "------\n",
      "Epoch 1738, Loss 3.000898, \n",
      "Params:  tensor([  5.2064, -16.3918])\n",
      "Grad:  tensor([-0.0274,  0.1553])\n",
      "------\n",
      "------\n",
      "Epoch 1739, Loss 3.000648, \n",
      "Params:  tensor([  5.2067, -16.3934])\n",
      "Grad:  tensor([-0.0274,  0.1551])\n",
      "------\n",
      "------\n",
      "Epoch 1740, Loss 3.000400, \n",
      "Params:  tensor([  5.2070, -16.3949])\n",
      "Grad:  tensor([-0.0273,  0.1548])\n",
      "------\n",
      "------\n",
      "Epoch 1741, Loss 3.000154, \n",
      "Params:  tensor([  5.2073, -16.3965])\n",
      "Grad:  tensor([-0.0273,  0.1546])\n",
      "------\n",
      "------\n",
      "Epoch 1742, Loss 2.999907, \n",
      "Params:  tensor([  5.2075, -16.3980])\n",
      "Grad:  tensor([-0.0273,  0.1543])\n",
      "------\n",
      "------\n",
      "Epoch 1743, Loss 2.999662, \n",
      "Params:  tensor([  5.2078, -16.3996])\n",
      "Grad:  tensor([-0.0272,  0.1540])\n",
      "------\n",
      "------\n",
      "Epoch 1744, Loss 2.999417, \n",
      "Params:  tensor([  5.2081, -16.4011])\n",
      "Grad:  tensor([-0.0272,  0.1538])\n",
      "------\n",
      "------\n",
      "Epoch 1745, Loss 2.999174, \n",
      "Params:  tensor([  5.2084, -16.4026])\n",
      "Grad:  tensor([-0.0271,  0.1535])\n",
      "------\n",
      "------\n",
      "Epoch 1746, Loss 2.998930, \n",
      "Params:  tensor([  5.2086, -16.4042])\n",
      "Grad:  tensor([-0.0271,  0.1533])\n",
      "------\n",
      "------\n",
      "Epoch 1747, Loss 2.998688, \n",
      "Params:  tensor([  5.2089, -16.4057])\n",
      "Grad:  tensor([-0.0270,  0.1530])\n",
      "------\n",
      "------\n",
      "Epoch 1748, Loss 2.998448, \n",
      "Params:  tensor([  5.2092, -16.4072])\n",
      "Grad:  tensor([-0.0270,  0.1527])\n",
      "------\n",
      "------\n",
      "Epoch 1749, Loss 2.998208, \n",
      "Params:  tensor([  5.2094, -16.4088])\n",
      "Grad:  tensor([-0.0269,  0.1525])\n",
      "------\n",
      "------\n",
      "Epoch 1750, Loss 2.997968, \n",
      "Params:  tensor([  5.2097, -16.4103])\n",
      "Grad:  tensor([-0.0269,  0.1522])\n",
      "------\n",
      "------\n",
      "Epoch 1751, Loss 2.997730, \n",
      "Params:  tensor([  5.2100, -16.4118])\n",
      "Grad:  tensor([-0.0268,  0.1520])\n",
      "------\n",
      "------\n",
      "Epoch 1752, Loss 2.997490, \n",
      "Params:  tensor([  5.2102, -16.4133])\n",
      "Grad:  tensor([-0.0268,  0.1517])\n",
      "------\n",
      "------\n",
      "Epoch 1753, Loss 2.997254, \n",
      "Params:  tensor([  5.2105, -16.4148])\n",
      "Grad:  tensor([-0.0267,  0.1514])\n",
      "------\n",
      "------\n",
      "Epoch 1754, Loss 2.997018, \n",
      "Params:  tensor([  5.2108, -16.4163])\n",
      "Grad:  tensor([-0.0267,  0.1512])\n",
      "------\n",
      "------\n",
      "Epoch 1755, Loss 2.996782, \n",
      "Params:  tensor([  5.2110, -16.4179])\n",
      "Grad:  tensor([-0.0266,  0.1509])\n",
      "------\n",
      "------\n",
      "Epoch 1756, Loss 2.996548, \n",
      "Params:  tensor([  5.2113, -16.4194])\n",
      "Grad:  tensor([-0.0266,  0.1507])\n",
      "------\n",
      "------\n",
      "Epoch 1757, Loss 2.996313, \n",
      "Params:  tensor([  5.2116, -16.4209])\n",
      "Grad:  tensor([-0.0266,  0.1504])\n",
      "------\n",
      "------\n",
      "Epoch 1758, Loss 2.996081, \n",
      "Params:  tensor([  5.2118, -16.4224])\n",
      "Grad:  tensor([-0.0265,  0.1502])\n",
      "------\n",
      "------\n",
      "Epoch 1759, Loss 2.995847, \n",
      "Params:  tensor([  5.2121, -16.4239])\n",
      "Grad:  tensor([-0.0265,  0.1499])\n",
      "------\n",
      "------\n",
      "Epoch 1760, Loss 2.995615, \n",
      "Params:  tensor([  5.2124, -16.4254])\n",
      "Grad:  tensor([-0.0264,  0.1496])\n",
      "------\n",
      "------\n",
      "Epoch 1761, Loss 2.995387, \n",
      "Params:  tensor([  5.2126, -16.4269])\n",
      "Grad:  tensor([-0.0264,  0.1494])\n",
      "------\n",
      "------\n",
      "Epoch 1762, Loss 2.995156, \n",
      "Params:  tensor([  5.2129, -16.4283])\n",
      "Grad:  tensor([-0.0263,  0.1491])\n",
      "------\n",
      "------\n",
      "Epoch 1763, Loss 2.994928, \n",
      "Params:  tensor([  5.2132, -16.4298])\n",
      "Grad:  tensor([-0.0263,  0.1489])\n",
      "------\n",
      "------\n",
      "Epoch 1764, Loss 2.994699, \n",
      "Params:  tensor([  5.2134, -16.4313])\n",
      "Grad:  tensor([-0.0263,  0.1486])\n",
      "------\n",
      "------\n",
      "Epoch 1765, Loss 2.994471, \n",
      "Params:  tensor([  5.2137, -16.4328])\n",
      "Grad:  tensor([-0.0262,  0.1484])\n",
      "------\n",
      "------\n",
      "Epoch 1766, Loss 2.994245, \n",
      "Params:  tensor([  5.2139, -16.4343])\n",
      "Grad:  tensor([-0.0262,  0.1481])\n",
      "------\n",
      "------\n",
      "Epoch 1767, Loss 2.994019, \n",
      "Params:  tensor([  5.2142, -16.4358])\n",
      "Grad:  tensor([-0.0261,  0.1479])\n",
      "------\n",
      "------\n",
      "Epoch 1768, Loss 2.993794, \n",
      "Params:  tensor([  5.2145, -16.4372])\n",
      "Grad:  tensor([-0.0261,  0.1476])\n",
      "------\n",
      "------\n",
      "Epoch 1769, Loss 2.993569, \n",
      "Params:  tensor([  5.2147, -16.4387])\n",
      "Grad:  tensor([-0.0260,  0.1474])\n",
      "------\n",
      "------\n",
      "Epoch 1770, Loss 2.993344, \n",
      "Params:  tensor([  5.2150, -16.4402])\n",
      "Grad:  tensor([-0.0260,  0.1471])\n",
      "------\n",
      "------\n",
      "Epoch 1771, Loss 2.993121, \n",
      "Params:  tensor([  5.2152, -16.4417])\n",
      "Grad:  tensor([-0.0260,  0.1469])\n",
      "------\n",
      "------\n",
      "Epoch 1772, Loss 2.992900, \n",
      "Params:  tensor([  5.2155, -16.4431])\n",
      "Grad:  tensor([-0.0259,  0.1466])\n",
      "------\n",
      "------\n",
      "Epoch 1773, Loss 2.992678, \n",
      "Params:  tensor([  5.2158, -16.4446])\n",
      "Grad:  tensor([-0.0259,  0.1464])\n",
      "------\n",
      "------\n",
      "Epoch 1774, Loss 2.992457, \n",
      "Params:  tensor([  5.2160, -16.4460])\n",
      "Grad:  tensor([-0.0258,  0.1461])\n",
      "------\n",
      "------\n",
      "Epoch 1775, Loss 2.992237, \n",
      "Params:  tensor([  5.2163, -16.4475])\n",
      "Grad:  tensor([-0.0258,  0.1459])\n",
      "------\n",
      "------\n",
      "Epoch 1776, Loss 2.992017, \n",
      "Params:  tensor([  5.2165, -16.4490])\n",
      "Grad:  tensor([-0.0257,  0.1456])\n",
      "------\n",
      "------\n",
      "Epoch 1777, Loss 2.991798, \n",
      "Params:  tensor([  5.2168, -16.4504])\n",
      "Grad:  tensor([-0.0257,  0.1454])\n",
      "------\n",
      "------\n",
      "Epoch 1778, Loss 2.991582, \n",
      "Params:  tensor([  5.2170, -16.4519])\n",
      "Grad:  tensor([-0.0256,  0.1451])\n",
      "------\n",
      "------\n",
      "Epoch 1779, Loss 2.991366, \n",
      "Params:  tensor([  5.2173, -16.4533])\n",
      "Grad:  tensor([-0.0256,  0.1449])\n",
      "------\n",
      "------\n",
      "Epoch 1780, Loss 2.991146, \n",
      "Params:  tensor([  5.2176, -16.4548])\n",
      "Grad:  tensor([-0.0256,  0.1446])\n",
      "------\n",
      "------\n",
      "Epoch 1781, Loss 2.990932, \n",
      "Params:  tensor([  5.2178, -16.4562])\n",
      "Grad:  tensor([-0.0255,  0.1444])\n",
      "------\n",
      "------\n",
      "Epoch 1782, Loss 2.990719, \n",
      "Params:  tensor([  5.2181, -16.4576])\n",
      "Grad:  tensor([-0.0255,  0.1442])\n",
      "------\n",
      "------\n",
      "Epoch 1783, Loss 2.990503, \n",
      "Params:  tensor([  5.2183, -16.4591])\n",
      "Grad:  tensor([-0.0254,  0.1439])\n",
      "------\n",
      "------\n",
      "Epoch 1784, Loss 2.990288, \n",
      "Params:  tensor([  5.2186, -16.4605])\n",
      "Grad:  tensor([-0.0254,  0.1437])\n",
      "------\n",
      "------\n",
      "Epoch 1785, Loss 2.990078, \n",
      "Params:  tensor([  5.2188, -16.4620])\n",
      "Grad:  tensor([-0.0253,  0.1434])\n",
      "------\n",
      "------\n",
      "Epoch 1786, Loss 2.989866, \n",
      "Params:  tensor([  5.2191, -16.4634])\n",
      "Grad:  tensor([-0.0253,  0.1432])\n",
      "------\n",
      "------\n",
      "Epoch 1787, Loss 2.989655, \n",
      "Params:  tensor([  5.2193, -16.4648])\n",
      "Grad:  tensor([-0.0252,  0.1429])\n",
      "------\n",
      "------\n",
      "Epoch 1788, Loss 2.989443, \n",
      "Params:  tensor([  5.2196, -16.4662])\n",
      "Grad:  tensor([-0.0252,  0.1427])\n",
      "------\n",
      "------\n",
      "Epoch 1789, Loss 2.989233, \n",
      "Params:  tensor([  5.2198, -16.4677])\n",
      "Grad:  tensor([-0.0252,  0.1424])\n",
      "------\n",
      "------\n",
      "Epoch 1790, Loss 2.989025, \n",
      "Params:  tensor([  5.2201, -16.4691])\n",
      "Grad:  tensor([-0.0251,  0.1422])\n",
      "------\n",
      "------\n",
      "Epoch 1791, Loss 2.988817, \n",
      "Params:  tensor([  5.2203, -16.4705])\n",
      "Grad:  tensor([-0.0251,  0.1420])\n",
      "------\n",
      "------\n",
      "Epoch 1792, Loss 2.988609, \n",
      "Params:  tensor([  5.2206, -16.4719])\n",
      "Grad:  tensor([-0.0250,  0.1417])\n",
      "------\n",
      "------\n",
      "Epoch 1793, Loss 2.988401, \n",
      "Params:  tensor([  5.2208, -16.4733])\n",
      "Grad:  tensor([-0.0250,  0.1415])\n",
      "------\n",
      "------\n",
      "Epoch 1794, Loss 2.988195, \n",
      "Params:  tensor([  5.2211, -16.4748])\n",
      "Grad:  tensor([-0.0249,  0.1412])\n",
      "------\n",
      "------\n",
      "Epoch 1795, Loss 2.987989, \n",
      "Params:  tensor([  5.2213, -16.4762])\n",
      "Grad:  tensor([-0.0249,  0.1410])\n",
      "------\n",
      "------\n",
      "Epoch 1796, Loss 2.987785, \n",
      "Params:  tensor([  5.2216, -16.4776])\n",
      "Grad:  tensor([-0.0249,  0.1408])\n",
      "------\n",
      "------\n",
      "Epoch 1797, Loss 2.987582, \n",
      "Params:  tensor([  5.2218, -16.4790])\n",
      "Grad:  tensor([-0.0248,  0.1405])\n",
      "------\n",
      "------\n",
      "Epoch 1798, Loss 2.987377, \n",
      "Params:  tensor([  5.2221, -16.4804])\n",
      "Grad:  tensor([-0.0248,  0.1403])\n",
      "------\n",
      "------\n",
      "Epoch 1799, Loss 2.987174, \n",
      "Params:  tensor([  5.2223, -16.4818])\n",
      "Grad:  tensor([-0.0247,  0.1400])\n",
      "------\n",
      "------\n",
      "Epoch 1800, Loss 2.986974, \n",
      "Params:  tensor([  5.2226, -16.4832])\n",
      "Grad:  tensor([-0.0247,  0.1398])\n",
      "------\n",
      "------\n",
      "Epoch 1801, Loss 2.986771, \n",
      "Params:  tensor([  5.2228, -16.4846])\n",
      "Grad:  tensor([-0.0246,  0.1396])\n",
      "------\n",
      "------\n",
      "Epoch 1802, Loss 2.986570, \n",
      "Params:  tensor([  5.2231, -16.4860])\n",
      "Grad:  tensor([-0.0246,  0.1393])\n",
      "------\n",
      "------\n",
      "Epoch 1803, Loss 2.986371, \n",
      "Params:  tensor([  5.2233, -16.4874])\n",
      "Grad:  tensor([-0.0246,  0.1391])\n",
      "------\n",
      "------\n",
      "Epoch 1804, Loss 2.986171, \n",
      "Params:  tensor([  5.2236, -16.4888])\n",
      "Grad:  tensor([-0.0245,  0.1389])\n",
      "------\n",
      "------\n",
      "Epoch 1805, Loss 2.985972, \n",
      "Params:  tensor([  5.2238, -16.4901])\n",
      "Grad:  tensor([-0.0245,  0.1386])\n",
      "------\n",
      "------\n",
      "Epoch 1806, Loss 2.985774, \n",
      "Params:  tensor([  5.2241, -16.4915])\n",
      "Grad:  tensor([-0.0245,  0.1384])\n",
      "------\n",
      "------\n",
      "Epoch 1807, Loss 2.985578, \n",
      "Params:  tensor([  5.2243, -16.4929])\n",
      "Grad:  tensor([-0.0244,  0.1382])\n",
      "------\n",
      "------\n",
      "Epoch 1808, Loss 2.985381, \n",
      "Params:  tensor([  5.2245, -16.4943])\n",
      "Grad:  tensor([-0.0244,  0.1379])\n",
      "------\n",
      "------\n",
      "Epoch 1809, Loss 2.985184, \n",
      "Params:  tensor([  5.2248, -16.4957])\n",
      "Grad:  tensor([-0.0243,  0.1377])\n",
      "------\n",
      "------\n",
      "Epoch 1810, Loss 2.984989, \n",
      "Params:  tensor([  5.2250, -16.4970])\n",
      "Grad:  tensor([-0.0243,  0.1374])\n",
      "------\n",
      "------\n",
      "Epoch 1811, Loss 2.984793, \n",
      "Params:  tensor([  5.2253, -16.4984])\n",
      "Grad:  tensor([-0.0243,  0.1372])\n",
      "------\n",
      "------\n",
      "Epoch 1812, Loss 2.984601, \n",
      "Params:  tensor([  5.2255, -16.4998])\n",
      "Grad:  tensor([-0.0242,  0.1370])\n",
      "------\n",
      "------\n",
      "Epoch 1813, Loss 2.984407, \n",
      "Params:  tensor([  5.2258, -16.5011])\n",
      "Grad:  tensor([-0.0242,  0.1368])\n",
      "------\n",
      "------\n",
      "Epoch 1814, Loss 2.984215, \n",
      "Params:  tensor([  5.2260, -16.5025])\n",
      "Grad:  tensor([-0.0241,  0.1365])\n",
      "------\n",
      "------\n",
      "Epoch 1815, Loss 2.984022, \n",
      "Params:  tensor([  5.2262, -16.5039])\n",
      "Grad:  tensor([-0.0241,  0.1363])\n",
      "------\n",
      "------\n",
      "Epoch 1816, Loss 2.983831, \n",
      "Params:  tensor([  5.2265, -16.5052])\n",
      "Grad:  tensor([-0.0240,  0.1361])\n",
      "------\n",
      "------\n",
      "Epoch 1817, Loss 2.983639, \n",
      "Params:  tensor([  5.2267, -16.5066])\n",
      "Grad:  tensor([-0.0240,  0.1358])\n",
      "------\n",
      "------\n",
      "Epoch 1818, Loss 2.983449, \n",
      "Params:  tensor([  5.2270, -16.5079])\n",
      "Grad:  tensor([-0.0239,  0.1356])\n",
      "------\n",
      "------\n",
      "Epoch 1819, Loss 2.983259, \n",
      "Params:  tensor([  5.2272, -16.5093])\n",
      "Grad:  tensor([-0.0239,  0.1354])\n",
      "------\n",
      "------\n",
      "Epoch 1820, Loss 2.983073, \n",
      "Params:  tensor([  5.2274, -16.5107])\n",
      "Grad:  tensor([-0.0239,  0.1351])\n",
      "------\n",
      "------\n",
      "Epoch 1821, Loss 2.982884, \n",
      "Params:  tensor([  5.2277, -16.5120])\n",
      "Grad:  tensor([-0.0238,  0.1349])\n",
      "------\n",
      "------\n",
      "Epoch 1822, Loss 2.982697, \n",
      "Params:  tensor([  5.2279, -16.5133])\n",
      "Grad:  tensor([-0.0238,  0.1347])\n",
      "------\n",
      "------\n",
      "Epoch 1823, Loss 2.982510, \n",
      "Params:  tensor([  5.2281, -16.5147])\n",
      "Grad:  tensor([-0.0237,  0.1344])\n",
      "------\n",
      "------\n",
      "Epoch 1824, Loss 2.982322, \n",
      "Params:  tensor([  5.2284, -16.5160])\n",
      "Grad:  tensor([-0.0237,  0.1342])\n",
      "------\n",
      "------\n",
      "Epoch 1825, Loss 2.982137, \n",
      "Params:  tensor([  5.2286, -16.5174])\n",
      "Grad:  tensor([-0.0237,  0.1340])\n",
      "------\n",
      "------\n",
      "Epoch 1826, Loss 2.981953, \n",
      "Params:  tensor([  5.2289, -16.5187])\n",
      "Grad:  tensor([-0.0236,  0.1338])\n",
      "------\n",
      "------\n",
      "Epoch 1827, Loss 2.981769, \n",
      "Params:  tensor([  5.2291, -16.5200])\n",
      "Grad:  tensor([-0.0236,  0.1335])\n",
      "------\n",
      "------\n",
      "Epoch 1828, Loss 2.981586, \n",
      "Params:  tensor([  5.2293, -16.5214])\n",
      "Grad:  tensor([-0.0236,  0.1333])\n",
      "------\n",
      "------\n",
      "Epoch 1829, Loss 2.981402, \n",
      "Params:  tensor([  5.2296, -16.5227])\n",
      "Grad:  tensor([-0.0235,  0.1331])\n",
      "------\n",
      "------\n",
      "Epoch 1830, Loss 2.981219, \n",
      "Params:  tensor([  5.2298, -16.5240])\n",
      "Grad:  tensor([-0.0235,  0.1329])\n",
      "------\n",
      "------\n",
      "Epoch 1831, Loss 2.981037, \n",
      "Params:  tensor([  5.2300, -16.5254])\n",
      "Grad:  tensor([-0.0235,  0.1326])\n",
      "------\n",
      "------\n",
      "Epoch 1832, Loss 2.980856, \n",
      "Params:  tensor([  5.2303, -16.5267])\n",
      "Grad:  tensor([-0.0234,  0.1324])\n",
      "------\n",
      "------\n",
      "Epoch 1833, Loss 2.980675, \n",
      "Params:  tensor([  5.2305, -16.5280])\n",
      "Grad:  tensor([-0.0234,  0.1322])\n",
      "------\n",
      "------\n",
      "Epoch 1834, Loss 2.980495, \n",
      "Params:  tensor([  5.2307, -16.5293])\n",
      "Grad:  tensor([-0.0233,  0.1320])\n",
      "------\n",
      "------\n",
      "Epoch 1835, Loss 2.980315, \n",
      "Params:  tensor([  5.2310, -16.5306])\n",
      "Grad:  tensor([-0.0233,  0.1317])\n",
      "------\n",
      "------\n",
      "Epoch 1836, Loss 2.980137, \n",
      "Params:  tensor([  5.2312, -16.5320])\n",
      "Grad:  tensor([-0.0232,  0.1315])\n",
      "------\n",
      "------\n",
      "Epoch 1837, Loss 2.979958, \n",
      "Params:  tensor([  5.2314, -16.5333])\n",
      "Grad:  tensor([-0.0232,  0.1313])\n",
      "------\n",
      "------\n",
      "Epoch 1838, Loss 2.979782, \n",
      "Params:  tensor([  5.2317, -16.5346])\n",
      "Grad:  tensor([-0.0232,  0.1311])\n",
      "------\n",
      "------\n",
      "Epoch 1839, Loss 2.979604, \n",
      "Params:  tensor([  5.2319, -16.5359])\n",
      "Grad:  tensor([-0.0231,  0.1308])\n",
      "------\n",
      "------\n",
      "Epoch 1840, Loss 2.979428, \n",
      "Params:  tensor([  5.2321, -16.5372])\n",
      "Grad:  tensor([-0.0231,  0.1306])\n",
      "------\n",
      "------\n",
      "Epoch 1841, Loss 2.979253, \n",
      "Params:  tensor([  5.2324, -16.5385])\n",
      "Grad:  tensor([-0.0230,  0.1304])\n",
      "------\n",
      "------\n",
      "Epoch 1842, Loss 2.979078, \n",
      "Params:  tensor([  5.2326, -16.5398])\n",
      "Grad:  tensor([-0.0230,  0.1302])\n",
      "------\n",
      "------\n",
      "Epoch 1843, Loss 2.978902, \n",
      "Params:  tensor([  5.2328, -16.5411])\n",
      "Grad:  tensor([-0.0229,  0.1300])\n",
      "------\n",
      "------\n",
      "Epoch 1844, Loss 2.978729, \n",
      "Params:  tensor([  5.2330, -16.5424])\n",
      "Grad:  tensor([-0.0229,  0.1297])\n",
      "------\n",
      "------\n",
      "Epoch 1845, Loss 2.978556, \n",
      "Params:  tensor([  5.2333, -16.5437])\n",
      "Grad:  tensor([-0.0229,  0.1295])\n",
      "------\n",
      "------\n",
      "Epoch 1846, Loss 2.978382, \n",
      "Params:  tensor([  5.2335, -16.5450])\n",
      "Grad:  tensor([-0.0228,  0.1293])\n",
      "------\n",
      "------\n",
      "Epoch 1847, Loss 2.978211, \n",
      "Params:  tensor([  5.2337, -16.5463])\n",
      "Grad:  tensor([-0.0228,  0.1291])\n",
      "------\n",
      "------\n",
      "Epoch 1848, Loss 2.978039, \n",
      "Params:  tensor([  5.2340, -16.5476])\n",
      "Grad:  tensor([-0.0228,  0.1288])\n",
      "------\n",
      "------\n",
      "Epoch 1849, Loss 2.977867, \n",
      "Params:  tensor([  5.2342, -16.5489])\n",
      "Grad:  tensor([-0.0227,  0.1286])\n",
      "------\n",
      "------\n",
      "Epoch 1850, Loss 2.977696, \n",
      "Params:  tensor([  5.2344, -16.5501])\n",
      "Grad:  tensor([-0.0227,  0.1284])\n",
      "------\n",
      "------\n",
      "Epoch 1851, Loss 2.977527, \n",
      "Params:  tensor([  5.2346, -16.5514])\n",
      "Grad:  tensor([-0.0227,  0.1282])\n",
      "------\n",
      "------\n",
      "Epoch 1852, Loss 2.977357, \n",
      "Params:  tensor([  5.2349, -16.5527])\n",
      "Grad:  tensor([-0.0226,  0.1280])\n",
      "------\n",
      "------\n",
      "Epoch 1853, Loss 2.977188, \n",
      "Params:  tensor([  5.2351, -16.5540])\n",
      "Grad:  tensor([-0.0226,  0.1278])\n",
      "------\n",
      "------\n",
      "Epoch 1854, Loss 2.977021, \n",
      "Params:  tensor([  5.2353, -16.5553])\n",
      "Grad:  tensor([-0.0225,  0.1275])\n",
      "------\n",
      "------\n",
      "Epoch 1855, Loss 2.976853, \n",
      "Params:  tensor([  5.2355, -16.5565])\n",
      "Grad:  tensor([-0.0225,  0.1273])\n",
      "------\n",
      "------\n",
      "Epoch 1856, Loss 2.976687, \n",
      "Params:  tensor([  5.2358, -16.5578])\n",
      "Grad:  tensor([-0.0225,  0.1271])\n",
      "------\n",
      "------\n",
      "Epoch 1857, Loss 2.976520, \n",
      "Params:  tensor([  5.2360, -16.5591])\n",
      "Grad:  tensor([-0.0224,  0.1269])\n",
      "------\n",
      "------\n",
      "Epoch 1858, Loss 2.976354, \n",
      "Params:  tensor([  5.2362, -16.5603])\n",
      "Grad:  tensor([-0.0224,  0.1267])\n",
      "------\n",
      "------\n",
      "Epoch 1859, Loss 2.976189, \n",
      "Params:  tensor([  5.2364, -16.5616])\n",
      "Grad:  tensor([-0.0223,  0.1265])\n",
      "------\n",
      "------\n",
      "Epoch 1860, Loss 2.976023, \n",
      "Params:  tensor([  5.2367, -16.5629])\n",
      "Grad:  tensor([-0.0223,  0.1263])\n",
      "------\n",
      "------\n",
      "Epoch 1861, Loss 2.975860, \n",
      "Params:  tensor([  5.2369, -16.5641])\n",
      "Grad:  tensor([-0.0223,  0.1260])\n",
      "------\n",
      "------\n",
      "Epoch 1862, Loss 2.975697, \n",
      "Params:  tensor([  5.2371, -16.5654])\n",
      "Grad:  tensor([-0.0222,  0.1258])\n",
      "------\n",
      "------\n",
      "Epoch 1863, Loss 2.975533, \n",
      "Params:  tensor([  5.2373, -16.5666])\n",
      "Grad:  tensor([-0.0222,  0.1256])\n",
      "------\n",
      "------\n",
      "Epoch 1864, Loss 2.975369, \n",
      "Params:  tensor([  5.2375, -16.5679])\n",
      "Grad:  tensor([-0.0222,  0.1254])\n",
      "------\n",
      "------\n",
      "Epoch 1865, Loss 2.975208, \n",
      "Params:  tensor([  5.2378, -16.5691])\n",
      "Grad:  tensor([-0.0221,  0.1252])\n",
      "------\n",
      "------\n",
      "Epoch 1866, Loss 2.975046, \n",
      "Params:  tensor([  5.2380, -16.5704])\n",
      "Grad:  tensor([-0.0221,  0.1250])\n",
      "------\n",
      "------\n",
      "Epoch 1867, Loss 2.974886, \n",
      "Params:  tensor([  5.2382, -16.5716])\n",
      "Grad:  tensor([-0.0220,  0.1248])\n",
      "------\n",
      "------\n",
      "Epoch 1868, Loss 2.974725, \n",
      "Params:  tensor([  5.2384, -16.5729])\n",
      "Grad:  tensor([-0.0220,  0.1245])\n",
      "------\n",
      "------\n",
      "Epoch 1869, Loss 2.974565, \n",
      "Params:  tensor([  5.2386, -16.5741])\n",
      "Grad:  tensor([-0.0220,  0.1243])\n",
      "------\n",
      "------\n",
      "Epoch 1870, Loss 2.974406, \n",
      "Params:  tensor([  5.2389, -16.5754])\n",
      "Grad:  tensor([-0.0219,  0.1241])\n",
      "------\n",
      "------\n",
      "Epoch 1871, Loss 2.974248, \n",
      "Params:  tensor([  5.2391, -16.5766])\n",
      "Grad:  tensor([-0.0219,  0.1239])\n",
      "------\n",
      "------\n",
      "Epoch 1872, Loss 2.974088, \n",
      "Params:  tensor([  5.2393, -16.5778])\n",
      "Grad:  tensor([-0.0219,  0.1237])\n",
      "------\n",
      "------\n",
      "Epoch 1873, Loss 2.973930, \n",
      "Params:  tensor([  5.2395, -16.5791])\n",
      "Grad:  tensor([-0.0218,  0.1235])\n",
      "------\n",
      "------\n",
      "Epoch 1874, Loss 2.973776, \n",
      "Params:  tensor([  5.2397, -16.5803])\n",
      "Grad:  tensor([-0.0218,  0.1233])\n",
      "------\n",
      "------\n",
      "Epoch 1875, Loss 2.973618, \n",
      "Params:  tensor([  5.2400, -16.5815])\n",
      "Grad:  tensor([-0.0217,  0.1231])\n",
      "------\n",
      "------\n",
      "Epoch 1876, Loss 2.973463, \n",
      "Params:  tensor([  5.2402, -16.5828])\n",
      "Grad:  tensor([-0.0217,  0.1229])\n",
      "------\n",
      "------\n",
      "Epoch 1877, Loss 2.973307, \n",
      "Params:  tensor([  5.2404, -16.5840])\n",
      "Grad:  tensor([-0.0217,  0.1227])\n",
      "------\n",
      "------\n",
      "Epoch 1878, Loss 2.973151, \n",
      "Params:  tensor([  5.2406, -16.5852])\n",
      "Grad:  tensor([-0.0216,  0.1224])\n",
      "------\n",
      "------\n",
      "Epoch 1879, Loss 2.972996, \n",
      "Params:  tensor([  5.2408, -16.5864])\n",
      "Grad:  tensor([-0.0216,  0.1222])\n",
      "------\n",
      "------\n",
      "Epoch 1880, Loss 2.972843, \n",
      "Params:  tensor([  5.2410, -16.5877])\n",
      "Grad:  tensor([-0.0215,  0.1220])\n",
      "------\n",
      "------\n",
      "Epoch 1881, Loss 2.972690, \n",
      "Params:  tensor([  5.2413, -16.5889])\n",
      "Grad:  tensor([-0.0215,  0.1218])\n",
      "------\n",
      "------\n",
      "Epoch 1882, Loss 2.972536, \n",
      "Params:  tensor([  5.2415, -16.5901])\n",
      "Grad:  tensor([-0.0215,  0.1216])\n",
      "------\n",
      "------\n",
      "Epoch 1883, Loss 2.972383, \n",
      "Params:  tensor([  5.2417, -16.5913])\n",
      "Grad:  tensor([-0.0214,  0.1214])\n",
      "------\n",
      "------\n",
      "Epoch 1884, Loss 2.972232, \n",
      "Params:  tensor([  5.2419, -16.5925])\n",
      "Grad:  tensor([-0.0214,  0.1212])\n",
      "------\n",
      "------\n",
      "Epoch 1885, Loss 2.972081, \n",
      "Params:  tensor([  5.2421, -16.5937])\n",
      "Grad:  tensor([-0.0214,  0.1210])\n",
      "------\n",
      "------\n",
      "Epoch 1886, Loss 2.971931, \n",
      "Params:  tensor([  5.2423, -16.5949])\n",
      "Grad:  tensor([-0.0213,  0.1208])\n",
      "------\n",
      "------\n",
      "Epoch 1887, Loss 2.971780, \n",
      "Params:  tensor([  5.2425, -16.5961])\n",
      "Grad:  tensor([-0.0213,  0.1206])\n",
      "------\n",
      "------\n",
      "Epoch 1888, Loss 2.971630, \n",
      "Params:  tensor([  5.2427, -16.5974])\n",
      "Grad:  tensor([-0.0213,  0.1204])\n",
      "------\n",
      "------\n",
      "Epoch 1889, Loss 2.971481, \n",
      "Params:  tensor([  5.2430, -16.5986])\n",
      "Grad:  tensor([-0.0212,  0.1202])\n",
      "------\n",
      "------\n",
      "Epoch 1890, Loss 2.971332, \n",
      "Params:  tensor([  5.2432, -16.5998])\n",
      "Grad:  tensor([-0.0212,  0.1200])\n",
      "------\n",
      "------\n",
      "Epoch 1891, Loss 2.971184, \n",
      "Params:  tensor([  5.2434, -16.6010])\n",
      "Grad:  tensor([-0.0212,  0.1198])\n",
      "------\n",
      "------\n",
      "Epoch 1892, Loss 2.971035, \n",
      "Params:  tensor([  5.2436, -16.6021])\n",
      "Grad:  tensor([-0.0211,  0.1196])\n",
      "------\n",
      "------\n",
      "Epoch 1893, Loss 2.970888, \n",
      "Params:  tensor([  5.2438, -16.6033])\n",
      "Grad:  tensor([-0.0211,  0.1194])\n",
      "------\n",
      "------\n",
      "Epoch 1894, Loss 2.970741, \n",
      "Params:  tensor([  5.2440, -16.6045])\n",
      "Grad:  tensor([-0.0211,  0.1192])\n",
      "------\n",
      "------\n",
      "Epoch 1895, Loss 2.970596, \n",
      "Params:  tensor([  5.2442, -16.6057])\n",
      "Grad:  tensor([-0.0210,  0.1190])\n",
      "------\n",
      "------\n",
      "Epoch 1896, Loss 2.970449, \n",
      "Params:  tensor([  5.2444, -16.6069])\n",
      "Grad:  tensor([-0.0210,  0.1188])\n",
      "------\n",
      "------\n",
      "Epoch 1897, Loss 2.970304, \n",
      "Params:  tensor([  5.2446, -16.6081])\n",
      "Grad:  tensor([-0.0209,  0.1186])\n",
      "------\n",
      "------\n",
      "Epoch 1898, Loss 2.970159, \n",
      "Params:  tensor([  5.2449, -16.6093])\n",
      "Grad:  tensor([-0.0209,  0.1183])\n",
      "------\n",
      "------\n",
      "Epoch 1899, Loss 2.970016, \n",
      "Params:  tensor([  5.2451, -16.6105])\n",
      "Grad:  tensor([-0.0209,  0.1182])\n",
      "------\n",
      "------\n",
      "Epoch 1900, Loss 2.969871, \n",
      "Params:  tensor([  5.2453, -16.6116])\n",
      "Grad:  tensor([-0.0208,  0.1180])\n",
      "------\n",
      "------\n",
      "Epoch 1901, Loss 2.969727, \n",
      "Params:  tensor([  5.2455, -16.6128])\n",
      "Grad:  tensor([-0.0208,  0.1178])\n",
      "------\n",
      "------\n",
      "Epoch 1902, Loss 2.969586, \n",
      "Params:  tensor([  5.2457, -16.6140])\n",
      "Grad:  tensor([-0.0208,  0.1175])\n",
      "------\n",
      "------\n",
      "Epoch 1903, Loss 2.969443, \n",
      "Params:  tensor([  5.2459, -16.6152])\n",
      "Grad:  tensor([-0.0207,  0.1173])\n",
      "------\n",
      "------\n",
      "Epoch 1904, Loss 2.969302, \n",
      "Params:  tensor([  5.2461, -16.6163])\n",
      "Grad:  tensor([-0.0207,  0.1172])\n",
      "------\n",
      "------\n",
      "Epoch 1905, Loss 2.969160, \n",
      "Params:  tensor([  5.2463, -16.6175])\n",
      "Grad:  tensor([-0.0206,  0.1170])\n",
      "------\n",
      "------\n",
      "Epoch 1906, Loss 2.969017, \n",
      "Params:  tensor([  5.2465, -16.6187])\n",
      "Grad:  tensor([-0.0206,  0.1168])\n",
      "------\n",
      "------\n",
      "Epoch 1907, Loss 2.968879, \n",
      "Params:  tensor([  5.2467, -16.6198])\n",
      "Grad:  tensor([-0.0206,  0.1166])\n",
      "------\n",
      "------\n",
      "Epoch 1908, Loss 2.968739, \n",
      "Params:  tensor([  5.2469, -16.6210])\n",
      "Grad:  tensor([-0.0205,  0.1164])\n",
      "------\n",
      "------\n",
      "Epoch 1909, Loss 2.968599, \n",
      "Params:  tensor([  5.2471, -16.6222])\n",
      "Grad:  tensor([-0.0205,  0.1162])\n",
      "------\n",
      "------\n",
      "Epoch 1910, Loss 2.968460, \n",
      "Params:  tensor([  5.2473, -16.6233])\n",
      "Grad:  tensor([-0.0205,  0.1160])\n",
      "------\n",
      "------\n",
      "Epoch 1911, Loss 2.968321, \n",
      "Params:  tensor([  5.2475, -16.6245])\n",
      "Grad:  tensor([-0.0204,  0.1158])\n",
      "------\n",
      "------\n",
      "Epoch 1912, Loss 2.968183, \n",
      "Params:  tensor([  5.2477, -16.6256])\n",
      "Grad:  tensor([-0.0204,  0.1156])\n",
      "------\n",
      "------\n",
      "Epoch 1913, Loss 2.968046, \n",
      "Params:  tensor([  5.2479, -16.6268])\n",
      "Grad:  tensor([-0.0204,  0.1154])\n",
      "------\n",
      "------\n",
      "Epoch 1914, Loss 2.967908, \n",
      "Params:  tensor([  5.2482, -16.6279])\n",
      "Grad:  tensor([-0.0204,  0.1152])\n",
      "------\n",
      "------\n",
      "Epoch 1915, Loss 2.967772, \n",
      "Params:  tensor([  5.2484, -16.6291])\n",
      "Grad:  tensor([-0.0203,  0.1150])\n",
      "------\n",
      "------\n",
      "Epoch 1916, Loss 2.967636, \n",
      "Params:  tensor([  5.2486, -16.6302])\n",
      "Grad:  tensor([-0.0203,  0.1148])\n",
      "------\n",
      "------\n",
      "Epoch 1917, Loss 2.967499, \n",
      "Params:  tensor([  5.2488, -16.6314])\n",
      "Grad:  tensor([-0.0202,  0.1146])\n",
      "------\n",
      "------\n",
      "Epoch 1918, Loss 2.967365, \n",
      "Params:  tensor([  5.2490, -16.6325])\n",
      "Grad:  tensor([-0.0202,  0.1144])\n",
      "------\n",
      "------\n",
      "Epoch 1919, Loss 2.967230, \n",
      "Params:  tensor([  5.2492, -16.6337])\n",
      "Grad:  tensor([-0.0202,  0.1142])\n",
      "------\n",
      "------\n",
      "Epoch 1920, Loss 2.967095, \n",
      "Params:  tensor([  5.2494, -16.6348])\n",
      "Grad:  tensor([-0.0202,  0.1140])\n",
      "------\n",
      "------\n",
      "Epoch 1921, Loss 2.966961, \n",
      "Params:  tensor([  5.2496, -16.6360])\n",
      "Grad:  tensor([-0.0201,  0.1138])\n",
      "------\n",
      "------\n",
      "Epoch 1922, Loss 2.966828, \n",
      "Params:  tensor([  5.2498, -16.6371])\n",
      "Grad:  tensor([-0.0201,  0.1136])\n",
      "------\n",
      "------\n",
      "Epoch 1923, Loss 2.966693, \n",
      "Params:  tensor([  5.2500, -16.6382])\n",
      "Grad:  tensor([-0.0200,  0.1134])\n",
      "------\n",
      "------\n",
      "Epoch 1924, Loss 2.966561, \n",
      "Params:  tensor([  5.2502, -16.6394])\n",
      "Grad:  tensor([-0.0200,  0.1132])\n",
      "------\n",
      "------\n",
      "Epoch 1925, Loss 2.966429, \n",
      "Params:  tensor([  5.2504, -16.6405])\n",
      "Grad:  tensor([-0.0200,  0.1130])\n",
      "------\n",
      "------\n",
      "Epoch 1926, Loss 2.966297, \n",
      "Params:  tensor([  5.2506, -16.6416])\n",
      "Grad:  tensor([-0.0199,  0.1128])\n",
      "------\n",
      "------\n",
      "Epoch 1927, Loss 2.966168, \n",
      "Params:  tensor([  5.2508, -16.6427])\n",
      "Grad:  tensor([-0.0199,  0.1127])\n",
      "------\n",
      "------\n",
      "Epoch 1928, Loss 2.966036, \n",
      "Params:  tensor([  5.2510, -16.6439])\n",
      "Grad:  tensor([-0.0199,  0.1125])\n",
      "------\n",
      "------\n",
      "Epoch 1929, Loss 2.965904, \n",
      "Params:  tensor([  5.2512, -16.6450])\n",
      "Grad:  tensor([-0.0198,  0.1123])\n",
      "------\n",
      "------\n",
      "Epoch 1930, Loss 2.965777, \n",
      "Params:  tensor([  5.2514, -16.6461])\n",
      "Grad:  tensor([-0.0198,  0.1121])\n",
      "------\n",
      "------\n",
      "Epoch 1931, Loss 2.965647, \n",
      "Params:  tensor([  5.2516, -16.6472])\n",
      "Grad:  tensor([-0.0198,  0.1119])\n",
      "------\n",
      "------\n",
      "Epoch 1932, Loss 2.965516, \n",
      "Params:  tensor([  5.2518, -16.6484])\n",
      "Grad:  tensor([-0.0197,  0.1117])\n",
      "------\n",
      "------\n",
      "Epoch 1933, Loss 2.965388, \n",
      "Params:  tensor([  5.2520, -16.6495])\n",
      "Grad:  tensor([-0.0197,  0.1115])\n",
      "------\n",
      "------\n",
      "Epoch 1934, Loss 2.965261, \n",
      "Params:  tensor([  5.2522, -16.6506])\n",
      "Grad:  tensor([-0.0197,  0.1113])\n",
      "------\n",
      "------\n",
      "Epoch 1935, Loss 2.965131, \n",
      "Params:  tensor([  5.2523, -16.6517])\n",
      "Grad:  tensor([-0.0196,  0.1111])\n",
      "------\n",
      "------\n",
      "Epoch 1936, Loss 2.965006, \n",
      "Params:  tensor([  5.2525, -16.6528])\n",
      "Grad:  tensor([-0.0196,  0.1109])\n",
      "------\n",
      "------\n",
      "Epoch 1937, Loss 2.964877, \n",
      "Params:  tensor([  5.2527, -16.6539])\n",
      "Grad:  tensor([-0.0196,  0.1108])\n",
      "------\n",
      "------\n",
      "Epoch 1938, Loss 2.964751, \n",
      "Params:  tensor([  5.2529, -16.6550])\n",
      "Grad:  tensor([-0.0195,  0.1106])\n",
      "------\n",
      "------\n",
      "Epoch 1939, Loss 2.964625, \n",
      "Params:  tensor([  5.2531, -16.6561])\n",
      "Grad:  tensor([-0.0195,  0.1104])\n",
      "------\n",
      "------\n",
      "Epoch 1940, Loss 2.964500, \n",
      "Params:  tensor([  5.2533, -16.6572])\n",
      "Grad:  tensor([-0.0195,  0.1102])\n",
      "------\n",
      "------\n",
      "Epoch 1941, Loss 2.964375, \n",
      "Params:  tensor([  5.2535, -16.6583])\n",
      "Grad:  tensor([-0.0195,  0.1100])\n",
      "------\n",
      "------\n",
      "Epoch 1942, Loss 2.964250, \n",
      "Params:  tensor([  5.2537, -16.6594])\n",
      "Grad:  tensor([-0.0194,  0.1098])\n",
      "------\n",
      "------\n",
      "Epoch 1943, Loss 2.964126, \n",
      "Params:  tensor([  5.2539, -16.6605])\n",
      "Grad:  tensor([-0.0194,  0.1096])\n",
      "------\n",
      "------\n",
      "Epoch 1944, Loss 2.964001, \n",
      "Params:  tensor([  5.2541, -16.6616])\n",
      "Grad:  tensor([-0.0194,  0.1094])\n",
      "------\n",
      "------\n",
      "Epoch 1945, Loss 2.963879, \n",
      "Params:  tensor([  5.2543, -16.6627])\n",
      "Grad:  tensor([-0.0193,  0.1093])\n",
      "------\n",
      "------\n",
      "Epoch 1946, Loss 2.963756, \n",
      "Params:  tensor([  5.2545, -16.6638])\n",
      "Grad:  tensor([-0.0193,  0.1091])\n",
      "------\n",
      "------\n",
      "Epoch 1947, Loss 2.963632, \n",
      "Params:  tensor([  5.2547, -16.6649])\n",
      "Grad:  tensor([-0.0192,  0.1089])\n",
      "------\n",
      "------\n",
      "Epoch 1948, Loss 2.963511, \n",
      "Params:  tensor([  5.2549, -16.6660])\n",
      "Grad:  tensor([-0.0192,  0.1087])\n",
      "------\n",
      "------\n",
      "Epoch 1949, Loss 2.963388, \n",
      "Params:  tensor([  5.2551, -16.6671])\n",
      "Grad:  tensor([-0.0192,  0.1085])\n",
      "------\n",
      "------\n",
      "Epoch 1950, Loss 2.963266, \n",
      "Params:  tensor([  5.2553, -16.6681])\n",
      "Grad:  tensor([-0.0191,  0.1083])\n",
      "------\n",
      "------\n",
      "Epoch 1951, Loss 2.963149, \n",
      "Params:  tensor([  5.2554, -16.6692])\n",
      "Grad:  tensor([-0.0191,  0.1081])\n",
      "------\n",
      "------\n",
      "Epoch 1952, Loss 2.963026, \n",
      "Params:  tensor([  5.2556, -16.6703])\n",
      "Grad:  tensor([-0.0191,  0.1080])\n",
      "------\n",
      "------\n",
      "Epoch 1953, Loss 2.962907, \n",
      "Params:  tensor([  5.2558, -16.6714])\n",
      "Grad:  tensor([-0.0190,  0.1078])\n",
      "------\n",
      "------\n",
      "Epoch 1954, Loss 2.962788, \n",
      "Params:  tensor([  5.2560, -16.6725])\n",
      "Grad:  tensor([-0.0190,  0.1076])\n",
      "------\n",
      "------\n",
      "Epoch 1955, Loss 2.962667, \n",
      "Params:  tensor([  5.2562, -16.6735])\n",
      "Grad:  tensor([-0.0190,  0.1074])\n",
      "------\n",
      "------\n",
      "Epoch 1956, Loss 2.962547, \n",
      "Params:  tensor([  5.2564, -16.6746])\n",
      "Grad:  tensor([-0.0189,  0.1072])\n",
      "------\n",
      "------\n",
      "Epoch 1957, Loss 2.962429, \n",
      "Params:  tensor([  5.2566, -16.6757])\n",
      "Grad:  tensor([-0.0189,  0.1071])\n",
      "------\n",
      "------\n",
      "Epoch 1958, Loss 2.962312, \n",
      "Params:  tensor([  5.2568, -16.6767])\n",
      "Grad:  tensor([-0.0189,  0.1069])\n",
      "------\n",
      "------\n",
      "Epoch 1959, Loss 2.962195, \n",
      "Params:  tensor([  5.2570, -16.6778])\n",
      "Grad:  tensor([-0.0188,  0.1067])\n",
      "------\n",
      "------\n",
      "Epoch 1960, Loss 2.962078, \n",
      "Params:  tensor([  5.2572, -16.6789])\n",
      "Grad:  tensor([-0.0188,  0.1065])\n",
      "------\n",
      "------\n",
      "Epoch 1961, Loss 2.961959, \n",
      "Params:  tensor([  5.2573, -16.6799])\n",
      "Grad:  tensor([-0.0188,  0.1063])\n",
      "------\n",
      "------\n",
      "Epoch 1962, Loss 2.961843, \n",
      "Params:  tensor([  5.2575, -16.6810])\n",
      "Grad:  tensor([-0.0187,  0.1062])\n",
      "------\n",
      "------\n",
      "Epoch 1963, Loss 2.961728, \n",
      "Params:  tensor([  5.2577, -16.6821])\n",
      "Grad:  tensor([-0.0187,  0.1060])\n",
      "------\n",
      "------\n",
      "Epoch 1964, Loss 2.961611, \n",
      "Params:  tensor([  5.2579, -16.6831])\n",
      "Grad:  tensor([-0.0187,  0.1058])\n",
      "------\n",
      "------\n",
      "Epoch 1965, Loss 2.961496, \n",
      "Params:  tensor([  5.2581, -16.6842])\n",
      "Grad:  tensor([-0.0187,  0.1056])\n",
      "------\n",
      "------\n",
      "Epoch 1966, Loss 2.961382, \n",
      "Params:  tensor([  5.2583, -16.6852])\n",
      "Grad:  tensor([-0.0186,  0.1054])\n",
      "------\n",
      "------\n",
      "Epoch 1967, Loss 2.961267, \n",
      "Params:  tensor([  5.2585, -16.6863])\n",
      "Grad:  tensor([-0.0186,  0.1052])\n",
      "------\n",
      "------\n",
      "Epoch 1968, Loss 2.961153, \n",
      "Params:  tensor([  5.2586, -16.6873])\n",
      "Grad:  tensor([-0.0186,  0.1051])\n",
      "------\n",
      "------\n",
      "Epoch 1969, Loss 2.961038, \n",
      "Params:  tensor([  5.2588, -16.6884])\n",
      "Grad:  tensor([-0.0185,  0.1049])\n",
      "------\n",
      "------\n",
      "Epoch 1970, Loss 2.960926, \n",
      "Params:  tensor([  5.2590, -16.6894])\n",
      "Grad:  tensor([-0.0185,  0.1047])\n",
      "------\n",
      "------\n",
      "Epoch 1971, Loss 2.960813, \n",
      "Params:  tensor([  5.2592, -16.6905])\n",
      "Grad:  tensor([-0.0185,  0.1045])\n",
      "------\n",
      "------\n",
      "Epoch 1972, Loss 2.960700, \n",
      "Params:  tensor([  5.2594, -16.6915])\n",
      "Grad:  tensor([-0.0184,  0.1044])\n",
      "------\n",
      "------\n",
      "Epoch 1973, Loss 2.960587, \n",
      "Params:  tensor([  5.2596, -16.6926])\n",
      "Grad:  tensor([-0.0184,  0.1042])\n",
      "------\n",
      "------\n",
      "Epoch 1974, Loss 2.960475, \n",
      "Params:  tensor([  5.2598, -16.6936])\n",
      "Grad:  tensor([-0.0184,  0.1040])\n",
      "------\n",
      "------\n",
      "Epoch 1975, Loss 2.960365, \n",
      "Params:  tensor([  5.2599, -16.6946])\n",
      "Grad:  tensor([-0.0183,  0.1038])\n",
      "------\n",
      "------\n",
      "Epoch 1976, Loss 2.960255, \n",
      "Params:  tensor([  5.2601, -16.6957])\n",
      "Grad:  tensor([-0.0183,  0.1037])\n",
      "------\n",
      "------\n",
      "Epoch 1977, Loss 2.960143, \n",
      "Params:  tensor([  5.2603, -16.6967])\n",
      "Grad:  tensor([-0.0183,  0.1035])\n",
      "------\n",
      "------\n",
      "Epoch 1978, Loss 2.960033, \n",
      "Params:  tensor([  5.2605, -16.6977])\n",
      "Grad:  tensor([-0.0182,  0.1033])\n",
      "------\n",
      "------\n",
      "Epoch 1979, Loss 2.959923, \n",
      "Params:  tensor([  5.2607, -16.6988])\n",
      "Grad:  tensor([-0.0182,  0.1031])\n",
      "------\n",
      "------\n",
      "Epoch 1980, Loss 2.959812, \n",
      "Params:  tensor([  5.2608, -16.6998])\n",
      "Grad:  tensor([-0.0182,  0.1029])\n",
      "------\n",
      "------\n",
      "Epoch 1981, Loss 2.959703, \n",
      "Params:  tensor([  5.2610, -16.7008])\n",
      "Grad:  tensor([-0.0182,  0.1028])\n",
      "------\n",
      "------\n",
      "Epoch 1982, Loss 2.959594, \n",
      "Params:  tensor([  5.2612, -16.7019])\n",
      "Grad:  tensor([-0.0181,  0.1026])\n",
      "------\n",
      "------\n",
      "Epoch 1983, Loss 2.959486, \n",
      "Params:  tensor([  5.2614, -16.7029])\n",
      "Grad:  tensor([-0.0181,  0.1024])\n",
      "------\n",
      "------\n",
      "Epoch 1984, Loss 2.959378, \n",
      "Params:  tensor([  5.2616, -16.7039])\n",
      "Grad:  tensor([-0.0181,  0.1022])\n",
      "------\n",
      "------\n",
      "Epoch 1985, Loss 2.959271, \n",
      "Params:  tensor([  5.2618, -16.7049])\n",
      "Grad:  tensor([-0.0180,  0.1021])\n",
      "------\n",
      "------\n",
      "Epoch 1986, Loss 2.959162, \n",
      "Params:  tensor([  5.2619, -16.7059])\n",
      "Grad:  tensor([-0.0180,  0.1019])\n",
      "------\n",
      "------\n",
      "Epoch 1987, Loss 2.959055, \n",
      "Params:  tensor([  5.2621, -16.7070])\n",
      "Grad:  tensor([-0.0180,  0.1017])\n",
      "------\n",
      "------\n",
      "Epoch 1988, Loss 2.958950, \n",
      "Params:  tensor([  5.2623, -16.7080])\n",
      "Grad:  tensor([-0.0179,  0.1016])\n",
      "------\n",
      "------\n",
      "Epoch 1989, Loss 2.958842, \n",
      "Params:  tensor([  5.2625, -16.7090])\n",
      "Grad:  tensor([-0.0179,  0.1014])\n",
      "------\n",
      "------\n",
      "Epoch 1990, Loss 2.958738, \n",
      "Params:  tensor([  5.2626, -16.7100])\n",
      "Grad:  tensor([-0.0179,  0.1012])\n",
      "------\n",
      "------\n",
      "Epoch 1991, Loss 2.958632, \n",
      "Params:  tensor([  5.2628, -16.7110])\n",
      "Grad:  tensor([-0.0179,  0.1010])\n",
      "------\n",
      "------\n",
      "Epoch 1992, Loss 2.958526, \n",
      "Params:  tensor([  5.2630, -16.7120])\n",
      "Grad:  tensor([-0.0178,  0.1009])\n",
      "------\n",
      "------\n",
      "Epoch 1993, Loss 2.958422, \n",
      "Params:  tensor([  5.2632, -16.7130])\n",
      "Grad:  tensor([-0.0178,  0.1007])\n",
      "------\n",
      "------\n",
      "Epoch 1994, Loss 2.958317, \n",
      "Params:  tensor([  5.2634, -16.7140])\n",
      "Grad:  tensor([-0.0178,  0.1005])\n",
      "------\n",
      "------\n",
      "Epoch 1995, Loss 2.958212, \n",
      "Params:  tensor([  5.2635, -16.7150])\n",
      "Grad:  tensor([-0.0177,  0.1004])\n",
      "------\n",
      "------\n",
      "Epoch 1996, Loss 2.958109, \n",
      "Params:  tensor([  5.2637, -16.7160])\n",
      "Grad:  tensor([-0.0177,  0.1002])\n",
      "------\n",
      "------\n",
      "Epoch 1997, Loss 2.958006, \n",
      "Params:  tensor([  5.2639, -16.7170])\n",
      "Grad:  tensor([-0.0176,  0.1000])\n",
      "------\n",
      "------\n",
      "Epoch 1998, Loss 2.957904, \n",
      "Params:  tensor([  5.2641, -16.7180])\n",
      "Grad:  tensor([-0.0176,  0.0998])\n",
      "------\n",
      "------\n",
      "Epoch 1999, Loss 2.957801, \n",
      "Params:  tensor([  5.2642, -16.7190])\n",
      "Grad:  tensor([-0.0176,  0.0997])\n",
      "------\n",
      "------\n",
      "Epoch 2000, Loss 2.957698, \n",
      "Params:  tensor([  5.2644, -16.7200])\n",
      "Grad:  tensor([-0.0176,  0.0995])\n",
      "------\n",
      "------\n",
      "Epoch 2001, Loss 2.957596, \n",
      "Params:  tensor([  5.2646, -16.7210])\n",
      "Grad:  tensor([-0.0176,  0.0993])\n",
      "------\n",
      "------\n",
      "Epoch 2002, Loss 2.957494, \n",
      "Params:  tensor([  5.2648, -16.7220])\n",
      "Grad:  tensor([-0.0175,  0.0992])\n",
      "------\n",
      "------\n",
      "Epoch 2003, Loss 2.957393, \n",
      "Params:  tensor([  5.2649, -16.7230])\n",
      "Grad:  tensor([-0.0175,  0.0990])\n",
      "------\n",
      "------\n",
      "Epoch 2004, Loss 2.957292, \n",
      "Params:  tensor([  5.2651, -16.7240])\n",
      "Grad:  tensor([-0.0174,  0.0988])\n",
      "------\n",
      "------\n",
      "Epoch 2005, Loss 2.957193, \n",
      "Params:  tensor([  5.2653, -16.7250])\n",
      "Grad:  tensor([-0.0174,  0.0987])\n",
      "------\n",
      "------\n",
      "Epoch 2006, Loss 2.957091, \n",
      "Params:  tensor([  5.2655, -16.7260])\n",
      "Grad:  tensor([-0.0174,  0.0985])\n",
      "------\n",
      "------\n",
      "Epoch 2007, Loss 2.956992, \n",
      "Params:  tensor([  5.2656, -16.7269])\n",
      "Grad:  tensor([-0.0174,  0.0983])\n",
      "------\n",
      "------\n",
      "Epoch 2008, Loss 2.956892, \n",
      "Params:  tensor([  5.2658, -16.7279])\n",
      "Grad:  tensor([-0.0173,  0.0982])\n",
      "------\n",
      "------\n",
      "Epoch 2009, Loss 2.956792, \n",
      "Params:  tensor([  5.2660, -16.7289])\n",
      "Grad:  tensor([-0.0173,  0.0980])\n",
      "------\n",
      "------\n",
      "Epoch 2010, Loss 2.956694, \n",
      "Params:  tensor([  5.2662, -16.7299])\n",
      "Grad:  tensor([-0.0173,  0.0978])\n",
      "------\n",
      "------\n",
      "Epoch 2011, Loss 2.956595, \n",
      "Params:  tensor([  5.2663, -16.7309])\n",
      "Grad:  tensor([-0.0172,  0.0977])\n",
      "------\n",
      "------\n",
      "Epoch 2012, Loss 2.956496, \n",
      "Params:  tensor([  5.2665, -16.7318])\n",
      "Grad:  tensor([-0.0172,  0.0975])\n",
      "------\n",
      "------\n",
      "Epoch 2013, Loss 2.956397, \n",
      "Params:  tensor([  5.2667, -16.7328])\n",
      "Grad:  tensor([-0.0172,  0.0973])\n",
      "------\n",
      "------\n",
      "Epoch 2014, Loss 2.956300, \n",
      "Params:  tensor([  5.2668, -16.7338])\n",
      "Grad:  tensor([-0.0172,  0.0972])\n",
      "------\n",
      "------\n",
      "Epoch 2015, Loss 2.956204, \n",
      "Params:  tensor([  5.2670, -16.7348])\n",
      "Grad:  tensor([-0.0171,  0.0970])\n",
      "------\n",
      "------\n",
      "Epoch 2016, Loss 2.956108, \n",
      "Params:  tensor([  5.2672, -16.7357])\n",
      "Grad:  tensor([-0.0171,  0.0968])\n",
      "------\n",
      "------\n",
      "Epoch 2017, Loss 2.956010, \n",
      "Params:  tensor([  5.2674, -16.7367])\n",
      "Grad:  tensor([-0.0171,  0.0967])\n",
      "------\n",
      "------\n",
      "Epoch 2018, Loss 2.955914, \n",
      "Params:  tensor([  5.2675, -16.7377])\n",
      "Grad:  tensor([-0.0171,  0.0965])\n",
      "------\n",
      "------\n",
      "Epoch 2019, Loss 2.955817, \n",
      "Params:  tensor([  5.2677, -16.7386])\n",
      "Grad:  tensor([-0.0170,  0.0963])\n",
      "------\n",
      "------\n",
      "Epoch 2020, Loss 2.955722, \n",
      "Params:  tensor([  5.2679, -16.7396])\n",
      "Grad:  tensor([-0.0170,  0.0962])\n",
      "------\n",
      "------\n",
      "Epoch 2021, Loss 2.955627, \n",
      "Params:  tensor([  5.2680, -16.7405])\n",
      "Grad:  tensor([-0.0170,  0.0960])\n",
      "------\n",
      "------\n",
      "Epoch 2022, Loss 2.955533, \n",
      "Params:  tensor([  5.2682, -16.7415])\n",
      "Grad:  tensor([-0.0169,  0.0959])\n",
      "------\n",
      "------\n",
      "Epoch 2023, Loss 2.955436, \n",
      "Params:  tensor([  5.2684, -16.7425])\n",
      "Grad:  tensor([-0.0169,  0.0957])\n",
      "------\n",
      "------\n",
      "Epoch 2024, Loss 2.955343, \n",
      "Params:  tensor([  5.2686, -16.7434])\n",
      "Grad:  tensor([-0.0169,  0.0955])\n",
      "------\n",
      "------\n",
      "Epoch 2025, Loss 2.955250, \n",
      "Params:  tensor([  5.2687, -16.7444])\n",
      "Grad:  tensor([-0.0169,  0.0954])\n",
      "------\n",
      "------\n",
      "Epoch 2026, Loss 2.955154, \n",
      "Params:  tensor([  5.2689, -16.7453])\n",
      "Grad:  tensor([-0.0168,  0.0952])\n",
      "------\n",
      "------\n",
      "Epoch 2027, Loss 2.955062, \n",
      "Params:  tensor([  5.2691, -16.7463])\n",
      "Grad:  tensor([-0.0168,  0.0950])\n",
      "------\n",
      "------\n",
      "Epoch 2028, Loss 2.954969, \n",
      "Params:  tensor([  5.2692, -16.7472])\n",
      "Grad:  tensor([-0.0168,  0.0949])\n",
      "------\n",
      "------\n",
      "Epoch 2029, Loss 2.954875, \n",
      "Params:  tensor([  5.2694, -16.7482])\n",
      "Grad:  tensor([-0.0167,  0.0947])\n",
      "------\n",
      "------\n",
      "Epoch 2030, Loss 2.954783, \n",
      "Params:  tensor([  5.2696, -16.7491])\n",
      "Grad:  tensor([-0.0167,  0.0946])\n",
      "------\n",
      "------\n",
      "Epoch 2031, Loss 2.954691, \n",
      "Params:  tensor([  5.2697, -16.7501])\n",
      "Grad:  tensor([-0.0167,  0.0944])\n",
      "------\n",
      "------\n",
      "Epoch 2032, Loss 2.954600, \n",
      "Params:  tensor([  5.2699, -16.7510])\n",
      "Grad:  tensor([-0.0167,  0.0942])\n",
      "------\n",
      "------\n",
      "Epoch 2033, Loss 2.954507, \n",
      "Params:  tensor([  5.2701, -16.7519])\n",
      "Grad:  tensor([-0.0166,  0.0941])\n",
      "------\n",
      "------\n",
      "Epoch 2034, Loss 2.954417, \n",
      "Params:  tensor([  5.2702, -16.7529])\n",
      "Grad:  tensor([-0.0166,  0.0939])\n",
      "------\n",
      "------\n",
      "Epoch 2035, Loss 2.954326, \n",
      "Params:  tensor([  5.2704, -16.7538])\n",
      "Grad:  tensor([-0.0165,  0.0938])\n",
      "------\n",
      "------\n",
      "Epoch 2036, Loss 2.954235, \n",
      "Params:  tensor([  5.2706, -16.7547])\n",
      "Grad:  tensor([-0.0165,  0.0936])\n",
      "------\n",
      "------\n",
      "Epoch 2037, Loss 2.954145, \n",
      "Params:  tensor([  5.2707, -16.7557])\n",
      "Grad:  tensor([-0.0165,  0.0934])\n",
      "------\n",
      "------\n",
      "Epoch 2038, Loss 2.954055, \n",
      "Params:  tensor([  5.2709, -16.7566])\n",
      "Grad:  tensor([-0.0165,  0.0933])\n",
      "------\n",
      "------\n",
      "Epoch 2039, Loss 2.953966, \n",
      "Params:  tensor([  5.2710, -16.7575])\n",
      "Grad:  tensor([-0.0164,  0.0931])\n",
      "------\n",
      "------\n",
      "Epoch 2040, Loss 2.953876, \n",
      "Params:  tensor([  5.2712, -16.7585])\n",
      "Grad:  tensor([-0.0164,  0.0930])\n",
      "------\n",
      "------\n",
      "Epoch 2041, Loss 2.953787, \n",
      "Params:  tensor([  5.2714, -16.7594])\n",
      "Grad:  tensor([-0.0164,  0.0928])\n",
      "------\n",
      "------\n",
      "Epoch 2042, Loss 2.953698, \n",
      "Params:  tensor([  5.2715, -16.7603])\n",
      "Grad:  tensor([-0.0164,  0.0926])\n",
      "------\n",
      "------\n",
      "Epoch 2043, Loss 2.953610, \n",
      "Params:  tensor([  5.2717, -16.7613])\n",
      "Grad:  tensor([-0.0163,  0.0925])\n",
      "------\n",
      "------\n",
      "Epoch 2044, Loss 2.953521, \n",
      "Params:  tensor([  5.2719, -16.7622])\n",
      "Grad:  tensor([-0.0163,  0.0923])\n",
      "------\n",
      "------\n",
      "Epoch 2045, Loss 2.953434, \n",
      "Params:  tensor([  5.2720, -16.7631])\n",
      "Grad:  tensor([-0.0163,  0.0922])\n",
      "------\n",
      "------\n",
      "Epoch 2046, Loss 2.953346, \n",
      "Params:  tensor([  5.2722, -16.7640])\n",
      "Grad:  tensor([-0.0163,  0.0920])\n",
      "------\n",
      "------\n",
      "Epoch 2047, Loss 2.953259, \n",
      "Params:  tensor([  5.2724, -16.7649])\n",
      "Grad:  tensor([-0.0162,  0.0919])\n",
      "------\n",
      "------\n",
      "Epoch 2048, Loss 2.953171, \n",
      "Params:  tensor([  5.2725, -16.7659])\n",
      "Grad:  tensor([-0.0162,  0.0917])\n",
      "------\n",
      "------\n",
      "Epoch 2049, Loss 2.953085, \n",
      "Params:  tensor([  5.2727, -16.7668])\n",
      "Grad:  tensor([-0.0162,  0.0915])\n",
      "------\n",
      "------\n",
      "Epoch 2050, Loss 2.953000, \n",
      "Params:  tensor([  5.2728, -16.7677])\n",
      "Grad:  tensor([-0.0162,  0.0914])\n",
      "------\n",
      "------\n",
      "Epoch 2051, Loss 2.952913, \n",
      "Params:  tensor([  5.2730, -16.7686])\n",
      "Grad:  tensor([-0.0161,  0.0912])\n",
      "------\n",
      "------\n",
      "Epoch 2052, Loss 2.952828, \n",
      "Params:  tensor([  5.2732, -16.7695])\n",
      "Grad:  tensor([-0.0161,  0.0911])\n",
      "------\n",
      "------\n",
      "Epoch 2053, Loss 2.952742, \n",
      "Params:  tensor([  5.2733, -16.7704])\n",
      "Grad:  tensor([-0.0161,  0.0909])\n",
      "------\n",
      "------\n",
      "Epoch 2054, Loss 2.952657, \n",
      "Params:  tensor([  5.2735, -16.7713])\n",
      "Grad:  tensor([-0.0160,  0.0908])\n",
      "------\n",
      "------\n",
      "Epoch 2055, Loss 2.952571, \n",
      "Params:  tensor([  5.2736, -16.7722])\n",
      "Grad:  tensor([-0.0160,  0.0906])\n",
      "------\n",
      "------\n",
      "Epoch 2056, Loss 2.952487, \n",
      "Params:  tensor([  5.2738, -16.7731])\n",
      "Grad:  tensor([-0.0160,  0.0905])\n",
      "------\n",
      "------\n",
      "Epoch 2057, Loss 2.952403, \n",
      "Params:  tensor([  5.2740, -16.7740])\n",
      "Grad:  tensor([-0.0160,  0.0903])\n",
      "------\n",
      "------\n",
      "Epoch 2058, Loss 2.952318, \n",
      "Params:  tensor([  5.2741, -16.7749])\n",
      "Grad:  tensor([-0.0159,  0.0902])\n",
      "------\n",
      "------\n",
      "Epoch 2059, Loss 2.952235, \n",
      "Params:  tensor([  5.2743, -16.7758])\n",
      "Grad:  tensor([-0.0159,  0.0900])\n",
      "------\n",
      "------\n",
      "Epoch 2060, Loss 2.952152, \n",
      "Params:  tensor([  5.2744, -16.7767])\n",
      "Grad:  tensor([-0.0159,  0.0899])\n",
      "------\n",
      "------\n",
      "Epoch 2061, Loss 2.952068, \n",
      "Params:  tensor([  5.2746, -16.7776])\n",
      "Grad:  tensor([-0.0158,  0.0897])\n",
      "------\n",
      "------\n",
      "Epoch 2062, Loss 2.951985, \n",
      "Params:  tensor([  5.2748, -16.7785])\n",
      "Grad:  tensor([-0.0158,  0.0895])\n",
      "------\n",
      "------\n",
      "Epoch 2063, Loss 2.951902, \n",
      "Params:  tensor([  5.2749, -16.7794])\n",
      "Grad:  tensor([-0.0158,  0.0894])\n",
      "------\n",
      "------\n",
      "Epoch 2064, Loss 2.951820, \n",
      "Params:  tensor([  5.2751, -16.7803])\n",
      "Grad:  tensor([-0.0158,  0.0892])\n",
      "------\n",
      "------\n",
      "Epoch 2065, Loss 2.951738, \n",
      "Params:  tensor([  5.2752, -16.7812])\n",
      "Grad:  tensor([-0.0157,  0.0891])\n",
      "------\n",
      "------\n",
      "Epoch 2066, Loss 2.951656, \n",
      "Params:  tensor([  5.2754, -16.7821])\n",
      "Grad:  tensor([-0.0157,  0.0889])\n",
      "------\n",
      "------\n",
      "Epoch 2067, Loss 2.951576, \n",
      "Params:  tensor([  5.2755, -16.7830])\n",
      "Grad:  tensor([-0.0157,  0.0888])\n",
      "------\n",
      "------\n",
      "Epoch 2068, Loss 2.951494, \n",
      "Params:  tensor([  5.2757, -16.7839])\n",
      "Grad:  tensor([-0.0157,  0.0886])\n",
      "------\n",
      "------\n",
      "Epoch 2069, Loss 2.951413, \n",
      "Params:  tensor([  5.2759, -16.7848])\n",
      "Grad:  tensor([-0.0157,  0.0885])\n",
      "------\n",
      "------\n",
      "Epoch 2070, Loss 2.951333, \n",
      "Params:  tensor([  5.2760, -16.7856])\n",
      "Grad:  tensor([-0.0156,  0.0883])\n",
      "------\n",
      "------\n",
      "Epoch 2071, Loss 2.951252, \n",
      "Params:  tensor([  5.2762, -16.7865])\n",
      "Grad:  tensor([-0.0156,  0.0882])\n",
      "------\n",
      "------\n",
      "Epoch 2072, Loss 2.951171, \n",
      "Params:  tensor([  5.2763, -16.7874])\n",
      "Grad:  tensor([-0.0155,  0.0880])\n",
      "------\n",
      "------\n",
      "Epoch 2073, Loss 2.951093, \n",
      "Params:  tensor([  5.2765, -16.7883])\n",
      "Grad:  tensor([-0.0155,  0.0879])\n",
      "------\n",
      "------\n",
      "Epoch 2074, Loss 2.951012, \n",
      "Params:  tensor([  5.2766, -16.7892])\n",
      "Grad:  tensor([-0.0155,  0.0877])\n",
      "------\n",
      "------\n",
      "Epoch 2075, Loss 2.950932, \n",
      "Params:  tensor([  5.2768, -16.7900])\n",
      "Grad:  tensor([-0.0155,  0.0876])\n",
      "------\n",
      "------\n",
      "Epoch 2076, Loss 2.950853, \n",
      "Params:  tensor([  5.2769, -16.7909])\n",
      "Grad:  tensor([-0.0154,  0.0874])\n",
      "------\n",
      "------\n",
      "Epoch 2077, Loss 2.950774, \n",
      "Params:  tensor([  5.2771, -16.7918])\n",
      "Grad:  tensor([-0.0154,  0.0873])\n",
      "------\n",
      "------\n",
      "Epoch 2078, Loss 2.950697, \n",
      "Params:  tensor([  5.2772, -16.7927])\n",
      "Grad:  tensor([-0.0154,  0.0871])\n",
      "------\n",
      "------\n",
      "Epoch 2079, Loss 2.950618, \n",
      "Params:  tensor([  5.2774, -16.7935])\n",
      "Grad:  tensor([-0.0154,  0.0870])\n",
      "------\n",
      "------\n",
      "Epoch 2080, Loss 2.950540, \n",
      "Params:  tensor([  5.2776, -16.7944])\n",
      "Grad:  tensor([-0.0154,  0.0868])\n",
      "------\n",
      "------\n",
      "Epoch 2081, Loss 2.950463, \n",
      "Params:  tensor([  5.2777, -16.7953])\n",
      "Grad:  tensor([-0.0153,  0.0867])\n",
      "------\n",
      "------\n",
      "Epoch 2082, Loss 2.950385, \n",
      "Params:  tensor([  5.2779, -16.7961])\n",
      "Grad:  tensor([-0.0153,  0.0866])\n",
      "------\n",
      "------\n",
      "Epoch 2083, Loss 2.950308, \n",
      "Params:  tensor([  5.2780, -16.7970])\n",
      "Grad:  tensor([-0.0153,  0.0864])\n",
      "------\n",
      "------\n",
      "Epoch 2084, Loss 2.950231, \n",
      "Params:  tensor([  5.2782, -16.7979])\n",
      "Grad:  tensor([-0.0152,  0.0863])\n",
      "------\n",
      "------\n",
      "Epoch 2085, Loss 2.950154, \n",
      "Params:  tensor([  5.2783, -16.7987])\n",
      "Grad:  tensor([-0.0152,  0.0861])\n",
      "------\n",
      "------\n",
      "Epoch 2086, Loss 2.950078, \n",
      "Params:  tensor([  5.2785, -16.7996])\n",
      "Grad:  tensor([-0.0152,  0.0860])\n",
      "------\n",
      "------\n",
      "Epoch 2087, Loss 2.950003, \n",
      "Params:  tensor([  5.2786, -16.8004])\n",
      "Grad:  tensor([-0.0152,  0.0858])\n",
      "------\n",
      "------\n",
      "Epoch 2088, Loss 2.949925, \n",
      "Params:  tensor([  5.2788, -16.8013])\n",
      "Grad:  tensor([-0.0152,  0.0857])\n",
      "------\n",
      "------\n",
      "Epoch 2089, Loss 2.949850, \n",
      "Params:  tensor([  5.2789, -16.8021])\n",
      "Grad:  tensor([-0.0151,  0.0855])\n",
      "------\n",
      "------\n",
      "Epoch 2090, Loss 2.949776, \n",
      "Params:  tensor([  5.2791, -16.8030])\n",
      "Grad:  tensor([-0.0151,  0.0854])\n",
      "------\n",
      "------\n",
      "Epoch 2091, Loss 2.949699, \n",
      "Params:  tensor([  5.2792, -16.8039])\n",
      "Grad:  tensor([-0.0151,  0.0852])\n",
      "------\n",
      "------\n",
      "Epoch 2092, Loss 2.949626, \n",
      "Params:  tensor([  5.2794, -16.8047])\n",
      "Grad:  tensor([-0.0150,  0.0851])\n",
      "------\n",
      "------\n",
      "Epoch 2093, Loss 2.949550, \n",
      "Params:  tensor([  5.2795, -16.8056])\n",
      "Grad:  tensor([-0.0150,  0.0850])\n",
      "------\n",
      "------\n",
      "Epoch 2094, Loss 2.949476, \n",
      "Params:  tensor([  5.2797, -16.8064])\n",
      "Grad:  tensor([-0.0150,  0.0848])\n",
      "------\n",
      "------\n",
      "Epoch 2095, Loss 2.949401, \n",
      "Params:  tensor([  5.2798, -16.8072])\n",
      "Grad:  tensor([-0.0149,  0.0847])\n",
      "------\n",
      "------\n",
      "Epoch 2096, Loss 2.949328, \n",
      "Params:  tensor([  5.2800, -16.8081])\n",
      "Grad:  tensor([-0.0150,  0.0845])\n",
      "------\n",
      "------\n",
      "Epoch 2097, Loss 2.949254, \n",
      "Params:  tensor([  5.2801, -16.8089])\n",
      "Grad:  tensor([-0.0149,  0.0844])\n",
      "------\n",
      "------\n",
      "Epoch 2098, Loss 2.949182, \n",
      "Params:  tensor([  5.2803, -16.8098])\n",
      "Grad:  tensor([-0.0149,  0.0842])\n",
      "------\n",
      "------\n",
      "Epoch 2099, Loss 2.949108, \n",
      "Params:  tensor([  5.2804, -16.8106])\n",
      "Grad:  tensor([-0.0149,  0.0841])\n",
      "------\n",
      "------\n",
      "Epoch 2100, Loss 2.949035, \n",
      "Params:  tensor([  5.2806, -16.8115])\n",
      "Grad:  tensor([-0.0148,  0.0839])\n",
      "------\n",
      "------\n",
      "Epoch 2101, Loss 2.948962, \n",
      "Params:  tensor([  5.2807, -16.8123])\n",
      "Grad:  tensor([-0.0148,  0.0838])\n",
      "------\n",
      "------\n",
      "Epoch 2102, Loss 2.948890, \n",
      "Params:  tensor([  5.2809, -16.8131])\n",
      "Grad:  tensor([-0.0148,  0.0837])\n",
      "------\n",
      "------\n",
      "Epoch 2103, Loss 2.948818, \n",
      "Params:  tensor([  5.2810, -16.8140])\n",
      "Grad:  tensor([-0.0148,  0.0835])\n",
      "------\n",
      "------\n",
      "Epoch 2104, Loss 2.948745, \n",
      "Params:  tensor([  5.2812, -16.8148])\n",
      "Grad:  tensor([-0.0148,  0.0834])\n",
      "------\n",
      "------\n",
      "Epoch 2105, Loss 2.948675, \n",
      "Params:  tensor([  5.2813, -16.8156])\n",
      "Grad:  tensor([-0.0147,  0.0832])\n",
      "------\n",
      "------\n",
      "Epoch 2106, Loss 2.948602, \n",
      "Params:  tensor([  5.2815, -16.8165])\n",
      "Grad:  tensor([-0.0147,  0.0831])\n",
      "------\n",
      "------\n",
      "Epoch 2107, Loss 2.948532, \n",
      "Params:  tensor([  5.2816, -16.8173])\n",
      "Grad:  tensor([-0.0146,  0.0830])\n",
      "------\n",
      "------\n",
      "Epoch 2108, Loss 2.948462, \n",
      "Params:  tensor([  5.2817, -16.8181])\n",
      "Grad:  tensor([-0.0146,  0.0828])\n",
      "------\n",
      "------\n",
      "Epoch 2109, Loss 2.948391, \n",
      "Params:  tensor([  5.2819, -16.8189])\n",
      "Grad:  tensor([-0.0146,  0.0827])\n",
      "------\n",
      "------\n",
      "Epoch 2110, Loss 2.948321, \n",
      "Params:  tensor([  5.2820, -16.8198])\n",
      "Grad:  tensor([-0.0146,  0.0825])\n",
      "------\n",
      "------\n",
      "Epoch 2111, Loss 2.948250, \n",
      "Params:  tensor([  5.2822, -16.8206])\n",
      "Grad:  tensor([-0.0145,  0.0824])\n",
      "------\n",
      "------\n",
      "Epoch 2112, Loss 2.948181, \n",
      "Params:  tensor([  5.2823, -16.8214])\n",
      "Grad:  tensor([-0.0145,  0.0823])\n",
      "------\n",
      "------\n",
      "Epoch 2113, Loss 2.948109, \n",
      "Params:  tensor([  5.2825, -16.8222])\n",
      "Grad:  tensor([-0.0145,  0.0821])\n",
      "------\n",
      "------\n",
      "Epoch 2114, Loss 2.948041, \n",
      "Params:  tensor([  5.2826, -16.8231])\n",
      "Grad:  tensor([-0.0145,  0.0820])\n",
      "------\n",
      "------\n",
      "Epoch 2115, Loss 2.947971, \n",
      "Params:  tensor([  5.2828, -16.8239])\n",
      "Grad:  tensor([-0.0144,  0.0818])\n",
      "------\n",
      "------\n",
      "Epoch 2116, Loss 2.947902, \n",
      "Params:  tensor([  5.2829, -16.8247])\n",
      "Grad:  tensor([-0.0144,  0.0817])\n",
      "------\n",
      "------\n",
      "Epoch 2117, Loss 2.947833, \n",
      "Params:  tensor([  5.2831, -16.8255])\n",
      "Grad:  tensor([-0.0144,  0.0816])\n",
      "------\n",
      "------\n",
      "Epoch 2118, Loss 2.947765, \n",
      "Params:  tensor([  5.2832, -16.8263])\n",
      "Grad:  tensor([-0.0144,  0.0814])\n",
      "------\n",
      "------\n",
      "Epoch 2119, Loss 2.947696, \n",
      "Params:  tensor([  5.2833, -16.8271])\n",
      "Grad:  tensor([-0.0144,  0.0813])\n",
      "------\n",
      "------\n",
      "Epoch 2120, Loss 2.947628, \n",
      "Params:  tensor([  5.2835, -16.8280])\n",
      "Grad:  tensor([-0.0143,  0.0811])\n",
      "------\n",
      "------\n",
      "Epoch 2121, Loss 2.947560, \n",
      "Params:  tensor([  5.2836, -16.8288])\n",
      "Grad:  tensor([-0.0143,  0.0810])\n",
      "------\n",
      "------\n",
      "Epoch 2122, Loss 2.947494, \n",
      "Params:  tensor([  5.2838, -16.8296])\n",
      "Grad:  tensor([-0.0143,  0.0809])\n",
      "------\n",
      "------\n",
      "Epoch 2123, Loss 2.947426, \n",
      "Params:  tensor([  5.2839, -16.8304])\n",
      "Grad:  tensor([-0.0143,  0.0807])\n",
      "------\n",
      "------\n",
      "Epoch 2124, Loss 2.947357, \n",
      "Params:  tensor([  5.2841, -16.8312])\n",
      "Grad:  tensor([-0.0142,  0.0806])\n",
      "------\n",
      "------\n",
      "Epoch 2125, Loss 2.947293, \n",
      "Params:  tensor([  5.2842, -16.8320])\n",
      "Grad:  tensor([-0.0142,  0.0805])\n",
      "------\n",
      "------\n",
      "Epoch 2126, Loss 2.947225, \n",
      "Params:  tensor([  5.2843, -16.8328])\n",
      "Grad:  tensor([-0.0142,  0.0803])\n",
      "------\n",
      "------\n",
      "Epoch 2127, Loss 2.947158, \n",
      "Params:  tensor([  5.2845, -16.8336])\n",
      "Grad:  tensor([-0.0142,  0.0802])\n",
      "------\n",
      "------\n",
      "Epoch 2128, Loss 2.947092, \n",
      "Params:  tensor([  5.2846, -16.8344])\n",
      "Grad:  tensor([-0.0141,  0.0800])\n",
      "------\n",
      "------\n",
      "Epoch 2129, Loss 2.947026, \n",
      "Params:  tensor([  5.2848, -16.8352])\n",
      "Grad:  tensor([-0.0141,  0.0799])\n",
      "------\n",
      "------\n",
      "Epoch 2130, Loss 2.946960, \n",
      "Params:  tensor([  5.2849, -16.8360])\n",
      "Grad:  tensor([-0.0141,  0.0798])\n",
      "------\n",
      "------\n",
      "Epoch 2131, Loss 2.946895, \n",
      "Params:  tensor([  5.2850, -16.8368])\n",
      "Grad:  tensor([-0.0141,  0.0796])\n",
      "------\n",
      "------\n",
      "Epoch 2132, Loss 2.946830, \n",
      "Params:  tensor([  5.2852, -16.8376])\n",
      "Grad:  tensor([-0.0141,  0.0795])\n",
      "------\n",
      "------\n",
      "Epoch 2133, Loss 2.946764, \n",
      "Params:  tensor([  5.2853, -16.8384])\n",
      "Grad:  tensor([-0.0140,  0.0794])\n",
      "------\n",
      "------\n",
      "Epoch 2134, Loss 2.946700, \n",
      "Params:  tensor([  5.2855, -16.8392])\n",
      "Grad:  tensor([-0.0140,  0.0792])\n",
      "------\n",
      "------\n",
      "Epoch 2135, Loss 2.946635, \n",
      "Params:  tensor([  5.2856, -16.8400])\n",
      "Grad:  tensor([-0.0140,  0.0791])\n",
      "------\n",
      "------\n",
      "Epoch 2136, Loss 2.946571, \n",
      "Params:  tensor([  5.2857, -16.8407])\n",
      "Grad:  tensor([-0.0139,  0.0790])\n",
      "------\n",
      "------\n",
      "Epoch 2137, Loss 2.946507, \n",
      "Params:  tensor([  5.2859, -16.8415])\n",
      "Grad:  tensor([-0.0139,  0.0788])\n",
      "------\n",
      "------\n",
      "Epoch 2138, Loss 2.946442, \n",
      "Params:  tensor([  5.2860, -16.8423])\n",
      "Grad:  tensor([-0.0139,  0.0787])\n",
      "------\n",
      "------\n",
      "Epoch 2139, Loss 2.946378, \n",
      "Params:  tensor([  5.2862, -16.8431])\n",
      "Grad:  tensor([-0.0139,  0.0786])\n",
      "------\n",
      "------\n",
      "Epoch 2140, Loss 2.946314, \n",
      "Params:  tensor([  5.2863, -16.8439])\n",
      "Grad:  tensor([-0.0138,  0.0784])\n",
      "------\n",
      "------\n",
      "Epoch 2141, Loss 2.946251, \n",
      "Params:  tensor([  5.2864, -16.8447])\n",
      "Grad:  tensor([-0.0138,  0.0783])\n",
      "------\n",
      "------\n",
      "Epoch 2142, Loss 2.946189, \n",
      "Params:  tensor([  5.2866, -16.8455])\n",
      "Grad:  tensor([-0.0138,  0.0782])\n",
      "------\n",
      "------\n",
      "Epoch 2143, Loss 2.946126, \n",
      "Params:  tensor([  5.2867, -16.8462])\n",
      "Grad:  tensor([-0.0138,  0.0780])\n",
      "------\n",
      "------\n",
      "Epoch 2144, Loss 2.946063, \n",
      "Params:  tensor([  5.2869, -16.8470])\n",
      "Grad:  tensor([-0.0138,  0.0779])\n",
      "------\n",
      "------\n",
      "Epoch 2145, Loss 2.946001, \n",
      "Params:  tensor([  5.2870, -16.8478])\n",
      "Grad:  tensor([-0.0137,  0.0778])\n",
      "------\n",
      "------\n",
      "Epoch 2146, Loss 2.945937, \n",
      "Params:  tensor([  5.2871, -16.8486])\n",
      "Grad:  tensor([-0.0137,  0.0776])\n",
      "------\n",
      "------\n",
      "Epoch 2147, Loss 2.945876, \n",
      "Params:  tensor([  5.2873, -16.8493])\n",
      "Grad:  tensor([-0.0137,  0.0775])\n",
      "------\n",
      "------\n",
      "Epoch 2148, Loss 2.945815, \n",
      "Params:  tensor([  5.2874, -16.8501])\n",
      "Grad:  tensor([-0.0137,  0.0774])\n",
      "------\n",
      "------\n",
      "Epoch 2149, Loss 2.945753, \n",
      "Params:  tensor([  5.2875, -16.8509])\n",
      "Grad:  tensor([-0.0136,  0.0772])\n",
      "------\n",
      "------\n",
      "Epoch 2150, Loss 2.945690, \n",
      "Params:  tensor([  5.2877, -16.8517])\n",
      "Grad:  tensor([-0.0136,  0.0771])\n",
      "------\n",
      "------\n",
      "Epoch 2151, Loss 2.945630, \n",
      "Params:  tensor([  5.2878, -16.8524])\n",
      "Grad:  tensor([-0.0136,  0.0770])\n",
      "------\n",
      "------\n",
      "Epoch 2152, Loss 2.945567, \n",
      "Params:  tensor([  5.2879, -16.8532])\n",
      "Grad:  tensor([-0.0136,  0.0768])\n",
      "------\n",
      "------\n",
      "Epoch 2153, Loss 2.945508, \n",
      "Params:  tensor([  5.2881, -16.8540])\n",
      "Grad:  tensor([-0.0135,  0.0767])\n",
      "------\n",
      "------\n",
      "Epoch 2154, Loss 2.945447, \n",
      "Params:  tensor([  5.2882, -16.8547])\n",
      "Grad:  tensor([-0.0135,  0.0766])\n",
      "------\n",
      "------\n",
      "Epoch 2155, Loss 2.945385, \n",
      "Params:  tensor([  5.2884, -16.8555])\n",
      "Grad:  tensor([-0.0135,  0.0765])\n",
      "------\n",
      "------\n",
      "Epoch 2156, Loss 2.945325, \n",
      "Params:  tensor([  5.2885, -16.8563])\n",
      "Grad:  tensor([-0.0135,  0.0763])\n",
      "------\n",
      "------\n",
      "Epoch 2157, Loss 2.945267, \n",
      "Params:  tensor([  5.2886, -16.8570])\n",
      "Grad:  tensor([-0.0135,  0.0762])\n",
      "------\n",
      "------\n",
      "Epoch 2158, Loss 2.945206, \n",
      "Params:  tensor([  5.2888, -16.8578])\n",
      "Grad:  tensor([-0.0134,  0.0761])\n",
      "------\n",
      "------\n",
      "Epoch 2159, Loss 2.945146, \n",
      "Params:  tensor([  5.2889, -16.8585])\n",
      "Grad:  tensor([-0.0134,  0.0759])\n",
      "------\n",
      "------\n",
      "Epoch 2160, Loss 2.945088, \n",
      "Params:  tensor([  5.2890, -16.8593])\n",
      "Grad:  tensor([-0.0134,  0.0758])\n",
      "------\n",
      "------\n",
      "Epoch 2161, Loss 2.945028, \n",
      "Params:  tensor([  5.2892, -16.8601])\n",
      "Grad:  tensor([-0.0134,  0.0757])\n",
      "------\n",
      "------\n",
      "Epoch 2162, Loss 2.944969, \n",
      "Params:  tensor([  5.2893, -16.8608])\n",
      "Grad:  tensor([-0.0133,  0.0755])\n",
      "------\n",
      "------\n",
      "Epoch 2163, Loss 2.944911, \n",
      "Params:  tensor([  5.2894, -16.8616])\n",
      "Grad:  tensor([-0.0133,  0.0754])\n",
      "------\n",
      "------\n",
      "Epoch 2164, Loss 2.944852, \n",
      "Params:  tensor([  5.2896, -16.8623])\n",
      "Grad:  tensor([-0.0133,  0.0753])\n",
      "------\n",
      "------\n",
      "Epoch 2165, Loss 2.944792, \n",
      "Params:  tensor([  5.2897, -16.8631])\n",
      "Grad:  tensor([-0.0133,  0.0752])\n",
      "------\n",
      "------\n",
      "Epoch 2166, Loss 2.944736, \n",
      "Params:  tensor([  5.2898, -16.8638])\n",
      "Grad:  tensor([-0.0133,  0.0750])\n",
      "------\n",
      "------\n",
      "Epoch 2167, Loss 2.944678, \n",
      "Params:  tensor([  5.2900, -16.8646])\n",
      "Grad:  tensor([-0.0132,  0.0749])\n",
      "------\n",
      "------\n",
      "Epoch 2168, Loss 2.944619, \n",
      "Params:  tensor([  5.2901, -16.8653])\n",
      "Grad:  tensor([-0.0132,  0.0748])\n",
      "------\n",
      "------\n",
      "Epoch 2169, Loss 2.944562, \n",
      "Params:  tensor([  5.2902, -16.8661])\n",
      "Grad:  tensor([-0.0132,  0.0747])\n",
      "------\n",
      "------\n",
      "Epoch 2170, Loss 2.944504, \n",
      "Params:  tensor([  5.2903, -16.8668])\n",
      "Grad:  tensor([-0.0132,  0.0745])\n",
      "------\n",
      "------\n",
      "Epoch 2171, Loss 2.944447, \n",
      "Params:  tensor([  5.2905, -16.8676])\n",
      "Grad:  tensor([-0.0132,  0.0744])\n",
      "------\n",
      "------\n",
      "Epoch 2172, Loss 2.944391, \n",
      "Params:  tensor([  5.2906, -16.8683])\n",
      "Grad:  tensor([-0.0131,  0.0743])\n",
      "------\n",
      "------\n",
      "Epoch 2173, Loss 2.944332, \n",
      "Params:  tensor([  5.2907, -16.8690])\n",
      "Grad:  tensor([-0.0131,  0.0742])\n",
      "------\n",
      "------\n",
      "Epoch 2174, Loss 2.944276, \n",
      "Params:  tensor([  5.2909, -16.8698])\n",
      "Grad:  tensor([-0.0131,  0.0740])\n",
      "------\n",
      "------\n",
      "Epoch 2175, Loss 2.944220, \n",
      "Params:  tensor([  5.2910, -16.8705])\n",
      "Grad:  tensor([-0.0131,  0.0739])\n",
      "------\n",
      "------\n",
      "Epoch 2176, Loss 2.944164, \n",
      "Params:  tensor([  5.2911, -16.8713])\n",
      "Grad:  tensor([-0.0130,  0.0738])\n",
      "------\n",
      "------\n",
      "Epoch 2177, Loss 2.944108, \n",
      "Params:  tensor([  5.2913, -16.8720])\n",
      "Grad:  tensor([-0.0130,  0.0736])\n",
      "------\n",
      "------\n",
      "Epoch 2178, Loss 2.944053, \n",
      "Params:  tensor([  5.2914, -16.8727])\n",
      "Grad:  tensor([-0.0130,  0.0735])\n",
      "------\n",
      "------\n",
      "Epoch 2179, Loss 2.943996, \n",
      "Params:  tensor([  5.2915, -16.8735])\n",
      "Grad:  tensor([-0.0130,  0.0734])\n",
      "------\n",
      "------\n",
      "Epoch 2180, Loss 2.943941, \n",
      "Params:  tensor([  5.2917, -16.8742])\n",
      "Grad:  tensor([-0.0129,  0.0733])\n",
      "------\n",
      "------\n",
      "Epoch 2181, Loss 2.943887, \n",
      "Params:  tensor([  5.2918, -16.8749])\n",
      "Grad:  tensor([-0.0129,  0.0731])\n",
      "------\n",
      "------\n",
      "Epoch 2182, Loss 2.943831, \n",
      "Params:  tensor([  5.2919, -16.8757])\n",
      "Grad:  tensor([-0.0129,  0.0730])\n",
      "------\n",
      "------\n",
      "Epoch 2183, Loss 2.943776, \n",
      "Params:  tensor([  5.2920, -16.8764])\n",
      "Grad:  tensor([-0.0129,  0.0729])\n",
      "------\n",
      "------\n",
      "Epoch 2184, Loss 2.943721, \n",
      "Params:  tensor([  5.2922, -16.8771])\n",
      "Grad:  tensor([-0.0129,  0.0728])\n",
      "------\n",
      "------\n",
      "Epoch 2185, Loss 2.943666, \n",
      "Params:  tensor([  5.2923, -16.8778])\n",
      "Grad:  tensor([-0.0128,  0.0727])\n",
      "------\n",
      "------\n",
      "Epoch 2186, Loss 2.943613, \n",
      "Params:  tensor([  5.2924, -16.8786])\n",
      "Grad:  tensor([-0.0128,  0.0725])\n",
      "------\n",
      "------\n",
      "Epoch 2187, Loss 2.943558, \n",
      "Params:  tensor([  5.2926, -16.8793])\n",
      "Grad:  tensor([-0.0128,  0.0724])\n",
      "------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Epoch 2188, Loss 2.943503, \n",
      "Params:  tensor([  5.2927, -16.8800])\n",
      "Grad:  tensor([-0.0128,  0.0723])\n",
      "------\n",
      "------\n",
      "Epoch 2189, Loss 2.943451, \n",
      "Params:  tensor([  5.2928, -16.8807])\n",
      "Grad:  tensor([-0.0127,  0.0722])\n",
      "------\n",
      "------\n",
      "Epoch 2190, Loss 2.943395, \n",
      "Params:  tensor([  5.2929, -16.8815])\n",
      "Grad:  tensor([-0.0127,  0.0720])\n",
      "------\n",
      "------\n",
      "Epoch 2191, Loss 2.943343, \n",
      "Params:  tensor([  5.2931, -16.8822])\n",
      "Grad:  tensor([-0.0127,  0.0719])\n",
      "------\n",
      "------\n",
      "Epoch 2192, Loss 2.943290, \n",
      "Params:  tensor([  5.2932, -16.8829])\n",
      "Grad:  tensor([-0.0127,  0.0718])\n",
      "------\n",
      "------\n",
      "Epoch 2193, Loss 2.943235, \n",
      "Params:  tensor([  5.2933, -16.8836])\n",
      "Grad:  tensor([-0.0127,  0.0717])\n",
      "------\n",
      "------\n",
      "Epoch 2194, Loss 2.943183, \n",
      "Params:  tensor([  5.2934, -16.8843])\n",
      "Grad:  tensor([-0.0126,  0.0715])\n",
      "------\n",
      "------\n",
      "Epoch 2195, Loss 2.943130, \n",
      "Params:  tensor([  5.2936, -16.8850])\n",
      "Grad:  tensor([-0.0126,  0.0714])\n",
      "------\n",
      "------\n",
      "Epoch 2196, Loss 2.943079, \n",
      "Params:  tensor([  5.2937, -16.8857])\n",
      "Grad:  tensor([-0.0126,  0.0713])\n",
      "------\n",
      "------\n",
      "Epoch 2197, Loss 2.943027, \n",
      "Params:  tensor([  5.2938, -16.8865])\n",
      "Grad:  tensor([-0.0126,  0.0712])\n",
      "------\n",
      "------\n",
      "Epoch 2198, Loss 2.942973, \n",
      "Params:  tensor([  5.2939, -16.8872])\n",
      "Grad:  tensor([-0.0126,  0.0711])\n",
      "------\n",
      "------\n",
      "Epoch 2199, Loss 2.942922, \n",
      "Params:  tensor([  5.2941, -16.8879])\n",
      "Grad:  tensor([-0.0125,  0.0709])\n",
      "------\n",
      "------\n",
      "Epoch 2200, Loss 2.942870, \n",
      "Params:  tensor([  5.2942, -16.8886])\n",
      "Grad:  tensor([-0.0125,  0.0708])\n",
      "------\n",
      "------\n",
      "Epoch 2201, Loss 2.942818, \n",
      "Params:  tensor([  5.2943, -16.8893])\n",
      "Grad:  tensor([-0.0125,  0.0707])\n",
      "------\n",
      "------\n",
      "Epoch 2202, Loss 2.942766, \n",
      "Params:  tensor([  5.2944, -16.8900])\n",
      "Grad:  tensor([-0.0125,  0.0706])\n",
      "------\n",
      "------\n",
      "Epoch 2203, Loss 2.942714, \n",
      "Params:  tensor([  5.2946, -16.8907])\n",
      "Grad:  tensor([-0.0124,  0.0705])\n",
      "------\n",
      "------\n",
      "Epoch 2204, Loss 2.942665, \n",
      "Params:  tensor([  5.2947, -16.8914])\n",
      "Grad:  tensor([-0.0124,  0.0703])\n",
      "------\n",
      "------\n",
      "Epoch 2205, Loss 2.942612, \n",
      "Params:  tensor([  5.2948, -16.8921])\n",
      "Grad:  tensor([-0.0124,  0.0702])\n",
      "------\n",
      "------\n",
      "Epoch 2206, Loss 2.942564, \n",
      "Params:  tensor([  5.2949, -16.8928])\n",
      "Grad:  tensor([-0.0124,  0.0701])\n",
      "------\n",
      "------\n",
      "Epoch 2207, Loss 2.942510, \n",
      "Params:  tensor([  5.2951, -16.8935])\n",
      "Grad:  tensor([-0.0124,  0.0700])\n",
      "------\n",
      "------\n",
      "Epoch 2208, Loss 2.942461, \n",
      "Params:  tensor([  5.2952, -16.8942])\n",
      "Grad:  tensor([-0.0123,  0.0699])\n",
      "------\n",
      "------\n",
      "Epoch 2209, Loss 2.942411, \n",
      "Params:  tensor([  5.2953, -16.8949])\n",
      "Grad:  tensor([-0.0123,  0.0697])\n",
      "------\n",
      "------\n",
      "Epoch 2210, Loss 2.942361, \n",
      "Params:  tensor([  5.2954, -16.8956])\n",
      "Grad:  tensor([-0.0123,  0.0696])\n",
      "------\n",
      "------\n",
      "Epoch 2211, Loss 2.942310, \n",
      "Params:  tensor([  5.2956, -16.8963])\n",
      "Grad:  tensor([-0.0123,  0.0695])\n",
      "------\n",
      "------\n",
      "Epoch 2212, Loss 2.942261, \n",
      "Params:  tensor([  5.2957, -16.8970])\n",
      "Grad:  tensor([-0.0122,  0.0694])\n",
      "------\n",
      "------\n",
      "Epoch 2213, Loss 2.942211, \n",
      "Params:  tensor([  5.2958, -16.8977])\n",
      "Grad:  tensor([-0.0122,  0.0693])\n",
      "------\n",
      "------\n",
      "Epoch 2214, Loss 2.942162, \n",
      "Params:  tensor([  5.2959, -16.8984])\n",
      "Grad:  tensor([-0.0122,  0.0692])\n",
      "------\n",
      "------\n",
      "Epoch 2215, Loss 2.942112, \n",
      "Params:  tensor([  5.2960, -16.8991])\n",
      "Grad:  tensor([-0.0122,  0.0690])\n",
      "------\n",
      "------\n",
      "Epoch 2216, Loss 2.942062, \n",
      "Params:  tensor([  5.2962, -16.8998])\n",
      "Grad:  tensor([-0.0122,  0.0689])\n",
      "------\n",
      "------\n",
      "Epoch 2217, Loss 2.942014, \n",
      "Params:  tensor([  5.2963, -16.9004])\n",
      "Grad:  tensor([-0.0122,  0.0688])\n",
      "------\n",
      "------\n",
      "Epoch 2218, Loss 2.941965, \n",
      "Params:  tensor([  5.2964, -16.9011])\n",
      "Grad:  tensor([-0.0121,  0.0687])\n",
      "------\n",
      "------\n",
      "Epoch 2219, Loss 2.941918, \n",
      "Params:  tensor([  5.2965, -16.9018])\n",
      "Grad:  tensor([-0.0121,  0.0686])\n",
      "------\n",
      "------\n",
      "Epoch 2220, Loss 2.941868, \n",
      "Params:  tensor([  5.2967, -16.9025])\n",
      "Grad:  tensor([-0.0121,  0.0685])\n",
      "------\n",
      "------\n",
      "Epoch 2221, Loss 2.941821, \n",
      "Params:  tensor([  5.2968, -16.9032])\n",
      "Grad:  tensor([-0.0121,  0.0683])\n",
      "------\n",
      "------\n",
      "Epoch 2222, Loss 2.941773, \n",
      "Params:  tensor([  5.2969, -16.9039])\n",
      "Grad:  tensor([-0.0120,  0.0682])\n",
      "------\n",
      "------\n",
      "Epoch 2223, Loss 2.941724, \n",
      "Params:  tensor([  5.2970, -16.9046])\n",
      "Grad:  tensor([-0.0120,  0.0681])\n",
      "------\n",
      "------\n",
      "Epoch 2224, Loss 2.941677, \n",
      "Params:  tensor([  5.2971, -16.9052])\n",
      "Grad:  tensor([-0.0120,  0.0680])\n",
      "------\n",
      "------\n",
      "Epoch 2225, Loss 2.941629, \n",
      "Params:  tensor([  5.2973, -16.9059])\n",
      "Grad:  tensor([-0.0120,  0.0679])\n",
      "------\n",
      "------\n",
      "Epoch 2226, Loss 2.941582, \n",
      "Params:  tensor([  5.2974, -16.9066])\n",
      "Grad:  tensor([-0.0120,  0.0678])\n",
      "------\n",
      "------\n",
      "Epoch 2227, Loss 2.941534, \n",
      "Params:  tensor([  5.2975, -16.9073])\n",
      "Grad:  tensor([-0.0119,  0.0676])\n",
      "------\n",
      "------\n",
      "Epoch 2228, Loss 2.941488, \n",
      "Params:  tensor([  5.2976, -16.9079])\n",
      "Grad:  tensor([-0.0119,  0.0675])\n",
      "------\n",
      "------\n",
      "Epoch 2229, Loss 2.941440, \n",
      "Params:  tensor([  5.2977, -16.9086])\n",
      "Grad:  tensor([-0.0119,  0.0674])\n",
      "------\n",
      "------\n",
      "Epoch 2230, Loss 2.941393, \n",
      "Params:  tensor([  5.2979, -16.9093])\n",
      "Grad:  tensor([-0.0119,  0.0673])\n",
      "------\n",
      "------\n",
      "Epoch 2231, Loss 2.941346, \n",
      "Params:  tensor([  5.2980, -16.9100])\n",
      "Grad:  tensor([-0.0119,  0.0672])\n",
      "------\n",
      "------\n",
      "Epoch 2232, Loss 2.941299, \n",
      "Params:  tensor([  5.2981, -16.9106])\n",
      "Grad:  tensor([-0.0118,  0.0671])\n",
      "------\n",
      "------\n",
      "Epoch 2233, Loss 2.941253, \n",
      "Params:  tensor([  5.2982, -16.9113])\n",
      "Grad:  tensor([-0.0118,  0.0670])\n",
      "------\n",
      "------\n",
      "Epoch 2234, Loss 2.941206, \n",
      "Params:  tensor([  5.2983, -16.9120])\n",
      "Grad:  tensor([-0.0118,  0.0668])\n",
      "------\n",
      "------\n",
      "Epoch 2235, Loss 2.941163, \n",
      "Params:  tensor([  5.2984, -16.9126])\n",
      "Grad:  tensor([-0.0118,  0.0667])\n",
      "------\n",
      "------\n",
      "Epoch 2236, Loss 2.941116, \n",
      "Params:  tensor([  5.2986, -16.9133])\n",
      "Grad:  tensor([-0.0118,  0.0666])\n",
      "------\n",
      "------\n",
      "Epoch 2237, Loss 2.941070, \n",
      "Params:  tensor([  5.2987, -16.9140])\n",
      "Grad:  tensor([-0.0117,  0.0665])\n",
      "------\n",
      "------\n",
      "Epoch 2238, Loss 2.941025, \n",
      "Params:  tensor([  5.2988, -16.9146])\n",
      "Grad:  tensor([-0.0117,  0.0664])\n",
      "------\n",
      "------\n",
      "Epoch 2239, Loss 2.940979, \n",
      "Params:  tensor([  5.2989, -16.9153])\n",
      "Grad:  tensor([-0.0117,  0.0663])\n",
      "------\n",
      "------\n",
      "Epoch 2240, Loss 2.940933, \n",
      "Params:  tensor([  5.2990, -16.9160])\n",
      "Grad:  tensor([-0.0117,  0.0662])\n",
      "------\n",
      "------\n",
      "Epoch 2241, Loss 2.940890, \n",
      "Params:  tensor([  5.2991, -16.9166])\n",
      "Grad:  tensor([-0.0117,  0.0661])\n",
      "------\n",
      "------\n",
      "Epoch 2242, Loss 2.940844, \n",
      "Params:  tensor([  5.2993, -16.9173])\n",
      "Grad:  tensor([-0.0117,  0.0659])\n",
      "------\n",
      "------\n",
      "Epoch 2243, Loss 2.940798, \n",
      "Params:  tensor([  5.2994, -16.9179])\n",
      "Grad:  tensor([-0.0116,  0.0658])\n",
      "------\n",
      "------\n",
      "Epoch 2244, Loss 2.940753, \n",
      "Params:  tensor([  5.2995, -16.9186])\n",
      "Grad:  tensor([-0.0116,  0.0657])\n",
      "------\n",
      "------\n",
      "Epoch 2245, Loss 2.940711, \n",
      "Params:  tensor([  5.2996, -16.9192])\n",
      "Grad:  tensor([-0.0116,  0.0656])\n",
      "------\n",
      "------\n",
      "Epoch 2246, Loss 2.940666, \n",
      "Params:  tensor([  5.2997, -16.9199])\n",
      "Grad:  tensor([-0.0116,  0.0655])\n",
      "------\n",
      "------\n",
      "Epoch 2247, Loss 2.940621, \n",
      "Params:  tensor([  5.2998, -16.9206])\n",
      "Grad:  tensor([-0.0115,  0.0654])\n",
      "------\n",
      "------\n",
      "Epoch 2248, Loss 2.940576, \n",
      "Params:  tensor([  5.3000, -16.9212])\n",
      "Grad:  tensor([-0.0115,  0.0653])\n",
      "------\n",
      "------\n",
      "Epoch 2249, Loss 2.940533, \n",
      "Params:  tensor([  5.3001, -16.9219])\n",
      "Grad:  tensor([-0.0115,  0.0652])\n",
      "------\n",
      "------\n",
      "Epoch 2250, Loss 2.940489, \n",
      "Params:  tensor([  5.3002, -16.9225])\n",
      "Grad:  tensor([-0.0115,  0.0650])\n",
      "------\n",
      "------\n",
      "Epoch 2251, Loss 2.940446, \n",
      "Params:  tensor([  5.3003, -16.9232])\n",
      "Grad:  tensor([-0.0115,  0.0649])\n",
      "------\n",
      "------\n",
      "Epoch 2252, Loss 2.940403, \n",
      "Params:  tensor([  5.3004, -16.9238])\n",
      "Grad:  tensor([-0.0114,  0.0648])\n",
      "------\n",
      "------\n",
      "Epoch 2253, Loss 2.940358, \n",
      "Params:  tensor([  5.3005, -16.9245])\n",
      "Grad:  tensor([-0.0114,  0.0647])\n",
      "------\n",
      "------\n",
      "Epoch 2254, Loss 2.940316, \n",
      "Params:  tensor([  5.3006, -16.9251])\n",
      "Grad:  tensor([-0.0114,  0.0646])\n",
      "------\n",
      "------\n",
      "Epoch 2255, Loss 2.940274, \n",
      "Params:  tensor([  5.3008, -16.9257])\n",
      "Grad:  tensor([-0.0114,  0.0645])\n",
      "------\n",
      "------\n",
      "Epoch 2256, Loss 2.940229, \n",
      "Params:  tensor([  5.3009, -16.9264])\n",
      "Grad:  tensor([-0.0114,  0.0644])\n",
      "------\n",
      "------\n",
      "Epoch 2257, Loss 2.940188, \n",
      "Params:  tensor([  5.3010, -16.9270])\n",
      "Grad:  tensor([-0.0114,  0.0643])\n",
      "------\n",
      "------\n",
      "Epoch 2258, Loss 2.940144, \n",
      "Params:  tensor([  5.3011, -16.9277])\n",
      "Grad:  tensor([-0.0114,  0.0642])\n",
      "------\n",
      "------\n",
      "Epoch 2259, Loss 2.940102, \n",
      "Params:  tensor([  5.3012, -16.9283])\n",
      "Grad:  tensor([-0.0113,  0.0641])\n",
      "------\n",
      "------\n",
      "Epoch 2260, Loss 2.940060, \n",
      "Params:  tensor([  5.3013, -16.9290])\n",
      "Grad:  tensor([-0.0113,  0.0640])\n",
      "------\n",
      "------\n",
      "Epoch 2261, Loss 2.940018, \n",
      "Params:  tensor([  5.3014, -16.9296])\n",
      "Grad:  tensor([-0.0113,  0.0638])\n",
      "------\n",
      "------\n",
      "Epoch 2262, Loss 2.939977, \n",
      "Params:  tensor([  5.3016, -16.9302])\n",
      "Grad:  tensor([-0.0113,  0.0637])\n",
      "------\n",
      "------\n",
      "Epoch 2263, Loss 2.939934, \n",
      "Params:  tensor([  5.3017, -16.9309])\n",
      "Grad:  tensor([-0.0112,  0.0636])\n",
      "------\n",
      "------\n",
      "Epoch 2264, Loss 2.939891, \n",
      "Params:  tensor([  5.3018, -16.9315])\n",
      "Grad:  tensor([-0.0112,  0.0635])\n",
      "------\n",
      "------\n",
      "Epoch 2265, Loss 2.939851, \n",
      "Params:  tensor([  5.3019, -16.9321])\n",
      "Grad:  tensor([-0.0112,  0.0634])\n",
      "------\n",
      "------\n",
      "Epoch 2266, Loss 2.939809, \n",
      "Params:  tensor([  5.3020, -16.9328])\n",
      "Grad:  tensor([-0.0112,  0.0633])\n",
      "------\n",
      "------\n",
      "Epoch 2267, Loss 2.939770, \n",
      "Params:  tensor([  5.3021, -16.9334])\n",
      "Grad:  tensor([-0.0112,  0.0632])\n",
      "------\n",
      "------\n",
      "Epoch 2268, Loss 2.939727, \n",
      "Params:  tensor([  5.3022, -16.9340])\n",
      "Grad:  tensor([-0.0111,  0.0631])\n",
      "------\n",
      "------\n",
      "Epoch 2269, Loss 2.939686, \n",
      "Params:  tensor([  5.3023, -16.9347])\n",
      "Grad:  tensor([-0.0111,  0.0630])\n",
      "------\n",
      "------\n",
      "Epoch 2270, Loss 2.939646, \n",
      "Params:  tensor([  5.3024, -16.9353])\n",
      "Grad:  tensor([-0.0111,  0.0629])\n",
      "------\n",
      "------\n",
      "Epoch 2271, Loss 2.939605, \n",
      "Params:  tensor([  5.3026, -16.9359])\n",
      "Grad:  tensor([-0.0111,  0.0628])\n",
      "------\n",
      "------\n",
      "Epoch 2272, Loss 2.939566, \n",
      "Params:  tensor([  5.3027, -16.9365])\n",
      "Grad:  tensor([-0.0111,  0.0627])\n",
      "------\n",
      "------\n",
      "Epoch 2273, Loss 2.939522, \n",
      "Params:  tensor([  5.3028, -16.9372])\n",
      "Grad:  tensor([-0.0111,  0.0626])\n",
      "------\n",
      "------\n",
      "Epoch 2274, Loss 2.939483, \n",
      "Params:  tensor([  5.3029, -16.9378])\n",
      "Grad:  tensor([-0.0110,  0.0624])\n",
      "------\n",
      "------\n",
      "Epoch 2275, Loss 2.939443, \n",
      "Params:  tensor([  5.3030, -16.9384])\n",
      "Grad:  tensor([-0.0110,  0.0623])\n",
      "------\n",
      "------\n",
      "Epoch 2276, Loss 2.939403, \n",
      "Params:  tensor([  5.3031, -16.9390])\n",
      "Grad:  tensor([-0.0110,  0.0622])\n",
      "------\n",
      "------\n",
      "Epoch 2277, Loss 2.939361, \n",
      "Params:  tensor([  5.3032, -16.9397])\n",
      "Grad:  tensor([-0.0110,  0.0621])\n",
      "------\n",
      "------\n",
      "Epoch 2278, Loss 2.939323, \n",
      "Params:  tensor([  5.3033, -16.9403])\n",
      "Grad:  tensor([-0.0110,  0.0620])\n",
      "------\n",
      "------\n",
      "Epoch 2279, Loss 2.939282, \n",
      "Params:  tensor([  5.3034, -16.9409])\n",
      "Grad:  tensor([-0.0109,  0.0619])\n",
      "------\n",
      "------\n",
      "Epoch 2280, Loss 2.939243, \n",
      "Params:  tensor([  5.3035, -16.9415])\n",
      "Grad:  tensor([-0.0109,  0.0618])\n",
      "------\n",
      "------\n",
      "Epoch 2281, Loss 2.939205, \n",
      "Params:  tensor([  5.3037, -16.9421])\n",
      "Grad:  tensor([-0.0109,  0.0617])\n",
      "------\n",
      "------\n",
      "Epoch 2282, Loss 2.939165, \n",
      "Params:  tensor([  5.3038, -16.9428])\n",
      "Grad:  tensor([-0.0109,  0.0616])\n",
      "------\n",
      "------\n",
      "Epoch 2283, Loss 2.939127, \n",
      "Params:  tensor([  5.3039, -16.9434])\n",
      "Grad:  tensor([-0.0109,  0.0615])\n",
      "------\n",
      "------\n",
      "Epoch 2284, Loss 2.939087, \n",
      "Params:  tensor([  5.3040, -16.9440])\n",
      "Grad:  tensor([-0.0108,  0.0614])\n",
      "------\n",
      "------\n",
      "Epoch 2285, Loss 2.939049, \n",
      "Params:  tensor([  5.3041, -16.9446])\n",
      "Grad:  tensor([-0.0108,  0.0613])\n",
      "------\n",
      "------\n",
      "Epoch 2286, Loss 2.939011, \n",
      "Params:  tensor([  5.3042, -16.9452])\n",
      "Grad:  tensor([-0.0108,  0.0612])\n",
      "------\n",
      "------\n",
      "Epoch 2287, Loss 2.938971, \n",
      "Params:  tensor([  5.3043, -16.9458])\n",
      "Grad:  tensor([-0.0108,  0.0611])\n",
      "------\n",
      "------\n",
      "Epoch 2288, Loss 2.938933, \n",
      "Params:  tensor([  5.3044, -16.9464])\n",
      "Grad:  tensor([-0.0108,  0.0610])\n",
      "------\n",
      "------\n",
      "Epoch 2289, Loss 2.938893, \n",
      "Params:  tensor([  5.3045, -16.9470])\n",
      "Grad:  tensor([-0.0108,  0.0609])\n",
      "------\n",
      "------\n",
      "Epoch 2290, Loss 2.938857, \n",
      "Params:  tensor([  5.3046, -16.9476])\n",
      "Grad:  tensor([-0.0107,  0.0608])\n",
      "------\n",
      "------\n",
      "Epoch 2291, Loss 2.938820, \n",
      "Params:  tensor([  5.3047, -16.9482])\n",
      "Grad:  tensor([-0.0107,  0.0607])\n",
      "------\n",
      "------\n",
      "Epoch 2292, Loss 2.938779, \n",
      "Params:  tensor([  5.3048, -16.9489])\n",
      "Grad:  tensor([-0.0107,  0.0606])\n",
      "------\n",
      "------\n",
      "Epoch 2293, Loss 2.938743, \n",
      "Params:  tensor([  5.3049, -16.9495])\n",
      "Grad:  tensor([-0.0107,  0.0605])\n",
      "------\n",
      "------\n",
      "Epoch 2294, Loss 2.938705, \n",
      "Params:  tensor([  5.3051, -16.9501])\n",
      "Grad:  tensor([-0.0107,  0.0604])\n",
      "------\n",
      "------\n",
      "Epoch 2295, Loss 2.938667, \n",
      "Params:  tensor([  5.3052, -16.9507])\n",
      "Grad:  tensor([-0.0106,  0.0603])\n",
      "------\n",
      "------\n",
      "Epoch 2296, Loss 2.938629, \n",
      "Params:  tensor([  5.3053, -16.9513])\n",
      "Grad:  tensor([-0.0106,  0.0602])\n",
      "------\n",
      "------\n",
      "Epoch 2297, Loss 2.938593, \n",
      "Params:  tensor([  5.3054, -16.9519])\n",
      "Grad:  tensor([-0.0106,  0.0601])\n",
      "------\n",
      "------\n",
      "Epoch 2298, Loss 2.938555, \n",
      "Params:  tensor([  5.3055, -16.9525])\n",
      "Grad:  tensor([-0.0106,  0.0600])\n",
      "------\n",
      "------\n",
      "Epoch 2299, Loss 2.938519, \n",
      "Params:  tensor([  5.3056, -16.9531])\n",
      "Grad:  tensor([-0.0106,  0.0598])\n",
      "------\n",
      "------\n",
      "Epoch 2300, Loss 2.938481, \n",
      "Params:  tensor([  5.3057, -16.9537])\n",
      "Grad:  tensor([-0.0106,  0.0597])\n",
      "------\n",
      "------\n",
      "Epoch 2301, Loss 2.938444, \n",
      "Params:  tensor([  5.3058, -16.9543])\n",
      "Grad:  tensor([-0.0105,  0.0596])\n",
      "------\n",
      "------\n",
      "Epoch 2302, Loss 2.938408, \n",
      "Params:  tensor([  5.3059, -16.9549])\n",
      "Grad:  tensor([-0.0105,  0.0595])\n",
      "------\n",
      "------\n",
      "Epoch 2303, Loss 2.938371, \n",
      "Params:  tensor([  5.3060, -16.9554])\n",
      "Grad:  tensor([-0.0105,  0.0594])\n",
      "------\n",
      "------\n",
      "Epoch 2304, Loss 2.938335, \n",
      "Params:  tensor([  5.3061, -16.9560])\n",
      "Grad:  tensor([-0.0105,  0.0593])\n",
      "------\n",
      "------\n",
      "Epoch 2305, Loss 2.938299, \n",
      "Params:  tensor([  5.3062, -16.9566])\n",
      "Grad:  tensor([-0.0105,  0.0592])\n",
      "------\n",
      "------\n",
      "Epoch 2306, Loss 2.938263, \n",
      "Params:  tensor([  5.3063, -16.9572])\n",
      "Grad:  tensor([-0.0105,  0.0591])\n",
      "------\n",
      "------\n",
      "Epoch 2307, Loss 2.938227, \n",
      "Params:  tensor([  5.3064, -16.9578])\n",
      "Grad:  tensor([-0.0104,  0.0590])\n",
      "------\n",
      "------\n",
      "Epoch 2308, Loss 2.938190, \n",
      "Params:  tensor([  5.3065, -16.9584])\n",
      "Grad:  tensor([-0.0104,  0.0589])\n",
      "------\n",
      "------\n",
      "Epoch 2309, Loss 2.938155, \n",
      "Params:  tensor([  5.3066, -16.9590])\n",
      "Grad:  tensor([-0.0104,  0.0588])\n",
      "------\n",
      "------\n",
      "Epoch 2310, Loss 2.938118, \n",
      "Params:  tensor([  5.3067, -16.9596])\n",
      "Grad:  tensor([-0.0104,  0.0587])\n",
      "------\n",
      "------\n",
      "Epoch 2311, Loss 2.938084, \n",
      "Params:  tensor([  5.3068, -16.9602])\n",
      "Grad:  tensor([-0.0104,  0.0586])\n",
      "------\n",
      "------\n",
      "Epoch 2312, Loss 2.938049, \n",
      "Params:  tensor([  5.3069, -16.9608])\n",
      "Grad:  tensor([-0.0103,  0.0585])\n",
      "------\n",
      "------\n",
      "Epoch 2313, Loss 2.938014, \n",
      "Params:  tensor([  5.3070, -16.9613])\n",
      "Grad:  tensor([-0.0103,  0.0584])\n",
      "------\n",
      "------\n",
      "Epoch 2314, Loss 2.937977, \n",
      "Params:  tensor([  5.3072, -16.9619])\n",
      "Grad:  tensor([-0.0103,  0.0583])\n",
      "------\n",
      "------\n",
      "Epoch 2315, Loss 2.937943, \n",
      "Params:  tensor([  5.3073, -16.9625])\n",
      "Grad:  tensor([-0.0103,  0.0582])\n",
      "------\n",
      "------\n",
      "Epoch 2316, Loss 2.937908, \n",
      "Params:  tensor([  5.3074, -16.9631])\n",
      "Grad:  tensor([-0.0103,  0.0581])\n",
      "------\n",
      "------\n",
      "Epoch 2317, Loss 2.937872, \n",
      "Params:  tensor([  5.3075, -16.9637])\n",
      "Grad:  tensor([-0.0103,  0.0580])\n",
      "------\n",
      "------\n",
      "Epoch 2318, Loss 2.937839, \n",
      "Params:  tensor([  5.3076, -16.9642])\n",
      "Grad:  tensor([-0.0102,  0.0580])\n",
      "------\n",
      "------\n",
      "Epoch 2319, Loss 2.937804, \n",
      "Params:  tensor([  5.3077, -16.9648])\n",
      "Grad:  tensor([-0.0102,  0.0578])\n",
      "------\n",
      "------\n",
      "Epoch 2320, Loss 2.937769, \n",
      "Params:  tensor([  5.3078, -16.9654])\n",
      "Grad:  tensor([-0.0102,  0.0578])\n",
      "------\n",
      "------\n",
      "Epoch 2321, Loss 2.937734, \n",
      "Params:  tensor([  5.3079, -16.9660])\n",
      "Grad:  tensor([-0.0102,  0.0577])\n",
      "------\n",
      "------\n",
      "Epoch 2322, Loss 2.937700, \n",
      "Params:  tensor([  5.3080, -16.9666])\n",
      "Grad:  tensor([-0.0102,  0.0576])\n",
      "------\n",
      "------\n",
      "Epoch 2323, Loss 2.937665, \n",
      "Params:  tensor([  5.3081, -16.9671])\n",
      "Grad:  tensor([-0.0102,  0.0575])\n",
      "------\n",
      "------\n",
      "Epoch 2324, Loss 2.937632, \n",
      "Params:  tensor([  5.3082, -16.9677])\n",
      "Grad:  tensor([-0.0101,  0.0574])\n",
      "------\n",
      "------\n",
      "Epoch 2325, Loss 2.937598, \n",
      "Params:  tensor([  5.3083, -16.9683])\n",
      "Grad:  tensor([-0.0101,  0.0573])\n",
      "------\n",
      "------\n",
      "Epoch 2326, Loss 2.937565, \n",
      "Params:  tensor([  5.3084, -16.9688])\n",
      "Grad:  tensor([-0.0101,  0.0572])\n",
      "------\n",
      "------\n",
      "Epoch 2327, Loss 2.937531, \n",
      "Params:  tensor([  5.3085, -16.9694])\n",
      "Grad:  tensor([-0.0101,  0.0571])\n",
      "------\n",
      "------\n",
      "Epoch 2328, Loss 2.937499, \n",
      "Params:  tensor([  5.3086, -16.9700])\n",
      "Grad:  tensor([-0.0101,  0.0570])\n",
      "------\n",
      "------\n",
      "Epoch 2329, Loss 2.937465, \n",
      "Params:  tensor([  5.3087, -16.9706])\n",
      "Grad:  tensor([-0.0101,  0.0569])\n",
      "------\n",
      "------\n",
      "Epoch 2330, Loss 2.937430, \n",
      "Params:  tensor([  5.3088, -16.9711])\n",
      "Grad:  tensor([-0.0100,  0.0568])\n",
      "------\n",
      "------\n",
      "Epoch 2331, Loss 2.937398, \n",
      "Params:  tensor([  5.3089, -16.9717])\n",
      "Grad:  tensor([-0.0100,  0.0567])\n",
      "------\n",
      "------\n",
      "Epoch 2332, Loss 2.937364, \n",
      "Params:  tensor([  5.3090, -16.9723])\n",
      "Grad:  tensor([-0.0100,  0.0566])\n",
      "------\n",
      "------\n",
      "Epoch 2333, Loss 2.937332, \n",
      "Params:  tensor([  5.3091, -16.9728])\n",
      "Grad:  tensor([-0.0100,  0.0565])\n",
      "------\n",
      "------\n",
      "Epoch 2334, Loss 2.937299, \n",
      "Params:  tensor([  5.3092, -16.9734])\n",
      "Grad:  tensor([-0.0100,  0.0564])\n",
      "------\n",
      "------\n",
      "Epoch 2335, Loss 2.937265, \n",
      "Params:  tensor([  5.3093, -16.9739])\n",
      "Grad:  tensor([-0.0100,  0.0563])\n",
      "------\n",
      "------\n",
      "Epoch 2336, Loss 2.937232, \n",
      "Params:  tensor([  5.3094, -16.9745])\n",
      "Grad:  tensor([-0.0099,  0.0562])\n",
      "------\n",
      "------\n",
      "Epoch 2337, Loss 2.937201, \n",
      "Params:  tensor([  5.3095, -16.9751])\n",
      "Grad:  tensor([-0.0099,  0.0561])\n",
      "------\n",
      "------\n",
      "Epoch 2338, Loss 2.937167, \n",
      "Params:  tensor([  5.3096, -16.9756])\n",
      "Grad:  tensor([-0.0099,  0.0560])\n",
      "------\n",
      "------\n",
      "Epoch 2339, Loss 2.937134, \n",
      "Params:  tensor([  5.3097, -16.9762])\n",
      "Grad:  tensor([-0.0099,  0.0559])\n",
      "------\n",
      "------\n",
      "Epoch 2340, Loss 2.937104, \n",
      "Params:  tensor([  5.3098, -16.9767])\n",
      "Grad:  tensor([-0.0099,  0.0558])\n",
      "------\n",
      "------\n",
      "Epoch 2341, Loss 2.937071, \n",
      "Params:  tensor([  5.3099, -16.9773])\n",
      "Grad:  tensor([-0.0098,  0.0557])\n",
      "------\n",
      "------\n",
      "Epoch 2342, Loss 2.937039, \n",
      "Params:  tensor([  5.3100, -16.9779])\n",
      "Grad:  tensor([-0.0098,  0.0556])\n",
      "------\n",
      "------\n",
      "Epoch 2343, Loss 2.937008, \n",
      "Params:  tensor([  5.3101, -16.9784])\n",
      "Grad:  tensor([-0.0098,  0.0555])\n",
      "------\n",
      "------\n",
      "Epoch 2344, Loss 2.936976, \n",
      "Params:  tensor([  5.3102, -16.9790])\n",
      "Grad:  tensor([-0.0098,  0.0554])\n",
      "------\n",
      "------\n",
      "Epoch 2345, Loss 2.936945, \n",
      "Params:  tensor([  5.3103, -16.9795])\n",
      "Grad:  tensor([-0.0098,  0.0553])\n",
      "------\n",
      "------\n",
      "Epoch 2346, Loss 2.936912, \n",
      "Params:  tensor([  5.3104, -16.9801])\n",
      "Grad:  tensor([-0.0098,  0.0553])\n",
      "------\n",
      "------\n",
      "Epoch 2347, Loss 2.936883, \n",
      "Params:  tensor([  5.3105, -16.9806])\n",
      "Grad:  tensor([-0.0097,  0.0552])\n",
      "------\n",
      "------\n",
      "Epoch 2348, Loss 2.936851, \n",
      "Params:  tensor([  5.3106, -16.9812])\n",
      "Grad:  tensor([-0.0097,  0.0551])\n",
      "------\n",
      "------\n",
      "Epoch 2349, Loss 2.936819, \n",
      "Params:  tensor([  5.3107, -16.9817])\n",
      "Grad:  tensor([-0.0097,  0.0550])\n",
      "------\n",
      "------\n",
      "Epoch 2350, Loss 2.936788, \n",
      "Params:  tensor([  5.3107, -16.9823])\n",
      "Grad:  tensor([-0.0097,  0.0549])\n",
      "------\n",
      "------\n",
      "Epoch 2351, Loss 2.936757, \n",
      "Params:  tensor([  5.3108, -16.9828])\n",
      "Grad:  tensor([-0.0097,  0.0548])\n",
      "------\n",
      "------\n",
      "Epoch 2352, Loss 2.936725, \n",
      "Params:  tensor([  5.3109, -16.9834])\n",
      "Grad:  tensor([-0.0097,  0.0547])\n",
      "------\n",
      "------\n",
      "Epoch 2353, Loss 2.936694, \n",
      "Params:  tensor([  5.3110, -16.9839])\n",
      "Grad:  tensor([-0.0096,  0.0546])\n",
      "------\n",
      "------\n",
      "Epoch 2354, Loss 2.936665, \n",
      "Params:  tensor([  5.3111, -16.9845])\n",
      "Grad:  tensor([-0.0096,  0.0545])\n",
      "------\n",
      "------\n",
      "Epoch 2355, Loss 2.936633, \n",
      "Params:  tensor([  5.3112, -16.9850])\n",
      "Grad:  tensor([-0.0096,  0.0544])\n",
      "------\n",
      "------\n",
      "Epoch 2356, Loss 2.936602, \n",
      "Params:  tensor([  5.3113, -16.9856])\n",
      "Grad:  tensor([-0.0096,  0.0543])\n",
      "------\n",
      "------\n",
      "Epoch 2357, Loss 2.936572, \n",
      "Params:  tensor([  5.3114, -16.9861])\n",
      "Grad:  tensor([-0.0096,  0.0542])\n",
      "------\n",
      "------\n",
      "Epoch 2358, Loss 2.936542, \n",
      "Params:  tensor([  5.3115, -16.9866])\n",
      "Grad:  tensor([-0.0095,  0.0541])\n",
      "------\n",
      "------\n",
      "Epoch 2359, Loss 2.936511, \n",
      "Params:  tensor([  5.3116, -16.9872])\n",
      "Grad:  tensor([-0.0096,  0.0540])\n",
      "------\n",
      "------\n",
      "Epoch 2360, Loss 2.936481, \n",
      "Params:  tensor([  5.3117, -16.9877])\n",
      "Grad:  tensor([-0.0095,  0.0540])\n",
      "------\n",
      "------\n",
      "Epoch 2361, Loss 2.936451, \n",
      "Params:  tensor([  5.3118, -16.9883])\n",
      "Grad:  tensor([-0.0095,  0.0539])\n",
      "------\n",
      "------\n",
      "Epoch 2362, Loss 2.936421, \n",
      "Params:  tensor([  5.3119, -16.9888])\n",
      "Grad:  tensor([-0.0095,  0.0538])\n",
      "------\n",
      "------\n",
      "Epoch 2363, Loss 2.936392, \n",
      "Params:  tensor([  5.3120, -16.9893])\n",
      "Grad:  tensor([-0.0095,  0.0537])\n",
      "------\n",
      "------\n",
      "Epoch 2364, Loss 2.936362, \n",
      "Params:  tensor([  5.3121, -16.9899])\n",
      "Grad:  tensor([-0.0094,  0.0536])\n",
      "------\n",
      "------\n",
      "Epoch 2365, Loss 2.936332, \n",
      "Params:  tensor([  5.3122, -16.9904])\n",
      "Grad:  tensor([-0.0094,  0.0535])\n",
      "------\n",
      "------\n",
      "Epoch 2366, Loss 2.936304, \n",
      "Params:  tensor([  5.3123, -16.9909])\n",
      "Grad:  tensor([-0.0094,  0.0534])\n",
      "------\n",
      "------\n",
      "Epoch 2367, Loss 2.936274, \n",
      "Params:  tensor([  5.3124, -16.9915])\n",
      "Grad:  tensor([-0.0094,  0.0533])\n",
      "------\n",
      "------\n",
      "Epoch 2368, Loss 2.936244, \n",
      "Params:  tensor([  5.3125, -16.9920])\n",
      "Grad:  tensor([-0.0094,  0.0532])\n",
      "------\n",
      "------\n",
      "Epoch 2369, Loss 2.936216, \n",
      "Params:  tensor([  5.3126, -16.9925])\n",
      "Grad:  tensor([-0.0094,  0.0531])\n",
      "------\n",
      "------\n",
      "Epoch 2370, Loss 2.936188, \n",
      "Params:  tensor([  5.3127, -16.9931])\n",
      "Grad:  tensor([-0.0094,  0.0530])\n",
      "------\n",
      "------\n",
      "Epoch 2371, Loss 2.936156, \n",
      "Params:  tensor([  5.3127, -16.9936])\n",
      "Grad:  tensor([-0.0094,  0.0530])\n",
      "------\n",
      "------\n",
      "Epoch 2372, Loss 2.936128, \n",
      "Params:  tensor([  5.3128, -16.9941])\n",
      "Grad:  tensor([-0.0093,  0.0529])\n",
      "------\n",
      "------\n",
      "Epoch 2373, Loss 2.936100, \n",
      "Params:  tensor([  5.3129, -16.9946])\n",
      "Grad:  tensor([-0.0093,  0.0528])\n",
      "------\n",
      "------\n",
      "Epoch 2374, Loss 2.936072, \n",
      "Params:  tensor([  5.3130, -16.9952])\n",
      "Grad:  tensor([-0.0093,  0.0527])\n",
      "------\n",
      "------\n",
      "Epoch 2375, Loss 2.936042, \n",
      "Params:  tensor([  5.3131, -16.9957])\n",
      "Grad:  tensor([-0.0093,  0.0526])\n",
      "------\n",
      "------\n",
      "Epoch 2376, Loss 2.936014, \n",
      "Params:  tensor([  5.3132, -16.9962])\n",
      "Grad:  tensor([-0.0093,  0.0525])\n",
      "------\n",
      "------\n",
      "Epoch 2377, Loss 2.935986, \n",
      "Params:  tensor([  5.3133, -16.9967])\n",
      "Grad:  tensor([-0.0093,  0.0524])\n",
      "------\n",
      "------\n",
      "Epoch 2378, Loss 2.935957, \n",
      "Params:  tensor([  5.3134, -16.9973])\n",
      "Grad:  tensor([-0.0093,  0.0523])\n",
      "------\n",
      "------\n",
      "Epoch 2379, Loss 2.935928, \n",
      "Params:  tensor([  5.3135, -16.9978])\n",
      "Grad:  tensor([-0.0092,  0.0522])\n",
      "------\n",
      "------\n",
      "Epoch 2380, Loss 2.935901, \n",
      "Params:  tensor([  5.3136, -16.9983])\n",
      "Grad:  tensor([-0.0092,  0.0522])\n",
      "------\n",
      "------\n",
      "Epoch 2381, Loss 2.935873, \n",
      "Params:  tensor([  5.3137, -16.9988])\n",
      "Grad:  tensor([-0.0092,  0.0521])\n",
      "------\n",
      "------\n",
      "Epoch 2382, Loss 2.935845, \n",
      "Params:  tensor([  5.3138, -16.9994])\n",
      "Grad:  tensor([-0.0092,  0.0520])\n",
      "------\n",
      "------\n",
      "Epoch 2383, Loss 2.935817, \n",
      "Params:  tensor([  5.3139, -16.9999])\n",
      "Grad:  tensor([-0.0092,  0.0519])\n",
      "------\n",
      "------\n",
      "Epoch 2384, Loss 2.935789, \n",
      "Params:  tensor([  5.3139, -17.0004])\n",
      "Grad:  tensor([-0.0092,  0.0518])\n",
      "------\n",
      "------\n",
      "Epoch 2385, Loss 2.935762, \n",
      "Params:  tensor([  5.3140, -17.0009])\n",
      "Grad:  tensor([-0.0092,  0.0517])\n",
      "------\n",
      "------\n",
      "Epoch 2386, Loss 2.935734, \n",
      "Params:  tensor([  5.3141, -17.0014])\n",
      "Grad:  tensor([-0.0091,  0.0516])\n",
      "------\n",
      "------\n",
      "Epoch 2387, Loss 2.935707, \n",
      "Params:  tensor([  5.3142, -17.0019])\n",
      "Grad:  tensor([-0.0091,  0.0515])\n",
      "------\n",
      "------\n",
      "Epoch 2388, Loss 2.935679, \n",
      "Params:  tensor([  5.3143, -17.0025])\n",
      "Grad:  tensor([-0.0091,  0.0514])\n",
      "------\n",
      "------\n",
      "Epoch 2389, Loss 2.935650, \n",
      "Params:  tensor([  5.3144, -17.0030])\n",
      "Grad:  tensor([-0.0091,  0.0514])\n",
      "------\n",
      "------\n",
      "Epoch 2390, Loss 2.935626, \n",
      "Params:  tensor([  5.3145, -17.0035])\n",
      "Grad:  tensor([-0.0090,  0.0513])\n",
      "------\n",
      "------\n",
      "Epoch 2391, Loss 2.935596, \n",
      "Params:  tensor([  5.3146, -17.0040])\n",
      "Grad:  tensor([-0.0090,  0.0512])\n",
      "------\n",
      "------\n",
      "Epoch 2392, Loss 2.935571, \n",
      "Params:  tensor([  5.3147, -17.0045])\n",
      "Grad:  tensor([-0.0090,  0.0511])\n",
      "------\n",
      "------\n",
      "Epoch 2393, Loss 2.935544, \n",
      "Params:  tensor([  5.3148, -17.0050])\n",
      "Grad:  tensor([-0.0090,  0.0510])\n",
      "------\n",
      "------\n",
      "Epoch 2394, Loss 2.935516, \n",
      "Params:  tensor([  5.3149, -17.0055])\n",
      "Grad:  tensor([-0.0090,  0.0509])\n",
      "------\n",
      "------\n",
      "Epoch 2395, Loss 2.935489, \n",
      "Params:  tensor([  5.3149, -17.0060])\n",
      "Grad:  tensor([-0.0090,  0.0508])\n",
      "------\n",
      "------\n",
      "Epoch 2396, Loss 2.935465, \n",
      "Params:  tensor([  5.3150, -17.0065])\n",
      "Grad:  tensor([-0.0090,  0.0507])\n",
      "------\n",
      "------\n",
      "Epoch 2397, Loss 2.935436, \n",
      "Params:  tensor([  5.3151, -17.0070])\n",
      "Grad:  tensor([-0.0090,  0.0507])\n",
      "------\n",
      "------\n",
      "Epoch 2398, Loss 2.935411, \n",
      "Params:  tensor([  5.3152, -17.0076])\n",
      "Grad:  tensor([-0.0089,  0.0506])\n",
      "------\n",
      "------\n",
      "Epoch 2399, Loss 2.935385, \n",
      "Params:  tensor([  5.3153, -17.0081])\n",
      "Grad:  tensor([-0.0089,  0.0505])\n",
      "------\n",
      "------\n",
      "Epoch 2400, Loss 2.935356, \n",
      "Params:  tensor([  5.3154, -17.0086])\n",
      "Grad:  tensor([-0.0089,  0.0504])\n",
      "------\n",
      "------\n",
      "Epoch 2401, Loss 2.935332, \n",
      "Params:  tensor([  5.3155, -17.0091])\n",
      "Grad:  tensor([-0.0089,  0.0503])\n",
      "------\n",
      "------\n",
      "Epoch 2402, Loss 2.935304, \n",
      "Params:  tensor([  5.3156, -17.0096])\n",
      "Grad:  tensor([-0.0089,  0.0502])\n",
      "------\n",
      "------\n",
      "Epoch 2403, Loss 2.935281, \n",
      "Params:  tensor([  5.3157, -17.0101])\n",
      "Grad:  tensor([-0.0088,  0.0502])\n",
      "------\n",
      "------\n",
      "Epoch 2404, Loss 2.935252, \n",
      "Params:  tensor([  5.3157, -17.0106])\n",
      "Grad:  tensor([-0.0088,  0.0501])\n",
      "------\n",
      "------\n",
      "Epoch 2405, Loss 2.935228, \n",
      "Params:  tensor([  5.3158, -17.0111])\n",
      "Grad:  tensor([-0.0088,  0.0500])\n",
      "------\n",
      "------\n",
      "Epoch 2406, Loss 2.935203, \n",
      "Params:  tensor([  5.3159, -17.0116])\n",
      "Grad:  tensor([-0.0088,  0.0499])\n",
      "------\n",
      "------\n",
      "Epoch 2407, Loss 2.935177, \n",
      "Params:  tensor([  5.3160, -17.0121])\n",
      "Grad:  tensor([-0.0088,  0.0498])\n",
      "------\n",
      "------\n",
      "Epoch 2408, Loss 2.935152, \n",
      "Params:  tensor([  5.3161, -17.0126])\n",
      "Grad:  tensor([-0.0088,  0.0497])\n",
      "------\n",
      "------\n",
      "Epoch 2409, Loss 2.935126, \n",
      "Params:  tensor([  5.3162, -17.0131])\n",
      "Grad:  tensor([-0.0088,  0.0496])\n",
      "------\n",
      "------\n",
      "Epoch 2410, Loss 2.935100, \n",
      "Params:  tensor([  5.3163, -17.0136])\n",
      "Grad:  tensor([-0.0088,  0.0496])\n",
      "------\n",
      "------\n",
      "Epoch 2411, Loss 2.935075, \n",
      "Params:  tensor([  5.3164, -17.0140])\n",
      "Grad:  tensor([-0.0087,  0.0495])\n",
      "------\n",
      "------\n",
      "Epoch 2412, Loss 2.935049, \n",
      "Params:  tensor([  5.3164, -17.0145])\n",
      "Grad:  tensor([-0.0087,  0.0494])\n",
      "------\n",
      "------\n",
      "Epoch 2413, Loss 2.935024, \n",
      "Params:  tensor([  5.3165, -17.0150])\n",
      "Grad:  tensor([-0.0087,  0.0493])\n",
      "------\n",
      "------\n",
      "Epoch 2414, Loss 2.935001, \n",
      "Params:  tensor([  5.3166, -17.0155])\n",
      "Grad:  tensor([-0.0087,  0.0492])\n",
      "------\n",
      "------\n",
      "Epoch 2415, Loss 2.934973, \n",
      "Params:  tensor([  5.3167, -17.0160])\n",
      "Grad:  tensor([-0.0087,  0.0491])\n",
      "------\n",
      "------\n",
      "Epoch 2416, Loss 2.934949, \n",
      "Params:  tensor([  5.3168, -17.0165])\n",
      "Grad:  tensor([-0.0087,  0.0491])\n",
      "------\n",
      "------\n",
      "Epoch 2417, Loss 2.934925, \n",
      "Params:  tensor([  5.3169, -17.0170])\n",
      "Grad:  tensor([-0.0086,  0.0490])\n",
      "------\n",
      "------\n",
      "Epoch 2418, Loss 2.934899, \n",
      "Params:  tensor([  5.3170, -17.0175])\n",
      "Grad:  tensor([-0.0086,  0.0489])\n",
      "------\n",
      "------\n",
      "Epoch 2419, Loss 2.934876, \n",
      "Params:  tensor([  5.3171, -17.0180])\n",
      "Grad:  tensor([-0.0086,  0.0488])\n",
      "------\n",
      "------\n",
      "Epoch 2420, Loss 2.934853, \n",
      "Params:  tensor([  5.3171, -17.0185])\n",
      "Grad:  tensor([-0.0086,  0.0487])\n",
      "------\n",
      "------\n",
      "Epoch 2421, Loss 2.934826, \n",
      "Params:  tensor([  5.3172, -17.0189])\n",
      "Grad:  tensor([-0.0086,  0.0486])\n",
      "------\n",
      "------\n",
      "Epoch 2422, Loss 2.934802, \n",
      "Params:  tensor([  5.3173, -17.0194])\n",
      "Grad:  tensor([-0.0086,  0.0486])\n",
      "------\n",
      "------\n",
      "Epoch 2423, Loss 2.934777, \n",
      "Params:  tensor([  5.3174, -17.0199])\n",
      "Grad:  tensor([-0.0086,  0.0485])\n",
      "------\n",
      "------\n",
      "Epoch 2424, Loss 2.934753, \n",
      "Params:  tensor([  5.3175, -17.0204])\n",
      "Grad:  tensor([-0.0086,  0.0484])\n",
      "------\n",
      "------\n",
      "Epoch 2425, Loss 2.934730, \n",
      "Params:  tensor([  5.3176, -17.0209])\n",
      "Grad:  tensor([-0.0086,  0.0483])\n",
      "------\n",
      "------\n",
      "Epoch 2426, Loss 2.934705, \n",
      "Params:  tensor([  5.3177, -17.0214])\n",
      "Grad:  tensor([-0.0085,  0.0482])\n",
      "------\n",
      "------\n",
      "Epoch 2427, Loss 2.934681, \n",
      "Params:  tensor([  5.3177, -17.0219])\n",
      "Grad:  tensor([-0.0085,  0.0481])\n",
      "------\n",
      "------\n",
      "Epoch 2428, Loss 2.934658, \n",
      "Params:  tensor([  5.3178, -17.0223])\n",
      "Grad:  tensor([-0.0085,  0.0481])\n",
      "------\n",
      "------\n",
      "Epoch 2429, Loss 2.934635, \n",
      "Params:  tensor([  5.3179, -17.0228])\n",
      "Grad:  tensor([-0.0085,  0.0480])\n",
      "------\n",
      "------\n",
      "Epoch 2430, Loss 2.934609, \n",
      "Params:  tensor([  5.3180, -17.0233])\n",
      "Grad:  tensor([-0.0085,  0.0479])\n",
      "------\n",
      "------\n",
      "Epoch 2431, Loss 2.934585, \n",
      "Params:  tensor([  5.3181, -17.0238])\n",
      "Grad:  tensor([-0.0084,  0.0478])\n",
      "------\n",
      "------\n",
      "Epoch 2432, Loss 2.934563, \n",
      "Params:  tensor([  5.3182, -17.0242])\n",
      "Grad:  tensor([-0.0084,  0.0477])\n",
      "------\n",
      "------\n",
      "Epoch 2433, Loss 2.934541, \n",
      "Params:  tensor([  5.3182, -17.0247])\n",
      "Grad:  tensor([-0.0084,  0.0477])\n",
      "------\n",
      "------\n",
      "Epoch 2434, Loss 2.934516, \n",
      "Params:  tensor([  5.3183, -17.0252])\n",
      "Grad:  tensor([-0.0084,  0.0476])\n",
      "------\n",
      "------\n",
      "Epoch 2435, Loss 2.934493, \n",
      "Params:  tensor([  5.3184, -17.0257])\n",
      "Grad:  tensor([-0.0084,  0.0475])\n",
      "------\n",
      "------\n",
      "Epoch 2436, Loss 2.934469, \n",
      "Params:  tensor([  5.3185, -17.0261])\n",
      "Grad:  tensor([-0.0084,  0.0474])\n",
      "------\n",
      "------\n",
      "Epoch 2437, Loss 2.934446, \n",
      "Params:  tensor([  5.3186, -17.0266])\n",
      "Grad:  tensor([-0.0084,  0.0473])\n",
      "------\n",
      "------\n",
      "Epoch 2438, Loss 2.934423, \n",
      "Params:  tensor([  5.3187, -17.0271])\n",
      "Grad:  tensor([-0.0083,  0.0473])\n",
      "------\n",
      "------\n",
      "Epoch 2439, Loss 2.934400, \n",
      "Params:  tensor([  5.3187, -17.0276])\n",
      "Grad:  tensor([-0.0083,  0.0472])\n",
      "------\n",
      "------\n",
      "Epoch 2440, Loss 2.934377, \n",
      "Params:  tensor([  5.3188, -17.0280])\n",
      "Grad:  tensor([-0.0083,  0.0471])\n",
      "------\n",
      "------\n",
      "Epoch 2441, Loss 2.934355, \n",
      "Params:  tensor([  5.3189, -17.0285])\n",
      "Grad:  tensor([-0.0083,  0.0470])\n",
      "------\n",
      "------\n",
      "Epoch 2442, Loss 2.934331, \n",
      "Params:  tensor([  5.3190, -17.0290])\n",
      "Grad:  tensor([-0.0083,  0.0469])\n",
      "------\n",
      "------\n",
      "Epoch 2443, Loss 2.934309, \n",
      "Params:  tensor([  5.3191, -17.0294])\n",
      "Grad:  tensor([-0.0083,  0.0469])\n",
      "------\n",
      "------\n",
      "Epoch 2444, Loss 2.934287, \n",
      "Params:  tensor([  5.3192, -17.0299])\n",
      "Grad:  tensor([-0.0083,  0.0468])\n",
      "------\n",
      "------\n",
      "Epoch 2445, Loss 2.934264, \n",
      "Params:  tensor([  5.3192, -17.0304])\n",
      "Grad:  tensor([-0.0083,  0.0467])\n",
      "------\n",
      "------\n",
      "Epoch 2446, Loss 2.934242, \n",
      "Params:  tensor([  5.3193, -17.0308])\n",
      "Grad:  tensor([-0.0083,  0.0466])\n",
      "------\n",
      "------\n",
      "Epoch 2447, Loss 2.934219, \n",
      "Params:  tensor([  5.3194, -17.0313])\n",
      "Grad:  tensor([-0.0082,  0.0465])\n",
      "------\n",
      "------\n",
      "Epoch 2448, Loss 2.934198, \n",
      "Params:  tensor([  5.3195, -17.0318])\n",
      "Grad:  tensor([-0.0082,  0.0465])\n",
      "------\n",
      "------\n",
      "Epoch 2449, Loss 2.934175, \n",
      "Params:  tensor([  5.3196, -17.0322])\n",
      "Grad:  tensor([-0.0082,  0.0464])\n",
      "------\n",
      "------\n",
      "Epoch 2450, Loss 2.934151, \n",
      "Params:  tensor([  5.3197, -17.0327])\n",
      "Grad:  tensor([-0.0082,  0.0463])\n",
      "------\n",
      "------\n",
      "Epoch 2451, Loss 2.934129, \n",
      "Params:  tensor([  5.3197, -17.0332])\n",
      "Grad:  tensor([-0.0082,  0.0462])\n",
      "------\n",
      "------\n",
      "Epoch 2452, Loss 2.934108, \n",
      "Params:  tensor([  5.3198, -17.0336])\n",
      "Grad:  tensor([-0.0082,  0.0461])\n",
      "------\n",
      "------\n",
      "Epoch 2453, Loss 2.934084, \n",
      "Params:  tensor([  5.3199, -17.0341])\n",
      "Grad:  tensor([-0.0081,  0.0461])\n",
      "------\n",
      "------\n",
      "Epoch 2454, Loss 2.934065, \n",
      "Params:  tensor([  5.3200, -17.0345])\n",
      "Grad:  tensor([-0.0081,  0.0460])\n",
      "------\n",
      "------\n",
      "Epoch 2455, Loss 2.934043, \n",
      "Params:  tensor([  5.3201, -17.0350])\n",
      "Grad:  tensor([-0.0081,  0.0459])\n",
      "------\n",
      "------\n",
      "Epoch 2456, Loss 2.934020, \n",
      "Params:  tensor([  5.3201, -17.0355])\n",
      "Grad:  tensor([-0.0081,  0.0458])\n",
      "------\n",
      "------\n",
      "Epoch 2457, Loss 2.934000, \n",
      "Params:  tensor([  5.3202, -17.0359])\n",
      "Grad:  tensor([-0.0081,  0.0457])\n",
      "------\n",
      "------\n",
      "Epoch 2458, Loss 2.933978, \n",
      "Params:  tensor([  5.3203, -17.0364])\n",
      "Grad:  tensor([-0.0081,  0.0457])\n",
      "------\n",
      "------\n",
      "Epoch 2459, Loss 2.933956, \n",
      "Params:  tensor([  5.3204, -17.0368])\n",
      "Grad:  tensor([-0.0080,  0.0456])\n",
      "------\n",
      "------\n",
      "Epoch 2460, Loss 2.933935, \n",
      "Params:  tensor([  5.3205, -17.0373])\n",
      "Grad:  tensor([-0.0080,  0.0455])\n",
      "------\n",
      "------\n",
      "Epoch 2461, Loss 2.933914, \n",
      "Params:  tensor([  5.3205, -17.0377])\n",
      "Grad:  tensor([-0.0080,  0.0454])\n",
      "------\n",
      "------\n",
      "Epoch 2462, Loss 2.933893, \n",
      "Params:  tensor([  5.3206, -17.0382])\n",
      "Grad:  tensor([-0.0080,  0.0454])\n",
      "------\n",
      "------\n",
      "Epoch 2463, Loss 2.933871, \n",
      "Params:  tensor([  5.3207, -17.0386])\n",
      "Grad:  tensor([-0.0080,  0.0453])\n",
      "------\n",
      "------\n",
      "Epoch 2464, Loss 2.933849, \n",
      "Params:  tensor([  5.3208, -17.0391])\n",
      "Grad:  tensor([-0.0080,  0.0452])\n",
      "------\n",
      "------\n",
      "Epoch 2465, Loss 2.933828, \n",
      "Params:  tensor([  5.3209, -17.0396])\n",
      "Grad:  tensor([-0.0080,  0.0451])\n",
      "------\n",
      "------\n",
      "Epoch 2466, Loss 2.933807, \n",
      "Params:  tensor([  5.3209, -17.0400])\n",
      "Grad:  tensor([-0.0080,  0.0451])\n",
      "------\n",
      "------\n",
      "Epoch 2467, Loss 2.933787, \n",
      "Params:  tensor([  5.3210, -17.0405])\n",
      "Grad:  tensor([-0.0079,  0.0450])\n",
      "------\n",
      "------\n",
      "Epoch 2468, Loss 2.933767, \n",
      "Params:  tensor([  5.3211, -17.0409])\n",
      "Grad:  tensor([-0.0079,  0.0449])\n",
      "------\n",
      "------\n",
      "Epoch 2469, Loss 2.933746, \n",
      "Params:  tensor([  5.3212, -17.0413])\n",
      "Grad:  tensor([-0.0079,  0.0448])\n",
      "------\n",
      "------\n",
      "Epoch 2470, Loss 2.933723, \n",
      "Params:  tensor([  5.3213, -17.0418])\n",
      "Grad:  tensor([-0.0079,  0.0448])\n",
      "------\n",
      "------\n",
      "Epoch 2471, Loss 2.933704, \n",
      "Params:  tensor([  5.3213, -17.0422])\n",
      "Grad:  tensor([-0.0079,  0.0447])\n",
      "------\n",
      "------\n",
      "Epoch 2472, Loss 2.933682, \n",
      "Params:  tensor([  5.3214, -17.0427])\n",
      "Grad:  tensor([-0.0079,  0.0446])\n",
      "------\n",
      "------\n",
      "Epoch 2473, Loss 2.933662, \n",
      "Params:  tensor([  5.3215, -17.0431])\n",
      "Grad:  tensor([-0.0079,  0.0445])\n",
      "------\n",
      "------\n",
      "Epoch 2474, Loss 2.933643, \n",
      "Params:  tensor([  5.3216, -17.0436])\n",
      "Grad:  tensor([-0.0079,  0.0444])\n",
      "------\n",
      "------\n",
      "Epoch 2475, Loss 2.933622, \n",
      "Params:  tensor([  5.3217, -17.0440])\n",
      "Grad:  tensor([-0.0078,  0.0444])\n",
      "------\n",
      "------\n",
      "Epoch 2476, Loss 2.933602, \n",
      "Params:  tensor([  5.3217, -17.0445])\n",
      "Grad:  tensor([-0.0078,  0.0443])\n",
      "------\n",
      "------\n",
      "Epoch 2477, Loss 2.933583, \n",
      "Params:  tensor([  5.3218, -17.0449])\n",
      "Grad:  tensor([-0.0078,  0.0442])\n",
      "------\n",
      "------\n",
      "Epoch 2478, Loss 2.933561, \n",
      "Params:  tensor([  5.3219, -17.0453])\n",
      "Grad:  tensor([-0.0078,  0.0441])\n",
      "------\n",
      "------\n",
      "Epoch 2479, Loss 2.933541, \n",
      "Params:  tensor([  5.3220, -17.0458])\n",
      "Grad:  tensor([-0.0078,  0.0441])\n",
      "------\n",
      "------\n",
      "Epoch 2480, Loss 2.933521, \n",
      "Params:  tensor([  5.3220, -17.0462])\n",
      "Grad:  tensor([-0.0078,  0.0440])\n",
      "------\n",
      "------\n",
      "Epoch 2481, Loss 2.933501, \n",
      "Params:  tensor([  5.3221, -17.0467])\n",
      "Grad:  tensor([-0.0078,  0.0439])\n",
      "------\n",
      "------\n",
      "Epoch 2482, Loss 2.933480, \n",
      "Params:  tensor([  5.3222, -17.0471])\n",
      "Grad:  tensor([-0.0077,  0.0438])\n",
      "------\n",
      "------\n",
      "Epoch 2483, Loss 2.933463, \n",
      "Params:  tensor([  5.3223, -17.0475])\n",
      "Grad:  tensor([-0.0077,  0.0438])\n",
      "------\n",
      "------\n",
      "Epoch 2484, Loss 2.933442, \n",
      "Params:  tensor([  5.3224, -17.0480])\n",
      "Grad:  tensor([-0.0077,  0.0437])\n",
      "------\n",
      "------\n",
      "Epoch 2485, Loss 2.933422, \n",
      "Params:  tensor([  5.3224, -17.0484])\n",
      "Grad:  tensor([-0.0077,  0.0436])\n",
      "------\n",
      "------\n",
      "Epoch 2486, Loss 2.933403, \n",
      "Params:  tensor([  5.3225, -17.0489])\n",
      "Grad:  tensor([-0.0077,  0.0436])\n",
      "------\n",
      "------\n",
      "Epoch 2487, Loss 2.933382, \n",
      "Params:  tensor([  5.3226, -17.0493])\n",
      "Grad:  tensor([-0.0077,  0.0435])\n",
      "------\n",
      "------\n",
      "Epoch 2488, Loss 2.933365, \n",
      "Params:  tensor([  5.3227, -17.0497])\n",
      "Grad:  tensor([-0.0077,  0.0434])\n",
      "------\n",
      "------\n",
      "Epoch 2489, Loss 2.933345, \n",
      "Params:  tensor([  5.3227, -17.0502])\n",
      "Grad:  tensor([-0.0077,  0.0433])\n",
      "------\n",
      "------\n",
      "Epoch 2490, Loss 2.933325, \n",
      "Params:  tensor([  5.3228, -17.0506])\n",
      "Grad:  tensor([-0.0076,  0.0433])\n",
      "------\n",
      "------\n",
      "Epoch 2491, Loss 2.933306, \n",
      "Params:  tensor([  5.3229, -17.0510])\n",
      "Grad:  tensor([-0.0076,  0.0432])\n",
      "------\n",
      "------\n",
      "Epoch 2492, Loss 2.933287, \n",
      "Params:  tensor([  5.3230, -17.0515])\n",
      "Grad:  tensor([-0.0076,  0.0431])\n",
      "------\n",
      "------\n",
      "Epoch 2493, Loss 2.933266, \n",
      "Params:  tensor([  5.3230, -17.0519])\n",
      "Grad:  tensor([-0.0076,  0.0430])\n",
      "------\n",
      "------\n",
      "Epoch 2494, Loss 2.933249, \n",
      "Params:  tensor([  5.3231, -17.0523])\n",
      "Grad:  tensor([-0.0076,  0.0430])\n",
      "------\n",
      "------\n",
      "Epoch 2495, Loss 2.933229, \n",
      "Params:  tensor([  5.3232, -17.0527])\n",
      "Grad:  tensor([-0.0076,  0.0429])\n",
      "------\n",
      "------\n",
      "Epoch 2496, Loss 2.933209, \n",
      "Params:  tensor([  5.3233, -17.0532])\n",
      "Grad:  tensor([-0.0076,  0.0428])\n",
      "------\n",
      "------\n",
      "Epoch 2497, Loss 2.933190, \n",
      "Params:  tensor([  5.3233, -17.0536])\n",
      "Grad:  tensor([-0.0075,  0.0427])\n",
      "------\n",
      "------\n",
      "Epoch 2498, Loss 2.933172, \n",
      "Params:  tensor([  5.3234, -17.0540])\n",
      "Grad:  tensor([-0.0075,  0.0427])\n",
      "------\n",
      "------\n",
      "Epoch 2499, Loss 2.933154, \n",
      "Params:  tensor([  5.3235, -17.0544])\n",
      "Grad:  tensor([-0.0075,  0.0426])\n",
      "------\n",
      "------\n",
      "Epoch 2500, Loss 2.933134, \n",
      "Params:  tensor([  5.3236, -17.0549])\n",
      "Grad:  tensor([-0.0075,  0.0425])\n",
      "------\n",
      "------\n",
      "Epoch 2501, Loss 2.933116, \n",
      "Params:  tensor([  5.3236, -17.0553])\n",
      "Grad:  tensor([-0.0075,  0.0425])\n",
      "------\n",
      "------\n",
      "Epoch 2502, Loss 2.933097, \n",
      "Params:  tensor([  5.3237, -17.0557])\n",
      "Grad:  tensor([-0.0075,  0.0424])\n",
      "------\n",
      "------\n",
      "Epoch 2503, Loss 2.933079, \n",
      "Params:  tensor([  5.3238, -17.0561])\n",
      "Grad:  tensor([-0.0075,  0.0423])\n",
      "------\n",
      "------\n",
      "Epoch 2504, Loss 2.933060, \n",
      "Params:  tensor([  5.3239, -17.0566])\n",
      "Grad:  tensor([-0.0075,  0.0422])\n",
      "------\n",
      "------\n",
      "Epoch 2505, Loss 2.933043, \n",
      "Params:  tensor([  5.3239, -17.0570])\n",
      "Grad:  tensor([-0.0074,  0.0422])\n",
      "------\n",
      "------\n",
      "Epoch 2506, Loss 2.933025, \n",
      "Params:  tensor([  5.3240, -17.0574])\n",
      "Grad:  tensor([-0.0074,  0.0421])\n",
      "------\n",
      "------\n",
      "Epoch 2507, Loss 2.933007, \n",
      "Params:  tensor([  5.3241, -17.0578])\n",
      "Grad:  tensor([-0.0074,  0.0420])\n",
      "------\n",
      "------\n",
      "Epoch 2508, Loss 2.932988, \n",
      "Params:  tensor([  5.3242, -17.0582])\n",
      "Grad:  tensor([-0.0074,  0.0420])\n",
      "------\n",
      "------\n",
      "Epoch 2509, Loss 2.932970, \n",
      "Params:  tensor([  5.3242, -17.0587])\n",
      "Grad:  tensor([-0.0074,  0.0419])\n",
      "------\n",
      "------\n",
      "Epoch 2510, Loss 2.932953, \n",
      "Params:  tensor([  5.3243, -17.0591])\n",
      "Grad:  tensor([-0.0074,  0.0418])\n",
      "------\n",
      "------\n",
      "Epoch 2511, Loss 2.932932, \n",
      "Params:  tensor([  5.3244, -17.0595])\n",
      "Grad:  tensor([-0.0074,  0.0417])\n",
      "------\n",
      "------\n",
      "Epoch 2512, Loss 2.932915, \n",
      "Params:  tensor([  5.3245, -17.0599])\n",
      "Grad:  tensor([-0.0073,  0.0417])\n",
      "------\n",
      "------\n",
      "Epoch 2513, Loss 2.932898, \n",
      "Params:  tensor([  5.3245, -17.0603])\n",
      "Grad:  tensor([-0.0073,  0.0416])\n",
      "------\n",
      "------\n",
      "Epoch 2514, Loss 2.932880, \n",
      "Params:  tensor([  5.3246, -17.0608])\n",
      "Grad:  tensor([-0.0073,  0.0415])\n",
      "------\n",
      "------\n",
      "Epoch 2515, Loss 2.932862, \n",
      "Params:  tensor([  5.3247, -17.0612])\n",
      "Grad:  tensor([-0.0073,  0.0415])\n",
      "------\n",
      "------\n",
      "Epoch 2516, Loss 2.932846, \n",
      "Params:  tensor([  5.3248, -17.0616])\n",
      "Grad:  tensor([-0.0073,  0.0414])\n",
      "------\n",
      "------\n",
      "Epoch 2517, Loss 2.932826, \n",
      "Params:  tensor([  5.3248, -17.0620])\n",
      "Grad:  tensor([-0.0073,  0.0413])\n",
      "------\n",
      "------\n",
      "Epoch 2518, Loss 2.932810, \n",
      "Params:  tensor([  5.3249, -17.0624])\n",
      "Grad:  tensor([-0.0073,  0.0412])\n",
      "------\n",
      "------\n",
      "Epoch 2519, Loss 2.932790, \n",
      "Params:  tensor([  5.3250, -17.0628])\n",
      "Grad:  tensor([-0.0073,  0.0412])\n",
      "------\n",
      "------\n",
      "Epoch 2520, Loss 2.932774, \n",
      "Params:  tensor([  5.3250, -17.0632])\n",
      "Grad:  tensor([-0.0073,  0.0411])\n",
      "------\n",
      "------\n",
      "Epoch 2521, Loss 2.932758, \n",
      "Params:  tensor([  5.3251, -17.0636])\n",
      "Grad:  tensor([-0.0073,  0.0410])\n",
      "------\n",
      "------\n",
      "Epoch 2522, Loss 2.932739, \n",
      "Params:  tensor([  5.3252, -17.0640])\n",
      "Grad:  tensor([-0.0073,  0.0410])\n",
      "------\n",
      "------\n",
      "Epoch 2523, Loss 2.932723, \n",
      "Params:  tensor([  5.3253, -17.0645])\n",
      "Grad:  tensor([-0.0072,  0.0409])\n",
      "------\n",
      "------\n",
      "Epoch 2524, Loss 2.932706, \n",
      "Params:  tensor([  5.3253, -17.0649])\n",
      "Grad:  tensor([-0.0072,  0.0408])\n",
      "------\n",
      "------\n",
      "Epoch 2525, Loss 2.932689, \n",
      "Params:  tensor([  5.3254, -17.0653])\n",
      "Grad:  tensor([-0.0072,  0.0408])\n",
      "------\n",
      "------\n",
      "Epoch 2526, Loss 2.932671, \n",
      "Params:  tensor([  5.3255, -17.0657])\n",
      "Grad:  tensor([-0.0072,  0.0407])\n",
      "------\n",
      "------\n",
      "Epoch 2527, Loss 2.932654, \n",
      "Params:  tensor([  5.3256, -17.0661])\n",
      "Grad:  tensor([-0.0072,  0.0406])\n",
      "------\n",
      "------\n",
      "Epoch 2528, Loss 2.932637, \n",
      "Params:  tensor([  5.3256, -17.0665])\n",
      "Grad:  tensor([-0.0072,  0.0405])\n",
      "------\n",
      "------\n",
      "Epoch 2529, Loss 2.932619, \n",
      "Params:  tensor([  5.3257, -17.0669])\n",
      "Grad:  tensor([-0.0072,  0.0405])\n",
      "------\n",
      "------\n",
      "Epoch 2530, Loss 2.932603, \n",
      "Params:  tensor([  5.3258, -17.0673])\n",
      "Grad:  tensor([-0.0071,  0.0404])\n",
      "------\n",
      "------\n",
      "Epoch 2531, Loss 2.932585, \n",
      "Params:  tensor([  5.3258, -17.0677])\n",
      "Grad:  tensor([-0.0071,  0.0403])\n",
      "------\n",
      "------\n",
      "Epoch 2532, Loss 2.932569, \n",
      "Params:  tensor([  5.3259, -17.0681])\n",
      "Grad:  tensor([-0.0071,  0.0403])\n",
      "------\n",
      "------\n",
      "Epoch 2533, Loss 2.932553, \n",
      "Params:  tensor([  5.3260, -17.0685])\n",
      "Grad:  tensor([-0.0071,  0.0402])\n",
      "------\n",
      "------\n",
      "Epoch 2534, Loss 2.932535, \n",
      "Params:  tensor([  5.3261, -17.0689])\n",
      "Grad:  tensor([-0.0071,  0.0401])\n",
      "------\n",
      "------\n",
      "Epoch 2535, Loss 2.932520, \n",
      "Params:  tensor([  5.3261, -17.0693])\n",
      "Grad:  tensor([-0.0071,  0.0401])\n",
      "------\n",
      "------\n",
      "Epoch 2536, Loss 2.932502, \n",
      "Params:  tensor([  5.3262, -17.0697])\n",
      "Grad:  tensor([-0.0071,  0.0400])\n",
      "------\n",
      "------\n",
      "Epoch 2537, Loss 2.932487, \n",
      "Params:  tensor([  5.3263, -17.0701])\n",
      "Grad:  tensor([-0.0071,  0.0399])\n",
      "------\n",
      "------\n",
      "Epoch 2538, Loss 2.932469, \n",
      "Params:  tensor([  5.3263, -17.0705])\n",
      "Grad:  tensor([-0.0070,  0.0399])\n",
      "------\n",
      "------\n",
      "Epoch 2539, Loss 2.932455, \n",
      "Params:  tensor([  5.3264, -17.0709])\n",
      "Grad:  tensor([-0.0070,  0.0398])\n",
      "------\n",
      "------\n",
      "Epoch 2540, Loss 2.932438, \n",
      "Params:  tensor([  5.3265, -17.0713])\n",
      "Grad:  tensor([-0.0070,  0.0397])\n",
      "------\n",
      "------\n",
      "Epoch 2541, Loss 2.932421, \n",
      "Params:  tensor([  5.3265, -17.0717])\n",
      "Grad:  tensor([-0.0070,  0.0397])\n",
      "------\n",
      "------\n",
      "Epoch 2542, Loss 2.932404, \n",
      "Params:  tensor([  5.3266, -17.0721])\n",
      "Grad:  tensor([-0.0070,  0.0396])\n",
      "------\n",
      "------\n",
      "Epoch 2543, Loss 2.932387, \n",
      "Params:  tensor([  5.3267, -17.0725])\n",
      "Grad:  tensor([-0.0070,  0.0395])\n",
      "------\n",
      "------\n",
      "Epoch 2544, Loss 2.932371, \n",
      "Params:  tensor([  5.3268, -17.0729])\n",
      "Grad:  tensor([-0.0070,  0.0395])\n",
      "------\n",
      "------\n",
      "Epoch 2545, Loss 2.932358, \n",
      "Params:  tensor([  5.3268, -17.0733])\n",
      "Grad:  tensor([-0.0070,  0.0394])\n",
      "------\n",
      "------\n",
      "Epoch 2546, Loss 2.932340, \n",
      "Params:  tensor([  5.3269, -17.0737])\n",
      "Grad:  tensor([-0.0069,  0.0393])\n",
      "------\n",
      "------\n",
      "Epoch 2547, Loss 2.932324, \n",
      "Params:  tensor([  5.3270, -17.0741])\n",
      "Grad:  tensor([-0.0069,  0.0393])\n",
      "------\n",
      "------\n",
      "Epoch 2548, Loss 2.932310, \n",
      "Params:  tensor([  5.3270, -17.0745])\n",
      "Grad:  tensor([-0.0069,  0.0392])\n",
      "------\n",
      "------\n",
      "Epoch 2549, Loss 2.932293, \n",
      "Params:  tensor([  5.3271, -17.0749])\n",
      "Grad:  tensor([-0.0069,  0.0391])\n",
      "------\n",
      "------\n",
      "Epoch 2550, Loss 2.932277, \n",
      "Params:  tensor([  5.3272, -17.0752])\n",
      "Grad:  tensor([-0.0069,  0.0391])\n",
      "------\n",
      "------\n",
      "Epoch 2551, Loss 2.932261, \n",
      "Params:  tensor([  5.3272, -17.0756])\n",
      "Grad:  tensor([-0.0069,  0.0390])\n",
      "------\n",
      "------\n",
      "Epoch 2552, Loss 2.932246, \n",
      "Params:  tensor([  5.3273, -17.0760])\n",
      "Grad:  tensor([-0.0069,  0.0389])\n",
      "------\n",
      "------\n",
      "Epoch 2553, Loss 2.932229, \n",
      "Params:  tensor([  5.3274, -17.0764])\n",
      "Grad:  tensor([-0.0069,  0.0389])\n",
      "------\n",
      "------\n",
      "Epoch 2554, Loss 2.932215, \n",
      "Params:  tensor([  5.3274, -17.0768])\n",
      "Grad:  tensor([-0.0069,  0.0388])\n",
      "------\n",
      "------\n",
      "Epoch 2555, Loss 2.932198, \n",
      "Params:  tensor([  5.3275, -17.0772])\n",
      "Grad:  tensor([-0.0068,  0.0387])\n",
      "------\n",
      "------\n",
      "Epoch 2556, Loss 2.932183, \n",
      "Params:  tensor([  5.3276, -17.0776])\n",
      "Grad:  tensor([-0.0068,  0.0387])\n",
      "------\n",
      "------\n",
      "Epoch 2557, Loss 2.932167, \n",
      "Params:  tensor([  5.3276, -17.0780])\n",
      "Grad:  tensor([-0.0068,  0.0386])\n",
      "------\n",
      "------\n",
      "Epoch 2558, Loss 2.932153, \n",
      "Params:  tensor([  5.3277, -17.0783])\n",
      "Grad:  tensor([-0.0068,  0.0385])\n",
      "------\n",
      "------\n",
      "Epoch 2559, Loss 2.932137, \n",
      "Params:  tensor([  5.3278, -17.0787])\n",
      "Grad:  tensor([-0.0068,  0.0385])\n",
      "------\n",
      "------\n",
      "Epoch 2560, Loss 2.932122, \n",
      "Params:  tensor([  5.3279, -17.0791])\n",
      "Grad:  tensor([-0.0068,  0.0384])\n",
      "------\n",
      "------\n",
      "Epoch 2561, Loss 2.932107, \n",
      "Params:  tensor([  5.3279, -17.0795])\n",
      "Grad:  tensor([-0.0068,  0.0383])\n",
      "------\n",
      "------\n",
      "Epoch 2562, Loss 2.932092, \n",
      "Params:  tensor([  5.3280, -17.0799])\n",
      "Grad:  tensor([-0.0068,  0.0383])\n",
      "------\n",
      "------\n",
      "Epoch 2563, Loss 2.932076, \n",
      "Params:  tensor([  5.3281, -17.0803])\n",
      "Grad:  tensor([-0.0067,  0.0382])\n",
      "------\n",
      "------\n",
      "Epoch 2564, Loss 2.932061, \n",
      "Params:  tensor([  5.3281, -17.0806])\n",
      "Grad:  tensor([-0.0067,  0.0381])\n",
      "------\n",
      "------\n",
      "Epoch 2565, Loss 2.932047, \n",
      "Params:  tensor([  5.3282, -17.0810])\n",
      "Grad:  tensor([-0.0067,  0.0381])\n",
      "------\n",
      "------\n",
      "Epoch 2566, Loss 2.932031, \n",
      "Params:  tensor([  5.3283, -17.0814])\n",
      "Grad:  tensor([-0.0067,  0.0380])\n",
      "------\n",
      "------\n",
      "Epoch 2567, Loss 2.932017, \n",
      "Params:  tensor([  5.3283, -17.0818])\n",
      "Grad:  tensor([-0.0067,  0.0379])\n",
      "------\n",
      "------\n",
      "Epoch 2568, Loss 2.932002, \n",
      "Params:  tensor([  5.3284, -17.0822])\n",
      "Grad:  tensor([-0.0067,  0.0379])\n",
      "------\n",
      "------\n",
      "Epoch 2569, Loss 2.931986, \n",
      "Params:  tensor([  5.3285, -17.0825])\n",
      "Grad:  tensor([-0.0067,  0.0378])\n",
      "------\n",
      "------\n",
      "Epoch 2570, Loss 2.931972, \n",
      "Params:  tensor([  5.3285, -17.0829])\n",
      "Grad:  tensor([-0.0067,  0.0378])\n",
      "------\n",
      "------\n",
      "Epoch 2571, Loss 2.931957, \n",
      "Params:  tensor([  5.3286, -17.0833])\n",
      "Grad:  tensor([-0.0067,  0.0377])\n",
      "------\n",
      "------\n",
      "Epoch 2572, Loss 2.931941, \n",
      "Params:  tensor([  5.3287, -17.0837])\n",
      "Grad:  tensor([-0.0067,  0.0376])\n",
      "------\n",
      "------\n",
      "Epoch 2573, Loss 2.931929, \n",
      "Params:  tensor([  5.3287, -17.0840])\n",
      "Grad:  tensor([-0.0066,  0.0376])\n",
      "------\n",
      "------\n",
      "Epoch 2574, Loss 2.931914, \n",
      "Params:  tensor([  5.3288, -17.0844])\n",
      "Grad:  tensor([-0.0066,  0.0375])\n",
      "------\n",
      "------\n",
      "Epoch 2575, Loss 2.931900, \n",
      "Params:  tensor([  5.3289, -17.0848])\n",
      "Grad:  tensor([-0.0066,  0.0374])\n",
      "------\n",
      "------\n",
      "Epoch 2576, Loss 2.931885, \n",
      "Params:  tensor([  5.3289, -17.0852])\n",
      "Grad:  tensor([-0.0066,  0.0374])\n",
      "------\n",
      "------\n",
      "Epoch 2577, Loss 2.931870, \n",
      "Params:  tensor([  5.3290, -17.0855])\n",
      "Grad:  tensor([-0.0066,  0.0373])\n",
      "------\n",
      "------\n",
      "Epoch 2578, Loss 2.931855, \n",
      "Params:  tensor([  5.3291, -17.0859])\n",
      "Grad:  tensor([-0.0066,  0.0372])\n",
      "------\n",
      "------\n",
      "Epoch 2579, Loss 2.931842, \n",
      "Params:  tensor([  5.3291, -17.0863])\n",
      "Grad:  tensor([-0.0066,  0.0372])\n",
      "------\n",
      "------\n",
      "Epoch 2580, Loss 2.931828, \n",
      "Params:  tensor([  5.3292, -17.0867])\n",
      "Grad:  tensor([-0.0066,  0.0371])\n",
      "------\n",
      "------\n",
      "Epoch 2581, Loss 2.931813, \n",
      "Params:  tensor([  5.3293, -17.0870])\n",
      "Grad:  tensor([-0.0065,  0.0371])\n",
      "------\n",
      "------\n",
      "Epoch 2582, Loss 2.931799, \n",
      "Params:  tensor([  5.3293, -17.0874])\n",
      "Grad:  tensor([-0.0065,  0.0370])\n",
      "------\n",
      "------\n",
      "Epoch 2583, Loss 2.931786, \n",
      "Params:  tensor([  5.3294, -17.0878])\n",
      "Grad:  tensor([-0.0065,  0.0369])\n",
      "------\n",
      "------\n",
      "Epoch 2584, Loss 2.931771, \n",
      "Params:  tensor([  5.3294, -17.0881])\n",
      "Grad:  tensor([-0.0065,  0.0369])\n",
      "------\n",
      "------\n",
      "Epoch 2585, Loss 2.931759, \n",
      "Params:  tensor([  5.3295, -17.0885])\n",
      "Grad:  tensor([-0.0065,  0.0368])\n",
      "------\n",
      "------\n",
      "Epoch 2586, Loss 2.931742, \n",
      "Params:  tensor([  5.3296, -17.0889])\n",
      "Grad:  tensor([-0.0065,  0.0367])\n",
      "------\n",
      "------\n",
      "Epoch 2587, Loss 2.931729, \n",
      "Params:  tensor([  5.3296, -17.0892])\n",
      "Grad:  tensor([-0.0065,  0.0367])\n",
      "------\n",
      "------\n",
      "Epoch 2588, Loss 2.931717, \n",
      "Params:  tensor([  5.3297, -17.0896])\n",
      "Grad:  tensor([-0.0065,  0.0366])\n",
      "------\n",
      "------\n",
      "Epoch 2589, Loss 2.931701, \n",
      "Params:  tensor([  5.3298, -17.0900])\n",
      "Grad:  tensor([-0.0065,  0.0366])\n",
      "------\n",
      "------\n",
      "Epoch 2590, Loss 2.931687, \n",
      "Params:  tensor([  5.3298, -17.0903])\n",
      "Grad:  tensor([-0.0065,  0.0365])\n",
      "------\n",
      "------\n",
      "Epoch 2591, Loss 2.931674, \n",
      "Params:  tensor([  5.3299, -17.0907])\n",
      "Grad:  tensor([-0.0064,  0.0364])\n",
      "------\n",
      "------\n",
      "Epoch 2592, Loss 2.931660, \n",
      "Params:  tensor([  5.3300, -17.0911])\n",
      "Grad:  tensor([-0.0064,  0.0364])\n",
      "------\n",
      "------\n",
      "Epoch 2593, Loss 2.931648, \n",
      "Params:  tensor([  5.3300, -17.0914])\n",
      "Grad:  tensor([-0.0064,  0.0363])\n",
      "------\n",
      "------\n",
      "Epoch 2594, Loss 2.931632, \n",
      "Params:  tensor([  5.3301, -17.0918])\n",
      "Grad:  tensor([-0.0064,  0.0362])\n",
      "------\n",
      "------\n",
      "Epoch 2595, Loss 2.931619, \n",
      "Params:  tensor([  5.3302, -17.0921])\n",
      "Grad:  tensor([-0.0064,  0.0362])\n",
      "------\n",
      "------\n",
      "Epoch 2596, Loss 2.931606, \n",
      "Params:  tensor([  5.3302, -17.0925])\n",
      "Grad:  tensor([-0.0064,  0.0361])\n",
      "------\n",
      "------\n",
      "Epoch 2597, Loss 2.931593, \n",
      "Params:  tensor([  5.3303, -17.0929])\n",
      "Grad:  tensor([-0.0064,  0.0361])\n",
      "------\n",
      "------\n",
      "Epoch 2598, Loss 2.931580, \n",
      "Params:  tensor([  5.3303, -17.0932])\n",
      "Grad:  tensor([-0.0064,  0.0360])\n",
      "------\n",
      "------\n",
      "Epoch 2599, Loss 2.931566, \n",
      "Params:  tensor([  5.3304, -17.0936])\n",
      "Grad:  tensor([-0.0064,  0.0359])\n",
      "------\n",
      "------\n",
      "Epoch 2600, Loss 2.931554, \n",
      "Params:  tensor([  5.3305, -17.0939])\n",
      "Grad:  tensor([-0.0064,  0.0359])\n",
      "------\n",
      "------\n",
      "Epoch 2601, Loss 2.931538, \n",
      "Params:  tensor([  5.3305, -17.0943])\n",
      "Grad:  tensor([-0.0063,  0.0358])\n",
      "------\n",
      "------\n",
      "Epoch 2602, Loss 2.931526, \n",
      "Params:  tensor([  5.3306, -17.0947])\n",
      "Grad:  tensor([-0.0063,  0.0358])\n",
      "------\n",
      "------\n",
      "Epoch 2603, Loss 2.931512, \n",
      "Params:  tensor([  5.3307, -17.0950])\n",
      "Grad:  tensor([-0.0063,  0.0357])\n",
      "------\n",
      "------\n",
      "Epoch 2604, Loss 2.931499, \n",
      "Params:  tensor([  5.3307, -17.0954])\n",
      "Grad:  tensor([-0.0063,  0.0356])\n",
      "------\n",
      "------\n",
      "Epoch 2605, Loss 2.931488, \n",
      "Params:  tensor([  5.3308, -17.0957])\n",
      "Grad:  tensor([-0.0063,  0.0356])\n",
      "------\n",
      "------\n",
      "Epoch 2606, Loss 2.931474, \n",
      "Params:  tensor([  5.3309, -17.0961])\n",
      "Grad:  tensor([-0.0063,  0.0355])\n",
      "------\n",
      "------\n",
      "Epoch 2607, Loss 2.931462, \n",
      "Params:  tensor([  5.3309, -17.0964])\n",
      "Grad:  tensor([-0.0062,  0.0355])\n",
      "------\n",
      "------\n",
      "Epoch 2608, Loss 2.931448, \n",
      "Params:  tensor([  5.3310, -17.0968])\n",
      "Grad:  tensor([-0.0062,  0.0354])\n",
      "------\n",
      "------\n",
      "Epoch 2609, Loss 2.931436, \n",
      "Params:  tensor([  5.3310, -17.0971])\n",
      "Grad:  tensor([-0.0062,  0.0353])\n",
      "------\n",
      "------\n",
      "Epoch 2610, Loss 2.931423, \n",
      "Params:  tensor([  5.3311, -17.0975])\n",
      "Grad:  tensor([-0.0062,  0.0353])\n",
      "------\n",
      "------\n",
      "Epoch 2611, Loss 2.931411, \n",
      "Params:  tensor([  5.3312, -17.0979])\n",
      "Grad:  tensor([-0.0062,  0.0352])\n",
      "------\n",
      "------\n",
      "Epoch 2612, Loss 2.931397, \n",
      "Params:  tensor([  5.3312, -17.0982])\n",
      "Grad:  tensor([-0.0062,  0.0352])\n",
      "------\n",
      "------\n",
      "Epoch 2613, Loss 2.931384, \n",
      "Params:  tensor([  5.3313, -17.0986])\n",
      "Grad:  tensor([-0.0062,  0.0351])\n",
      "------\n",
      "------\n",
      "Epoch 2614, Loss 2.931371, \n",
      "Params:  tensor([  5.3313, -17.0989])\n",
      "Grad:  tensor([-0.0062,  0.0350])\n",
      "------\n",
      "------\n",
      "Epoch 2615, Loss 2.931358, \n",
      "Params:  tensor([  5.3314, -17.0993])\n",
      "Grad:  tensor([-0.0062,  0.0350])\n",
      "------\n",
      "------\n",
      "Epoch 2616, Loss 2.931346, \n",
      "Params:  tensor([  5.3315, -17.0996])\n",
      "Grad:  tensor([-0.0062,  0.0349])\n",
      "------\n",
      "------\n",
      "Epoch 2617, Loss 2.931335, \n",
      "Params:  tensor([  5.3315, -17.1000])\n",
      "Grad:  tensor([-0.0062,  0.0349])\n",
      "------\n",
      "------\n",
      "Epoch 2618, Loss 2.931322, \n",
      "Params:  tensor([  5.3316, -17.1003])\n",
      "Grad:  tensor([-0.0062,  0.0348])\n",
      "------\n",
      "------\n",
      "Epoch 2619, Loss 2.931308, \n",
      "Params:  tensor([  5.3317, -17.1006])\n",
      "Grad:  tensor([-0.0061,  0.0347])\n",
      "------\n",
      "------\n",
      "Epoch 2620, Loss 2.931296, \n",
      "Params:  tensor([  5.3317, -17.1010])\n",
      "Grad:  tensor([-0.0061,  0.0347])\n",
      "------\n",
      "------\n",
      "Epoch 2621, Loss 2.931282, \n",
      "Params:  tensor([  5.3318, -17.1013])\n",
      "Grad:  tensor([-0.0061,  0.0346])\n",
      "------\n",
      "------\n",
      "Epoch 2622, Loss 2.931272, \n",
      "Params:  tensor([  5.3318, -17.1017])\n",
      "Grad:  tensor([-0.0061,  0.0346])\n",
      "------\n",
      "------\n",
      "Epoch 2623, Loss 2.931258, \n",
      "Params:  tensor([  5.3319, -17.1020])\n",
      "Grad:  tensor([-0.0061,  0.0345])\n",
      "------\n",
      "------\n",
      "Epoch 2624, Loss 2.931245, \n",
      "Params:  tensor([  5.3320, -17.1024])\n",
      "Grad:  tensor([-0.0061,  0.0344])\n",
      "------\n",
      "------\n",
      "Epoch 2625, Loss 2.931234, \n",
      "Params:  tensor([  5.3320, -17.1027])\n",
      "Grad:  tensor([-0.0061,  0.0344])\n",
      "------\n",
      "------\n",
      "Epoch 2626, Loss 2.931222, \n",
      "Params:  tensor([  5.3321, -17.1031])\n",
      "Grad:  tensor([-0.0061,  0.0343])\n",
      "------\n",
      "------\n",
      "Epoch 2627, Loss 2.931211, \n",
      "Params:  tensor([  5.3321, -17.1034])\n",
      "Grad:  tensor([-0.0060,  0.0343])\n",
      "------\n",
      "------\n",
      "Epoch 2628, Loss 2.931196, \n",
      "Params:  tensor([  5.3322, -17.1038])\n",
      "Grad:  tensor([-0.0060,  0.0342])\n",
      "------\n",
      "------\n",
      "Epoch 2629, Loss 2.931185, \n",
      "Params:  tensor([  5.3323, -17.1041])\n",
      "Grad:  tensor([-0.0060,  0.0342])\n",
      "------\n",
      "------\n",
      "Epoch 2630, Loss 2.931173, \n",
      "Params:  tensor([  5.3323, -17.1044])\n",
      "Grad:  tensor([-0.0060,  0.0341])\n",
      "------\n",
      "------\n",
      "Epoch 2631, Loss 2.931162, \n",
      "Params:  tensor([  5.3324, -17.1048])\n",
      "Grad:  tensor([-0.0060,  0.0340])\n",
      "------\n",
      "------\n",
      "Epoch 2632, Loss 2.931149, \n",
      "Params:  tensor([  5.3324, -17.1051])\n",
      "Grad:  tensor([-0.0060,  0.0340])\n",
      "------\n",
      "------\n",
      "Epoch 2633, Loss 2.931138, \n",
      "Params:  tensor([  5.3325, -17.1055])\n",
      "Grad:  tensor([-0.0060,  0.0339])\n",
      "------\n",
      "------\n",
      "Epoch 2634, Loss 2.931126, \n",
      "Params:  tensor([  5.3326, -17.1058])\n",
      "Grad:  tensor([-0.0060,  0.0339])\n",
      "------\n",
      "------\n",
      "Epoch 2635, Loss 2.931114, \n",
      "Params:  tensor([  5.3326, -17.1061])\n",
      "Grad:  tensor([-0.0060,  0.0338])\n",
      "------\n",
      "------\n",
      "Epoch 2636, Loss 2.931101, \n",
      "Params:  tensor([  5.3327, -17.1065])\n",
      "Grad:  tensor([-0.0060,  0.0337])\n",
      "------\n",
      "------\n",
      "Epoch 2637, Loss 2.931090, \n",
      "Params:  tensor([  5.3327, -17.1068])\n",
      "Grad:  tensor([-0.0059,  0.0337])\n",
      "------\n",
      "------\n",
      "Epoch 2638, Loss 2.931079, \n",
      "Params:  tensor([  5.3328, -17.1071])\n",
      "Grad:  tensor([-0.0059,  0.0336])\n",
      "------\n",
      "------\n",
      "Epoch 2639, Loss 2.931067, \n",
      "Params:  tensor([  5.3329, -17.1075])\n",
      "Grad:  tensor([-0.0059,  0.0336])\n",
      "------\n",
      "------\n",
      "Epoch 2640, Loss 2.931054, \n",
      "Params:  tensor([  5.3329, -17.1078])\n",
      "Grad:  tensor([-0.0059,  0.0335])\n",
      "------\n",
      "------\n",
      "Epoch 2641, Loss 2.931044, \n",
      "Params:  tensor([  5.3330, -17.1081])\n",
      "Grad:  tensor([-0.0059,  0.0335])\n",
      "------\n",
      "------\n",
      "Epoch 2642, Loss 2.931034, \n",
      "Params:  tensor([  5.3330, -17.1085])\n",
      "Grad:  tensor([-0.0059,  0.0334])\n",
      "------\n",
      "------\n",
      "Epoch 2643, Loss 2.931021, \n",
      "Params:  tensor([  5.3331, -17.1088])\n",
      "Grad:  tensor([-0.0059,  0.0333])\n",
      "------\n",
      "------\n",
      "Epoch 2644, Loss 2.931010, \n",
      "Params:  tensor([  5.3332, -17.1091])\n",
      "Grad:  tensor([-0.0059,  0.0333])\n",
      "------\n",
      "------\n",
      "Epoch 2645, Loss 2.930999, \n",
      "Params:  tensor([  5.3332, -17.1095])\n",
      "Grad:  tensor([-0.0059,  0.0332])\n",
      "------\n",
      "------\n",
      "Epoch 2646, Loss 2.930987, \n",
      "Params:  tensor([  5.3333, -17.1098])\n",
      "Grad:  tensor([-0.0059,  0.0332])\n",
      "------\n",
      "------\n",
      "Epoch 2647, Loss 2.930976, \n",
      "Params:  tensor([  5.3333, -17.1101])\n",
      "Grad:  tensor([-0.0059,  0.0331])\n",
      "------\n",
      "------\n",
      "Epoch 2648, Loss 2.930964, \n",
      "Params:  tensor([  5.3334, -17.1105])\n",
      "Grad:  tensor([-0.0059,  0.0331])\n",
      "------\n",
      "------\n",
      "Epoch 2649, Loss 2.930953, \n",
      "Params:  tensor([  5.3335, -17.1108])\n",
      "Grad:  tensor([-0.0058,  0.0330])\n",
      "------\n",
      "------\n",
      "Epoch 2650, Loss 2.930941, \n",
      "Params:  tensor([  5.3335, -17.1111])\n",
      "Grad:  tensor([-0.0058,  0.0330])\n",
      "------\n",
      "------\n",
      "Epoch 2651, Loss 2.930932, \n",
      "Params:  tensor([  5.3336, -17.1115])\n",
      "Grad:  tensor([-0.0058,  0.0329])\n",
      "------\n",
      "------\n",
      "Epoch 2652, Loss 2.930921, \n",
      "Params:  tensor([  5.3336, -17.1118])\n",
      "Grad:  tensor([-0.0058,  0.0328])\n",
      "------\n",
      "------\n",
      "Epoch 2653, Loss 2.930908, \n",
      "Params:  tensor([  5.3337, -17.1121])\n",
      "Grad:  tensor([-0.0058,  0.0328])\n",
      "------\n",
      "------\n",
      "Epoch 2654, Loss 2.930899, \n",
      "Params:  tensor([  5.3337, -17.1124])\n",
      "Grad:  tensor([-0.0058,  0.0327])\n",
      "------\n",
      "------\n",
      "Epoch 2655, Loss 2.930885, \n",
      "Params:  tensor([  5.3338, -17.1128])\n",
      "Grad:  tensor([-0.0058,  0.0327])\n",
      "------\n",
      "------\n",
      "Epoch 2656, Loss 2.930876, \n",
      "Params:  tensor([  5.3339, -17.1131])\n",
      "Grad:  tensor([-0.0058,  0.0326])\n",
      "------\n",
      "------\n",
      "Epoch 2657, Loss 2.930863, \n",
      "Params:  tensor([  5.3339, -17.1134])\n",
      "Grad:  tensor([-0.0057,  0.0326])\n",
      "------\n",
      "------\n",
      "Epoch 2658, Loss 2.930854, \n",
      "Params:  tensor([  5.3340, -17.1137])\n",
      "Grad:  tensor([-0.0057,  0.0325])\n",
      "------\n",
      "------\n",
      "Epoch 2659, Loss 2.930841, \n",
      "Params:  tensor([  5.3340, -17.1141])\n",
      "Grad:  tensor([-0.0057,  0.0325])\n",
      "------\n",
      "------\n",
      "Epoch 2660, Loss 2.930833, \n",
      "Params:  tensor([  5.3341, -17.1144])\n",
      "Grad:  tensor([-0.0057,  0.0324])\n",
      "------\n",
      "------\n",
      "Epoch 2661, Loss 2.930821, \n",
      "Params:  tensor([  5.3341, -17.1147])\n",
      "Grad:  tensor([-0.0057,  0.0323])\n",
      "------\n",
      "------\n",
      "Epoch 2662, Loss 2.930811, \n",
      "Params:  tensor([  5.3342, -17.1150])\n",
      "Grad:  tensor([-0.0057,  0.0323])\n",
      "------\n",
      "------\n",
      "Epoch 2663, Loss 2.930801, \n",
      "Params:  tensor([  5.3343, -17.1154])\n",
      "Grad:  tensor([-0.0057,  0.0322])\n",
      "------\n",
      "------\n",
      "Epoch 2664, Loss 2.930788, \n",
      "Params:  tensor([  5.3343, -17.1157])\n",
      "Grad:  tensor([-0.0057,  0.0322])\n",
      "------\n",
      "------\n",
      "Epoch 2665, Loss 2.930778, \n",
      "Params:  tensor([  5.3344, -17.1160])\n",
      "Grad:  tensor([-0.0057,  0.0321])\n",
      "------\n",
      "------\n",
      "Epoch 2666, Loss 2.930767, \n",
      "Params:  tensor([  5.3344, -17.1163])\n",
      "Grad:  tensor([-0.0057,  0.0321])\n",
      "------\n",
      "------\n",
      "Epoch 2667, Loss 2.930757, \n",
      "Params:  tensor([  5.3345, -17.1166])\n",
      "Grad:  tensor([-0.0057,  0.0320])\n",
      "------\n",
      "------\n",
      "Epoch 2668, Loss 2.930746, \n",
      "Params:  tensor([  5.3345, -17.1170])\n",
      "Grad:  tensor([-0.0056,  0.0320])\n",
      "------\n",
      "------\n",
      "Epoch 2669, Loss 2.930736, \n",
      "Params:  tensor([  5.3346, -17.1173])\n",
      "Grad:  tensor([-0.0056,  0.0319])\n",
      "------\n",
      "------\n",
      "Epoch 2670, Loss 2.930724, \n",
      "Params:  tensor([  5.3347, -17.1176])\n",
      "Grad:  tensor([-0.0056,  0.0319])\n",
      "------\n",
      "------\n",
      "Epoch 2671, Loss 2.930715, \n",
      "Params:  tensor([  5.3347, -17.1179])\n",
      "Grad:  tensor([-0.0056,  0.0318])\n",
      "------\n",
      "------\n",
      "Epoch 2672, Loss 2.930704, \n",
      "Params:  tensor([  5.3348, -17.1182])\n",
      "Grad:  tensor([-0.0056,  0.0317])\n",
      "------\n",
      "------\n",
      "Epoch 2673, Loss 2.930694, \n",
      "Params:  tensor([  5.3348, -17.1186])\n",
      "Grad:  tensor([-0.0056,  0.0317])\n",
      "------\n",
      "------\n",
      "Epoch 2674, Loss 2.930685, \n",
      "Params:  tensor([  5.3349, -17.1189])\n",
      "Grad:  tensor([-0.0056,  0.0316])\n",
      "------\n",
      "------\n",
      "Epoch 2675, Loss 2.930674, \n",
      "Params:  tensor([  5.3349, -17.1192])\n",
      "Grad:  tensor([-0.0056,  0.0316])\n",
      "------\n",
      "------\n",
      "Epoch 2676, Loss 2.930663, \n",
      "Params:  tensor([  5.3350, -17.1195])\n",
      "Grad:  tensor([-0.0056,  0.0315])\n",
      "------\n",
      "------\n",
      "Epoch 2677, Loss 2.930654, \n",
      "Params:  tensor([  5.3350, -17.1198])\n",
      "Grad:  tensor([-0.0056,  0.0315])\n",
      "------\n",
      "------\n",
      "Epoch 2678, Loss 2.930644, \n",
      "Params:  tensor([  5.3351, -17.1201])\n",
      "Grad:  tensor([-0.0055,  0.0314])\n",
      "------\n",
      "------\n",
      "Epoch 2679, Loss 2.930631, \n",
      "Params:  tensor([  5.3352, -17.1204])\n",
      "Grad:  tensor([-0.0055,  0.0314])\n",
      "------\n",
      "------\n",
      "Epoch 2680, Loss 2.930621, \n",
      "Params:  tensor([  5.3352, -17.1208])\n",
      "Grad:  tensor([-0.0055,  0.0313])\n",
      "------\n",
      "------\n",
      "Epoch 2681, Loss 2.930613, \n",
      "Params:  tensor([  5.3353, -17.1211])\n",
      "Grad:  tensor([-0.0055,  0.0313])\n",
      "------\n",
      "------\n",
      "Epoch 2682, Loss 2.930603, \n",
      "Params:  tensor([  5.3353, -17.1214])\n",
      "Grad:  tensor([-0.0055,  0.0312])\n",
      "------\n",
      "------\n",
      "Epoch 2683, Loss 2.930593, \n",
      "Params:  tensor([  5.3354, -17.1217])\n",
      "Grad:  tensor([-0.0055,  0.0312])\n",
      "------\n",
      "------\n",
      "Epoch 2684, Loss 2.930582, \n",
      "Params:  tensor([  5.3354, -17.1220])\n",
      "Grad:  tensor([-0.0055,  0.0311])\n",
      "------\n",
      "------\n",
      "Epoch 2685, Loss 2.930571, \n",
      "Params:  tensor([  5.3355, -17.1223])\n",
      "Grad:  tensor([-0.0055,  0.0310])\n",
      "------\n",
      "------\n",
      "Epoch 2686, Loss 2.930562, \n",
      "Params:  tensor([  5.3355, -17.1226])\n",
      "Grad:  tensor([-0.0055,  0.0310])\n",
      "------\n",
      "------\n",
      "Epoch 2687, Loss 2.930552, \n",
      "Params:  tensor([  5.3356, -17.1229])\n",
      "Grad:  tensor([-0.0055,  0.0309])\n",
      "------\n",
      "------\n",
      "Epoch 2688, Loss 2.930543, \n",
      "Params:  tensor([  5.3356, -17.1232])\n",
      "Grad:  tensor([-0.0055,  0.0309])\n",
      "------\n",
      "------\n",
      "Epoch 2689, Loss 2.930534, \n",
      "Params:  tensor([  5.3357, -17.1236])\n",
      "Grad:  tensor([-0.0055,  0.0308])\n",
      "------\n",
      "------\n",
      "Epoch 2690, Loss 2.930523, \n",
      "Params:  tensor([  5.3358, -17.1239])\n",
      "Grad:  tensor([-0.0054,  0.0308])\n",
      "------\n",
      "------\n",
      "Epoch 2691, Loss 2.930514, \n",
      "Params:  tensor([  5.3358, -17.1242])\n",
      "Grad:  tensor([-0.0054,  0.0307])\n",
      "------\n",
      "------\n",
      "Epoch 2692, Loss 2.930502, \n",
      "Params:  tensor([  5.3359, -17.1245])\n",
      "Grad:  tensor([-0.0054,  0.0307])\n",
      "------\n",
      "------\n",
      "Epoch 2693, Loss 2.930493, \n",
      "Params:  tensor([  5.3359, -17.1248])\n",
      "Grad:  tensor([-0.0054,  0.0306])\n",
      "------\n",
      "------\n",
      "Epoch 2694, Loss 2.930482, \n",
      "Params:  tensor([  5.3360, -17.1251])\n",
      "Grad:  tensor([-0.0054,  0.0306])\n",
      "------\n",
      "------\n",
      "Epoch 2695, Loss 2.930474, \n",
      "Params:  tensor([  5.3360, -17.1254])\n",
      "Grad:  tensor([-0.0054,  0.0305])\n",
      "------\n",
      "------\n",
      "Epoch 2696, Loss 2.930464, \n",
      "Params:  tensor([  5.3361, -17.1257])\n",
      "Grad:  tensor([-0.0054,  0.0305])\n",
      "------\n",
      "------\n",
      "Epoch 2697, Loss 2.930454, \n",
      "Params:  tensor([  5.3361, -17.1260])\n",
      "Grad:  tensor([-0.0054,  0.0304])\n",
      "------\n",
      "------\n",
      "Epoch 2698, Loss 2.930445, \n",
      "Params:  tensor([  5.3362, -17.1263])\n",
      "Grad:  tensor([-0.0054,  0.0304])\n",
      "------\n",
      "------\n",
      "Epoch 2699, Loss 2.930436, \n",
      "Params:  tensor([  5.3362, -17.1266])\n",
      "Grad:  tensor([-0.0054,  0.0303])\n",
      "------\n",
      "------\n",
      "Epoch 2700, Loss 2.930426, \n",
      "Params:  tensor([  5.3363, -17.1269])\n",
      "Grad:  tensor([-0.0054,  0.0303])\n",
      "------\n",
      "------\n",
      "Epoch 2701, Loss 2.930416, \n",
      "Params:  tensor([  5.3364, -17.1272])\n",
      "Grad:  tensor([-0.0054,  0.0302])\n",
      "------\n",
      "------\n",
      "Epoch 2702, Loss 2.930408, \n",
      "Params:  tensor([  5.3364, -17.1275])\n",
      "Grad:  tensor([-0.0053,  0.0302])\n",
      "------\n",
      "------\n",
      "Epoch 2703, Loss 2.930398, \n",
      "Params:  tensor([  5.3365, -17.1278])\n",
      "Grad:  tensor([-0.0053,  0.0301])\n",
      "------\n",
      "------\n",
      "Epoch 2704, Loss 2.930388, \n",
      "Params:  tensor([  5.3365, -17.1281])\n",
      "Grad:  tensor([-0.0053,  0.0301])\n",
      "------\n",
      "------\n",
      "Epoch 2705, Loss 2.930380, \n",
      "Params:  tensor([  5.3366, -17.1284])\n",
      "Grad:  tensor([-0.0053,  0.0300])\n",
      "------\n",
      "------\n",
      "Epoch 2706, Loss 2.930370, \n",
      "Params:  tensor([  5.3366, -17.1287])\n",
      "Grad:  tensor([-0.0053,  0.0300])\n",
      "------\n",
      "------\n",
      "Epoch 2707, Loss 2.930360, \n",
      "Params:  tensor([  5.3367, -17.1290])\n",
      "Grad:  tensor([-0.0053,  0.0299])\n",
      "------\n",
      "------\n",
      "Epoch 2708, Loss 2.930353, \n",
      "Params:  tensor([  5.3367, -17.1293])\n",
      "Grad:  tensor([-0.0053,  0.0299])\n",
      "------\n",
      "------\n",
      "Epoch 2709, Loss 2.930342, \n",
      "Params:  tensor([  5.3368, -17.1296])\n",
      "Grad:  tensor([-0.0053,  0.0298])\n",
      "------\n",
      "------\n",
      "Epoch 2710, Loss 2.930335, \n",
      "Params:  tensor([  5.3368, -17.1299])\n",
      "Grad:  tensor([-0.0053,  0.0298])\n",
      "------\n",
      "------\n",
      "Epoch 2711, Loss 2.930325, \n",
      "Params:  tensor([  5.3369, -17.1302])\n",
      "Grad:  tensor([-0.0053,  0.0297])\n",
      "------\n",
      "------\n",
      "Epoch 2712, Loss 2.930315, \n",
      "Params:  tensor([  5.3369, -17.1305])\n",
      "Grad:  tensor([-0.0053,  0.0297])\n",
      "------\n",
      "------\n",
      "Epoch 2713, Loss 2.930306, \n",
      "Params:  tensor([  5.3370, -17.1308])\n",
      "Grad:  tensor([-0.0052,  0.0296])\n",
      "------\n",
      "------\n",
      "Epoch 2714, Loss 2.930298, \n",
      "Params:  tensor([  5.3370, -17.1311])\n",
      "Grad:  tensor([-0.0052,  0.0296])\n",
      "------\n",
      "------\n",
      "Epoch 2715, Loss 2.930288, \n",
      "Params:  tensor([  5.3371, -17.1314])\n",
      "Grad:  tensor([-0.0052,  0.0295])\n",
      "------\n",
      "------\n",
      "Epoch 2716, Loss 2.930279, \n",
      "Params:  tensor([  5.3371, -17.1317])\n",
      "Grad:  tensor([-0.0052,  0.0295])\n",
      "------\n",
      "------\n",
      "Epoch 2717, Loss 2.930270, \n",
      "Params:  tensor([  5.3372, -17.1320])\n",
      "Grad:  tensor([-0.0052,  0.0294])\n",
      "------\n",
      "------\n",
      "Epoch 2718, Loss 2.930262, \n",
      "Params:  tensor([  5.3372, -17.1323])\n",
      "Grad:  tensor([-0.0052,  0.0294])\n",
      "------\n",
      "------\n",
      "Epoch 2719, Loss 2.930254, \n",
      "Params:  tensor([  5.3373, -17.1326])\n",
      "Grad:  tensor([-0.0052,  0.0293])\n",
      "------\n",
      "------\n",
      "Epoch 2720, Loss 2.930244, \n",
      "Params:  tensor([  5.3373, -17.1329])\n",
      "Grad:  tensor([-0.0052,  0.0293])\n",
      "------\n",
      "------\n",
      "Epoch 2721, Loss 2.930235, \n",
      "Params:  tensor([  5.3374, -17.1332])\n",
      "Grad:  tensor([-0.0052,  0.0292])\n",
      "------\n",
      "------\n",
      "Epoch 2722, Loss 2.930226, \n",
      "Params:  tensor([  5.3375, -17.1334])\n",
      "Grad:  tensor([-0.0052,  0.0292])\n",
      "------\n",
      "------\n",
      "Epoch 2723, Loss 2.930218, \n",
      "Params:  tensor([  5.3375, -17.1337])\n",
      "Grad:  tensor([-0.0051,  0.0291])\n",
      "------\n",
      "------\n",
      "Epoch 2724, Loss 2.930209, \n",
      "Params:  tensor([  5.3376, -17.1340])\n",
      "Grad:  tensor([-0.0051,  0.0291])\n",
      "------\n",
      "------\n",
      "Epoch 2725, Loss 2.930201, \n",
      "Params:  tensor([  5.3376, -17.1343])\n",
      "Grad:  tensor([-0.0051,  0.0290])\n",
      "------\n",
      "------\n",
      "Epoch 2726, Loss 2.930190, \n",
      "Params:  tensor([  5.3377, -17.1346])\n",
      "Grad:  tensor([-0.0051,  0.0290])\n",
      "------\n",
      "------\n",
      "Epoch 2727, Loss 2.930183, \n",
      "Params:  tensor([  5.3377, -17.1349])\n",
      "Grad:  tensor([-0.0051,  0.0289])\n",
      "------\n",
      "------\n",
      "Epoch 2728, Loss 2.930173, \n",
      "Params:  tensor([  5.3378, -17.1352])\n",
      "Grad:  tensor([-0.0051,  0.0289])\n",
      "------\n",
      "------\n",
      "Epoch 2729, Loss 2.930166, \n",
      "Params:  tensor([  5.3378, -17.1355])\n",
      "Grad:  tensor([-0.0051,  0.0288])\n",
      "------\n",
      "------\n",
      "Epoch 2730, Loss 2.930156, \n",
      "Params:  tensor([  5.3379, -17.1358])\n",
      "Grad:  tensor([-0.0051,  0.0288])\n",
      "------\n",
      "------\n",
      "Epoch 2731, Loss 2.930149, \n",
      "Params:  tensor([  5.3379, -17.1360])\n",
      "Grad:  tensor([-0.0051,  0.0287])\n",
      "------\n",
      "------\n",
      "Epoch 2732, Loss 2.930139, \n",
      "Params:  tensor([  5.3380, -17.1363])\n",
      "Grad:  tensor([-0.0051,  0.0287])\n",
      "------\n",
      "------\n",
      "Epoch 2733, Loss 2.930131, \n",
      "Params:  tensor([  5.3380, -17.1366])\n",
      "Grad:  tensor([-0.0050,  0.0286])\n",
      "------\n",
      "------\n",
      "Epoch 2734, Loss 2.930123, \n",
      "Params:  tensor([  5.3381, -17.1369])\n",
      "Grad:  tensor([-0.0050,  0.0286])\n",
      "------\n",
      "------\n",
      "Epoch 2735, Loss 2.930113, \n",
      "Params:  tensor([  5.3381, -17.1372])\n",
      "Grad:  tensor([-0.0050,  0.0285])\n",
      "------\n",
      "------\n",
      "Epoch 2736, Loss 2.930107, \n",
      "Params:  tensor([  5.3382, -17.1375])\n",
      "Grad:  tensor([-0.0051,  0.0285])\n",
      "------\n",
      "------\n",
      "Epoch 2737, Loss 2.930099, \n",
      "Params:  tensor([  5.3382, -17.1378])\n",
      "Grad:  tensor([-0.0050,  0.0284])\n",
      "------\n",
      "------\n",
      "Epoch 2738, Loss 2.930090, \n",
      "Params:  tensor([  5.3383, -17.1380])\n",
      "Grad:  tensor([-0.0050,  0.0284])\n",
      "------\n",
      "------\n",
      "Epoch 2739, Loss 2.930081, \n",
      "Params:  tensor([  5.3383, -17.1383])\n",
      "Grad:  tensor([-0.0050,  0.0283])\n",
      "------\n",
      "------\n",
      "Epoch 2740, Loss 2.930073, \n",
      "Params:  tensor([  5.3384, -17.1386])\n",
      "Grad:  tensor([-0.0050,  0.0283])\n",
      "------\n",
      "------\n",
      "Epoch 2741, Loss 2.930064, \n",
      "Params:  tensor([  5.3384, -17.1389])\n",
      "Grad:  tensor([-0.0050,  0.0282])\n",
      "------\n",
      "------\n",
      "Epoch 2742, Loss 2.930056, \n",
      "Params:  tensor([  5.3385, -17.1392])\n",
      "Grad:  tensor([-0.0050,  0.0282])\n",
      "------\n",
      "------\n",
      "Epoch 2743, Loss 2.930048, \n",
      "Params:  tensor([  5.3385, -17.1395])\n",
      "Grad:  tensor([-0.0050,  0.0281])\n",
      "------\n",
      "------\n",
      "Epoch 2744, Loss 2.930041, \n",
      "Params:  tensor([  5.3386, -17.1397])\n",
      "Grad:  tensor([-0.0050,  0.0281])\n",
      "------\n",
      "------\n",
      "Epoch 2745, Loss 2.930032, \n",
      "Params:  tensor([  5.3386, -17.1400])\n",
      "Grad:  tensor([-0.0050,  0.0280])\n",
      "------\n",
      "------\n",
      "Epoch 2746, Loss 2.930022, \n",
      "Params:  tensor([  5.3387, -17.1403])\n",
      "Grad:  tensor([-0.0050,  0.0280])\n",
      "------\n",
      "------\n",
      "Epoch 2747, Loss 2.930016, \n",
      "Params:  tensor([  5.3387, -17.1406])\n",
      "Grad:  tensor([-0.0049,  0.0279])\n",
      "------\n",
      "------\n",
      "Epoch 2748, Loss 2.930008, \n",
      "Params:  tensor([  5.3388, -17.1409])\n",
      "Grad:  tensor([-0.0049,  0.0279])\n",
      "------\n",
      "------\n",
      "Epoch 2749, Loss 2.930000, \n",
      "Params:  tensor([  5.3388, -17.1411])\n",
      "Grad:  tensor([-0.0049,  0.0279])\n",
      "------\n",
      "------\n",
      "Epoch 2750, Loss 2.929992, \n",
      "Params:  tensor([  5.3389, -17.1414])\n",
      "Grad:  tensor([-0.0049,  0.0278])\n",
      "------\n",
      "------\n",
      "Epoch 2751, Loss 2.929983, \n",
      "Params:  tensor([  5.3389, -17.1417])\n",
      "Grad:  tensor([-0.0049,  0.0278])\n",
      "------\n",
      "------\n",
      "Epoch 2752, Loss 2.929975, \n",
      "Params:  tensor([  5.3390, -17.1420])\n",
      "Grad:  tensor([-0.0049,  0.0277])\n",
      "------\n",
      "------\n",
      "Epoch 2753, Loss 2.929968, \n",
      "Params:  tensor([  5.3390, -17.1422])\n",
      "Grad:  tensor([-0.0049,  0.0277])\n",
      "------\n",
      "------\n",
      "Epoch 2754, Loss 2.929960, \n",
      "Params:  tensor([  5.3391, -17.1425])\n",
      "Grad:  tensor([-0.0049,  0.0276])\n",
      "------\n",
      "------\n",
      "Epoch 2755, Loss 2.929953, \n",
      "Params:  tensor([  5.3391, -17.1428])\n",
      "Grad:  tensor([-0.0049,  0.0276])\n",
      "------\n",
      "------\n",
      "Epoch 2756, Loss 2.929945, \n",
      "Params:  tensor([  5.3392, -17.1431])\n",
      "Grad:  tensor([-0.0049,  0.0275])\n",
      "------\n",
      "------\n",
      "Epoch 2757, Loss 2.929936, \n",
      "Params:  tensor([  5.3392, -17.1433])\n",
      "Grad:  tensor([-0.0049,  0.0275])\n",
      "------\n",
      "------\n",
      "Epoch 2758, Loss 2.929929, \n",
      "Params:  tensor([  5.3392, -17.1436])\n",
      "Grad:  tensor([-0.0049,  0.0274])\n",
      "------\n",
      "------\n",
      "Epoch 2759, Loss 2.929921, \n",
      "Params:  tensor([  5.3393, -17.1439])\n",
      "Grad:  tensor([-0.0048,  0.0274])\n",
      "------\n",
      "------\n",
      "Epoch 2760, Loss 2.929914, \n",
      "Params:  tensor([  5.3393, -17.1442])\n",
      "Grad:  tensor([-0.0049,  0.0273])\n",
      "------\n",
      "------\n",
      "Epoch 2761, Loss 2.929905, \n",
      "Params:  tensor([  5.3394, -17.1444])\n",
      "Grad:  tensor([-0.0048,  0.0273])\n",
      "------\n",
      "------\n",
      "Epoch 2762, Loss 2.929896, \n",
      "Params:  tensor([  5.3394, -17.1447])\n",
      "Grad:  tensor([-0.0048,  0.0272])\n",
      "------\n",
      "------\n",
      "Epoch 2763, Loss 2.929891, \n",
      "Params:  tensor([  5.3395, -17.1450])\n",
      "Grad:  tensor([-0.0048,  0.0272])\n",
      "------\n",
      "------\n",
      "Epoch 2764, Loss 2.929882, \n",
      "Params:  tensor([  5.3395, -17.1453])\n",
      "Grad:  tensor([-0.0048,  0.0271])\n",
      "------\n",
      "------\n",
      "Epoch 2765, Loss 2.929875, \n",
      "Params:  tensor([  5.3396, -17.1455])\n",
      "Grad:  tensor([-0.0048,  0.0271])\n",
      "------\n",
      "------\n",
      "Epoch 2766, Loss 2.929868, \n",
      "Params:  tensor([  5.3396, -17.1458])\n",
      "Grad:  tensor([-0.0048,  0.0271])\n",
      "------\n",
      "------\n",
      "Epoch 2767, Loss 2.929859, \n",
      "Params:  tensor([  5.3397, -17.1461])\n",
      "Grad:  tensor([-0.0048,  0.0270])\n",
      "------\n",
      "------\n",
      "Epoch 2768, Loss 2.929852, \n",
      "Params:  tensor([  5.3397, -17.1463])\n",
      "Grad:  tensor([-0.0048,  0.0270])\n",
      "------\n",
      "------\n",
      "Epoch 2769, Loss 2.929845, \n",
      "Params:  tensor([  5.3398, -17.1466])\n",
      "Grad:  tensor([-0.0048,  0.0269])\n",
      "------\n",
      "------\n",
      "Epoch 2770, Loss 2.929838, \n",
      "Params:  tensor([  5.3398, -17.1469])\n",
      "Grad:  tensor([-0.0047,  0.0269])\n",
      "------\n",
      "------\n",
      "Epoch 2771, Loss 2.929830, \n",
      "Params:  tensor([  5.3399, -17.1471])\n",
      "Grad:  tensor([-0.0047,  0.0268])\n",
      "------\n",
      "------\n",
      "Epoch 2772, Loss 2.929822, \n",
      "Params:  tensor([  5.3399, -17.1474])\n",
      "Grad:  tensor([-0.0047,  0.0268])\n",
      "------\n",
      "------\n",
      "Epoch 2773, Loss 2.929816, \n",
      "Params:  tensor([  5.3400, -17.1477])\n",
      "Grad:  tensor([-0.0047,  0.0267])\n",
      "------\n",
      "------\n",
      "Epoch 2774, Loss 2.929807, \n",
      "Params:  tensor([  5.3400, -17.1479])\n",
      "Grad:  tensor([-0.0047,  0.0267])\n",
      "------\n",
      "------\n",
      "Epoch 2775, Loss 2.929800, \n",
      "Params:  tensor([  5.3401, -17.1482])\n",
      "Grad:  tensor([-0.0047,  0.0266])\n",
      "------\n",
      "------\n",
      "Epoch 2776, Loss 2.929794, \n",
      "Params:  tensor([  5.3401, -17.1485])\n",
      "Grad:  tensor([-0.0047,  0.0266])\n",
      "------\n",
      "------\n",
      "Epoch 2777, Loss 2.929786, \n",
      "Params:  tensor([  5.3402, -17.1487])\n",
      "Grad:  tensor([-0.0047,  0.0266])\n",
      "------\n",
      "------\n",
      "Epoch 2778, Loss 2.929778, \n",
      "Params:  tensor([  5.3402, -17.1490])\n",
      "Grad:  tensor([-0.0047,  0.0265])\n",
      "------\n",
      "------\n",
      "Epoch 2779, Loss 2.929771, \n",
      "Params:  tensor([  5.3402, -17.1493])\n",
      "Grad:  tensor([-0.0047,  0.0265])\n",
      "------\n",
      "------\n",
      "Epoch 2780, Loss 2.929765, \n",
      "Params:  tensor([  5.3403, -17.1495])\n",
      "Grad:  tensor([-0.0047,  0.0264])\n",
      "------\n",
      "------\n",
      "Epoch 2781, Loss 2.929757, \n",
      "Params:  tensor([  5.3403, -17.1498])\n",
      "Grad:  tensor([-0.0047,  0.0264])\n",
      "------\n",
      "------\n",
      "Epoch 2782, Loss 2.929750, \n",
      "Params:  tensor([  5.3404, -17.1501])\n",
      "Grad:  tensor([-0.0046,  0.0263])\n",
      "------\n",
      "------\n",
      "Epoch 2783, Loss 2.929743, \n",
      "Params:  tensor([  5.3404, -17.1503])\n",
      "Grad:  tensor([-0.0046,  0.0263])\n",
      "------\n",
      "------\n",
      "Epoch 2784, Loss 2.929735, \n",
      "Params:  tensor([  5.3405, -17.1506])\n",
      "Grad:  tensor([-0.0046,  0.0262])\n",
      "------\n",
      "------\n",
      "Epoch 2785, Loss 2.929729, \n",
      "Params:  tensor([  5.3405, -17.1508])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.0047,  0.0262])\n",
      "------\n",
      "------\n",
      "Epoch 2786, Loss 2.929722, \n",
      "Params:  tensor([  5.3406, -17.1511])\n",
      "Grad:  tensor([-0.0046,  0.0262])\n",
      "------\n",
      "------\n",
      "Epoch 2787, Loss 2.929714, \n",
      "Params:  tensor([  5.3406, -17.1514])\n",
      "Grad:  tensor([-0.0046,  0.0261])\n",
      "------\n",
      "------\n",
      "Epoch 2788, Loss 2.929707, \n",
      "Params:  tensor([  5.3407, -17.1516])\n",
      "Grad:  tensor([-0.0046,  0.0261])\n",
      "------\n",
      "------\n",
      "Epoch 2789, Loss 2.929701, \n",
      "Params:  tensor([  5.3407, -17.1519])\n",
      "Grad:  tensor([-0.0046,  0.0260])\n",
      "------\n",
      "------\n",
      "Epoch 2790, Loss 2.929692, \n",
      "Params:  tensor([  5.3408, -17.1522])\n",
      "Grad:  tensor([-0.0046,  0.0260])\n",
      "------\n",
      "------\n",
      "Epoch 2791, Loss 2.929685, \n",
      "Params:  tensor([  5.3408, -17.1524])\n",
      "Grad:  tensor([-0.0046,  0.0259])\n",
      "------\n",
      "------\n",
      "Epoch 2792, Loss 2.929681, \n",
      "Params:  tensor([  5.3408, -17.1527])\n",
      "Grad:  tensor([-0.0046,  0.0259])\n",
      "------\n",
      "------\n",
      "Epoch 2793, Loss 2.929672, \n",
      "Params:  tensor([  5.3409, -17.1529])\n",
      "Grad:  tensor([-0.0046,  0.0258])\n",
      "------\n",
      "------\n",
      "Epoch 2794, Loss 2.929666, \n",
      "Params:  tensor([  5.3409, -17.1532])\n",
      "Grad:  tensor([-0.0046,  0.0258])\n",
      "------\n",
      "------\n",
      "Epoch 2795, Loss 2.929659, \n",
      "Params:  tensor([  5.3410, -17.1534])\n",
      "Grad:  tensor([-0.0045,  0.0258])\n",
      "------\n",
      "------\n",
      "Epoch 2796, Loss 2.929653, \n",
      "Params:  tensor([  5.3410, -17.1537])\n",
      "Grad:  tensor([-0.0045,  0.0257])\n",
      "------\n",
      "------\n",
      "Epoch 2797, Loss 2.929646, \n",
      "Params:  tensor([  5.3411, -17.1540])\n",
      "Grad:  tensor([-0.0045,  0.0257])\n",
      "------\n",
      "------\n",
      "Epoch 2798, Loss 2.929638, \n",
      "Params:  tensor([  5.3411, -17.1542])\n",
      "Grad:  tensor([-0.0045,  0.0256])\n",
      "------\n",
      "------\n",
      "Epoch 2799, Loss 2.929632, \n",
      "Params:  tensor([  5.3412, -17.1545])\n",
      "Grad:  tensor([-0.0045,  0.0256])\n",
      "------\n",
      "------\n",
      "Epoch 2800, Loss 2.929626, \n",
      "Params:  tensor([  5.3412, -17.1547])\n",
      "Grad:  tensor([-0.0045,  0.0255])\n",
      "------\n",
      "------\n",
      "Epoch 2801, Loss 2.929620, \n",
      "Params:  tensor([  5.3413, -17.1550])\n",
      "Grad:  tensor([-0.0045,  0.0255])\n",
      "------\n",
      "------\n",
      "Epoch 2802, Loss 2.929611, \n",
      "Params:  tensor([  5.3413, -17.1552])\n",
      "Grad:  tensor([-0.0045,  0.0254])\n",
      "------\n",
      "------\n",
      "Epoch 2803, Loss 2.929605, \n",
      "Params:  tensor([  5.3413, -17.1555])\n",
      "Grad:  tensor([-0.0045,  0.0254])\n",
      "------\n",
      "------\n",
      "Epoch 2804, Loss 2.929600, \n",
      "Params:  tensor([  5.3414, -17.1557])\n",
      "Grad:  tensor([-0.0045,  0.0254])\n",
      "------\n",
      "------\n",
      "Epoch 2805, Loss 2.929592, \n",
      "Params:  tensor([  5.3414, -17.1560])\n",
      "Grad:  tensor([-0.0045,  0.0253])\n",
      "------\n",
      "------\n",
      "Epoch 2806, Loss 2.929586, \n",
      "Params:  tensor([  5.3415, -17.1562])\n",
      "Grad:  tensor([-0.0045,  0.0253])\n",
      "------\n",
      "------\n",
      "Epoch 2807, Loss 2.929579, \n",
      "Params:  tensor([  5.3415, -17.1565])\n",
      "Grad:  tensor([-0.0045,  0.0252])\n",
      "------\n",
      "------\n",
      "Epoch 2808, Loss 2.929572, \n",
      "Params:  tensor([  5.3416, -17.1568])\n",
      "Grad:  tensor([-0.0044,  0.0252])\n",
      "------\n",
      "------\n",
      "Epoch 2809, Loss 2.929566, \n",
      "Params:  tensor([  5.3416, -17.1570])\n",
      "Grad:  tensor([-0.0044,  0.0251])\n",
      "------\n",
      "------\n",
      "Epoch 2810, Loss 2.929559, \n",
      "Params:  tensor([  5.3417, -17.1573])\n",
      "Grad:  tensor([-0.0044,  0.0251])\n",
      "------\n",
      "------\n",
      "Epoch 2811, Loss 2.929551, \n",
      "Params:  tensor([  5.3417, -17.1575])\n",
      "Grad:  tensor([-0.0044,  0.0251])\n",
      "------\n",
      "------\n",
      "Epoch 2812, Loss 2.929545, \n",
      "Params:  tensor([  5.3417, -17.1578])\n",
      "Grad:  tensor([-0.0044,  0.0250])\n",
      "------\n",
      "------\n",
      "Epoch 2813, Loss 2.929540, \n",
      "Params:  tensor([  5.3418, -17.1580])\n",
      "Grad:  tensor([-0.0044,  0.0250])\n",
      "------\n",
      "------\n",
      "Epoch 2814, Loss 2.929533, \n",
      "Params:  tensor([  5.3418, -17.1583])\n",
      "Grad:  tensor([-0.0044,  0.0249])\n",
      "------\n",
      "------\n",
      "Epoch 2815, Loss 2.929528, \n",
      "Params:  tensor([  5.3419, -17.1585])\n",
      "Grad:  tensor([-0.0044,  0.0249])\n",
      "------\n",
      "------\n",
      "Epoch 2816, Loss 2.929521, \n",
      "Params:  tensor([  5.3419, -17.1588])\n",
      "Grad:  tensor([-0.0044,  0.0249])\n",
      "------\n",
      "------\n",
      "Epoch 2817, Loss 2.929513, \n",
      "Params:  tensor([  5.3420, -17.1590])\n",
      "Grad:  tensor([-0.0044,  0.0248])\n",
      "------\n",
      "------\n",
      "Epoch 2818, Loss 2.929507, \n",
      "Params:  tensor([  5.3420, -17.1592])\n",
      "Grad:  tensor([-0.0043,  0.0248])\n",
      "------\n",
      "------\n",
      "Epoch 2819, Loss 2.929501, \n",
      "Params:  tensor([  5.3421, -17.1595])\n",
      "Grad:  tensor([-0.0044,  0.0247])\n",
      "------\n",
      "------\n",
      "Epoch 2820, Loss 2.929496, \n",
      "Params:  tensor([  5.3421, -17.1597])\n",
      "Grad:  tensor([-0.0044,  0.0247])\n",
      "------\n",
      "------\n",
      "Epoch 2821, Loss 2.929489, \n",
      "Params:  tensor([  5.3421, -17.1600])\n",
      "Grad:  tensor([-0.0044,  0.0246])\n",
      "------\n",
      "------\n",
      "Epoch 2822, Loss 2.929482, \n",
      "Params:  tensor([  5.3422, -17.1602])\n",
      "Grad:  tensor([-0.0043,  0.0246])\n",
      "------\n",
      "------\n",
      "Epoch 2823, Loss 2.929476, \n",
      "Params:  tensor([  5.3422, -17.1605])\n",
      "Grad:  tensor([-0.0043,  0.0246])\n",
      "------\n",
      "------\n",
      "Epoch 2824, Loss 2.929471, \n",
      "Params:  tensor([  5.3423, -17.1607])\n",
      "Grad:  tensor([-0.0043,  0.0245])\n",
      "------\n",
      "------\n",
      "Epoch 2825, Loss 2.929463, \n",
      "Params:  tensor([  5.3423, -17.1610])\n",
      "Grad:  tensor([-0.0043,  0.0245])\n",
      "------\n",
      "------\n",
      "Epoch 2826, Loss 2.929458, \n",
      "Params:  tensor([  5.3424, -17.1612])\n",
      "Grad:  tensor([-0.0043,  0.0244])\n",
      "------\n",
      "------\n",
      "Epoch 2827, Loss 2.929452, \n",
      "Params:  tensor([  5.3424, -17.1615])\n",
      "Grad:  tensor([-0.0043,  0.0244])\n",
      "------\n",
      "------\n",
      "Epoch 2828, Loss 2.929445, \n",
      "Params:  tensor([  5.3424, -17.1617])\n",
      "Grad:  tensor([-0.0043,  0.0243])\n",
      "------\n",
      "------\n",
      "Epoch 2829, Loss 2.929439, \n",
      "Params:  tensor([  5.3425, -17.1619])\n",
      "Grad:  tensor([-0.0043,  0.0243])\n",
      "------\n",
      "------\n",
      "Epoch 2830, Loss 2.929433, \n",
      "Params:  tensor([  5.3425, -17.1622])\n",
      "Grad:  tensor([-0.0043,  0.0243])\n",
      "------\n",
      "------\n",
      "Epoch 2831, Loss 2.929427, \n",
      "Params:  tensor([  5.3426, -17.1624])\n",
      "Grad:  tensor([-0.0043,  0.0242])\n",
      "------\n",
      "------\n",
      "Epoch 2832, Loss 2.929421, \n",
      "Params:  tensor([  5.3426, -17.1627])\n",
      "Grad:  tensor([-0.0043,  0.0242])\n",
      "------\n",
      "------\n",
      "Epoch 2833, Loss 2.929415, \n",
      "Params:  tensor([  5.3427, -17.1629])\n",
      "Grad:  tensor([-0.0043,  0.0241])\n",
      "------\n",
      "------\n",
      "Epoch 2834, Loss 2.929409, \n",
      "Params:  tensor([  5.3427, -17.1632])\n",
      "Grad:  tensor([-0.0043,  0.0241])\n",
      "------\n",
      "------\n",
      "Epoch 2835, Loss 2.929404, \n",
      "Params:  tensor([  5.3427, -17.1634])\n",
      "Grad:  tensor([-0.0043,  0.0241])\n",
      "------\n",
      "------\n",
      "Epoch 2836, Loss 2.929396, \n",
      "Params:  tensor([  5.3428, -17.1636])\n",
      "Grad:  tensor([-0.0042,  0.0240])\n",
      "------\n",
      "------\n",
      "Epoch 2837, Loss 2.929391, \n",
      "Params:  tensor([  5.3428, -17.1639])\n",
      "Grad:  tensor([-0.0042,  0.0240])\n",
      "------\n",
      "------\n",
      "Epoch 2838, Loss 2.929383, \n",
      "Params:  tensor([  5.3429, -17.1641])\n",
      "Grad:  tensor([-0.0042,  0.0239])\n",
      "------\n",
      "------\n",
      "Epoch 2839, Loss 2.929380, \n",
      "Params:  tensor([  5.3429, -17.1644])\n",
      "Grad:  tensor([-0.0042,  0.0239])\n",
      "------\n",
      "------\n",
      "Epoch 2840, Loss 2.929373, \n",
      "Params:  tensor([  5.3430, -17.1646])\n",
      "Grad:  tensor([-0.0042,  0.0239])\n",
      "------\n",
      "------\n",
      "Epoch 2841, Loss 2.929368, \n",
      "Params:  tensor([  5.3430, -17.1648])\n",
      "Grad:  tensor([-0.0042,  0.0238])\n",
      "------\n",
      "------\n",
      "Epoch 2842, Loss 2.929361, \n",
      "Params:  tensor([  5.3430, -17.1651])\n",
      "Grad:  tensor([-0.0042,  0.0238])\n",
      "------\n",
      "------\n",
      "Epoch 2843, Loss 2.929356, \n",
      "Params:  tensor([  5.3431, -17.1653])\n",
      "Grad:  tensor([-0.0042,  0.0237])\n",
      "------\n",
      "------\n",
      "Epoch 2844, Loss 2.929351, \n",
      "Params:  tensor([  5.3431, -17.1655])\n",
      "Grad:  tensor([-0.0042,  0.0237])\n",
      "------\n",
      "------\n",
      "Epoch 2845, Loss 2.929344, \n",
      "Params:  tensor([  5.3432, -17.1658])\n",
      "Grad:  tensor([-0.0042,  0.0237])\n",
      "------\n",
      "------\n",
      "Epoch 2846, Loss 2.929338, \n",
      "Params:  tensor([  5.3432, -17.1660])\n",
      "Grad:  tensor([-0.0042,  0.0236])\n",
      "------\n",
      "------\n",
      "Epoch 2847, Loss 2.929332, \n",
      "Params:  tensor([  5.3432, -17.1662])\n",
      "Grad:  tensor([-0.0042,  0.0236])\n",
      "------\n",
      "------\n",
      "Epoch 2848, Loss 2.929328, \n",
      "Params:  tensor([  5.3433, -17.1665])\n",
      "Grad:  tensor([-0.0042,  0.0235])\n",
      "------\n",
      "------\n",
      "Epoch 2849, Loss 2.929321, \n",
      "Params:  tensor([  5.3433, -17.1667])\n",
      "Grad:  tensor([-0.0041,  0.0235])\n",
      "------\n",
      "------\n",
      "Epoch 2850, Loss 2.929316, \n",
      "Params:  tensor([  5.3434, -17.1670])\n",
      "Grad:  tensor([-0.0041,  0.0235])\n",
      "------\n",
      "------\n",
      "Epoch 2851, Loss 2.929309, \n",
      "Params:  tensor([  5.3434, -17.1672])\n",
      "Grad:  tensor([-0.0041,  0.0234])\n",
      "------\n",
      "------\n",
      "Epoch 2852, Loss 2.929304, \n",
      "Params:  tensor([  5.3435, -17.1674])\n",
      "Grad:  tensor([-0.0041,  0.0234])\n",
      "------\n",
      "------\n",
      "Epoch 2853, Loss 2.929300, \n",
      "Params:  tensor([  5.3435, -17.1677])\n",
      "Grad:  tensor([-0.0041,  0.0233])\n",
      "------\n",
      "------\n",
      "Epoch 2854, Loss 2.929293, \n",
      "Params:  tensor([  5.3435, -17.1679])\n",
      "Grad:  tensor([-0.0041,  0.0233])\n",
      "------\n",
      "------\n",
      "Epoch 2855, Loss 2.929288, \n",
      "Params:  tensor([  5.3436, -17.1681])\n",
      "Grad:  tensor([-0.0041,  0.0233])\n",
      "------\n",
      "------\n",
      "Epoch 2856, Loss 2.929282, \n",
      "Params:  tensor([  5.3436, -17.1684])\n",
      "Grad:  tensor([-0.0041,  0.0232])\n",
      "------\n",
      "------\n",
      "Epoch 2857, Loss 2.929277, \n",
      "Params:  tensor([  5.3437, -17.1686])\n",
      "Grad:  tensor([-0.0041,  0.0232])\n",
      "------\n",
      "------\n",
      "Epoch 2858, Loss 2.929271, \n",
      "Params:  tensor([  5.3437, -17.1688])\n",
      "Grad:  tensor([-0.0041,  0.0231])\n",
      "------\n",
      "------\n",
      "Epoch 2859, Loss 2.929266, \n",
      "Params:  tensor([  5.3437, -17.1690])\n",
      "Grad:  tensor([-0.0041,  0.0231])\n",
      "------\n",
      "------\n",
      "Epoch 2860, Loss 2.929260, \n",
      "Params:  tensor([  5.3438, -17.1693])\n",
      "Grad:  tensor([-0.0041,  0.0231])\n",
      "------\n",
      "------\n",
      "Epoch 2861, Loss 2.929255, \n",
      "Params:  tensor([  5.3438, -17.1695])\n",
      "Grad:  tensor([-0.0041,  0.0230])\n",
      "------\n",
      "------\n",
      "Epoch 2862, Loss 2.929250, \n",
      "Params:  tensor([  5.3439, -17.1697])\n",
      "Grad:  tensor([-0.0041,  0.0230])\n",
      "------\n",
      "------\n",
      "Epoch 2863, Loss 2.929244, \n",
      "Params:  tensor([  5.3439, -17.1700])\n",
      "Grad:  tensor([-0.0040,  0.0229])\n",
      "------\n",
      "------\n",
      "Epoch 2864, Loss 2.929238, \n",
      "Params:  tensor([  5.3439, -17.1702])\n",
      "Grad:  tensor([-0.0040,  0.0229])\n",
      "------\n",
      "------\n",
      "Epoch 2865, Loss 2.929234, \n",
      "Params:  tensor([  5.3440, -17.1704])\n",
      "Grad:  tensor([-0.0040,  0.0229])\n",
      "------\n",
      "------\n",
      "Epoch 2866, Loss 2.929228, \n",
      "Params:  tensor([  5.3440, -17.1707])\n",
      "Grad:  tensor([-0.0040,  0.0228])\n",
      "------\n",
      "------\n",
      "Epoch 2867, Loss 2.929222, \n",
      "Params:  tensor([  5.3441, -17.1709])\n",
      "Grad:  tensor([-0.0040,  0.0228])\n",
      "------\n",
      "------\n",
      "Epoch 2868, Loss 2.929217, \n",
      "Params:  tensor([  5.3441, -17.1711])\n",
      "Grad:  tensor([-0.0040,  0.0227])\n",
      "------\n",
      "------\n",
      "Epoch 2869, Loss 2.929211, \n",
      "Params:  tensor([  5.3441, -17.1713])\n",
      "Grad:  tensor([-0.0040,  0.0227])\n",
      "------\n",
      "------\n",
      "Epoch 2870, Loss 2.929208, \n",
      "Params:  tensor([  5.3442, -17.1716])\n",
      "Grad:  tensor([-0.0040,  0.0227])\n",
      "------\n",
      "------\n",
      "Epoch 2871, Loss 2.929201, \n",
      "Params:  tensor([  5.3442, -17.1718])\n",
      "Grad:  tensor([-0.0040,  0.0226])\n",
      "------\n",
      "------\n",
      "Epoch 2872, Loss 2.929195, \n",
      "Params:  tensor([  5.3443, -17.1720])\n",
      "Grad:  tensor([-0.0040,  0.0226])\n",
      "------\n",
      "------\n",
      "Epoch 2873, Loss 2.929191, \n",
      "Params:  tensor([  5.3443, -17.1722])\n",
      "Grad:  tensor([-0.0040,  0.0226])\n",
      "------\n",
      "------\n",
      "Epoch 2874, Loss 2.929185, \n",
      "Params:  tensor([  5.3443, -17.1725])\n",
      "Grad:  tensor([-0.0040,  0.0225])\n",
      "------\n",
      "------\n",
      "Epoch 2875, Loss 2.929180, \n",
      "Params:  tensor([  5.3444, -17.1727])\n",
      "Grad:  tensor([-0.0040,  0.0225])\n",
      "------\n",
      "------\n",
      "Epoch 2876, Loss 2.929175, \n",
      "Params:  tensor([  5.3444, -17.1729])\n",
      "Grad:  tensor([-0.0040,  0.0224])\n",
      "------\n",
      "------\n",
      "Epoch 2877, Loss 2.929170, \n",
      "Params:  tensor([  5.3445, -17.1731])\n",
      "Grad:  tensor([-0.0040,  0.0224])\n",
      "------\n",
      "------\n",
      "Epoch 2878, Loss 2.929165, \n",
      "Params:  tensor([  5.3445, -17.1734])\n",
      "Grad:  tensor([-0.0040,  0.0224])\n",
      "------\n",
      "------\n",
      "Epoch 2879, Loss 2.929160, \n",
      "Params:  tensor([  5.3445, -17.1736])\n",
      "Grad:  tensor([-0.0039,  0.0223])\n",
      "------\n",
      "------\n",
      "Epoch 2880, Loss 2.929155, \n",
      "Params:  tensor([  5.3446, -17.1738])\n",
      "Grad:  tensor([-0.0039,  0.0223])\n",
      "------\n",
      "------\n",
      "Epoch 2881, Loss 2.929149, \n",
      "Params:  tensor([  5.3446, -17.1740])\n",
      "Grad:  tensor([-0.0039,  0.0223])\n",
      "------\n",
      "------\n",
      "Epoch 2882, Loss 2.929143, \n",
      "Params:  tensor([  5.3447, -17.1742])\n",
      "Grad:  tensor([-0.0039,  0.0222])\n",
      "------\n",
      "------\n",
      "Epoch 2883, Loss 2.929139, \n",
      "Params:  tensor([  5.3447, -17.1745])\n",
      "Grad:  tensor([-0.0039,  0.0222])\n",
      "------\n",
      "------\n",
      "Epoch 2884, Loss 2.929133, \n",
      "Params:  tensor([  5.3447, -17.1747])\n",
      "Grad:  tensor([-0.0039,  0.0221])\n",
      "------\n",
      "------\n",
      "Epoch 2885, Loss 2.929128, \n",
      "Params:  tensor([  5.3448, -17.1749])\n",
      "Grad:  tensor([-0.0039,  0.0221])\n",
      "------\n",
      "------\n",
      "Epoch 2886, Loss 2.929122, \n",
      "Params:  tensor([  5.3448, -17.1751])\n",
      "Grad:  tensor([-0.0039,  0.0221])\n",
      "------\n",
      "------\n",
      "Epoch 2887, Loss 2.929119, \n",
      "Params:  tensor([  5.3449, -17.1754])\n",
      "Grad:  tensor([-0.0039,  0.0220])\n",
      "------\n",
      "------\n",
      "Epoch 2888, Loss 2.929113, \n",
      "Params:  tensor([  5.3449, -17.1756])\n",
      "Grad:  tensor([-0.0039,  0.0220])\n",
      "------\n",
      "------\n",
      "Epoch 2889, Loss 2.929108, \n",
      "Params:  tensor([  5.3449, -17.1758])\n",
      "Grad:  tensor([-0.0039,  0.0220])\n",
      "------\n",
      "------\n",
      "Epoch 2890, Loss 2.929104, \n",
      "Params:  tensor([  5.3450, -17.1760])\n",
      "Grad:  tensor([-0.0039,  0.0219])\n",
      "------\n",
      "------\n",
      "Epoch 2891, Loss 2.929099, \n",
      "Params:  tensor([  5.3450, -17.1762])\n",
      "Grad:  tensor([-0.0039,  0.0219])\n",
      "------\n",
      "------\n",
      "Epoch 2892, Loss 2.929093, \n",
      "Params:  tensor([  5.3450, -17.1764])\n",
      "Grad:  tensor([-0.0039,  0.0218])\n",
      "------\n",
      "------\n",
      "Epoch 2893, Loss 2.929088, \n",
      "Params:  tensor([  5.3451, -17.1767])\n",
      "Grad:  tensor([-0.0039,  0.0218])\n",
      "------\n",
      "------\n",
      "Epoch 2894, Loss 2.929083, \n",
      "Params:  tensor([  5.3451, -17.1769])\n",
      "Grad:  tensor([-0.0038,  0.0218])\n",
      "------\n",
      "------\n",
      "Epoch 2895, Loss 2.929079, \n",
      "Params:  tensor([  5.3452, -17.1771])\n",
      "Grad:  tensor([-0.0038,  0.0217])\n",
      "------\n",
      "------\n",
      "Epoch 2896, Loss 2.929074, \n",
      "Params:  tensor([  5.3452, -17.1773])\n",
      "Grad:  tensor([-0.0038,  0.0217])\n",
      "------\n",
      "------\n",
      "Epoch 2897, Loss 2.929069, \n",
      "Params:  tensor([  5.3452, -17.1775])\n",
      "Grad:  tensor([-0.0038,  0.0217])\n",
      "------\n",
      "------\n",
      "Epoch 2898, Loss 2.929065, \n",
      "Params:  tensor([  5.3453, -17.1777])\n",
      "Grad:  tensor([-0.0038,  0.0216])\n",
      "------\n",
      "------\n",
      "Epoch 2899, Loss 2.929058, \n",
      "Params:  tensor([  5.3453, -17.1780])\n",
      "Grad:  tensor([-0.0038,  0.0216])\n",
      "------\n",
      "------\n",
      "Epoch 2900, Loss 2.929054, \n",
      "Params:  tensor([  5.3454, -17.1782])\n",
      "Grad:  tensor([-0.0038,  0.0215])\n",
      "------\n",
      "------\n",
      "Epoch 2901, Loss 2.929050, \n",
      "Params:  tensor([  5.3454, -17.1784])\n",
      "Grad:  tensor([-0.0038,  0.0215])\n",
      "------\n",
      "------\n",
      "Epoch 2902, Loss 2.929044, \n",
      "Params:  tensor([  5.3454, -17.1786])\n",
      "Grad:  tensor([-0.0038,  0.0215])\n",
      "------\n",
      "------\n",
      "Epoch 2903, Loss 2.929041, \n",
      "Params:  tensor([  5.3455, -17.1788])\n",
      "Grad:  tensor([-0.0038,  0.0214])\n",
      "------\n",
      "------\n",
      "Epoch 2904, Loss 2.929036, \n",
      "Params:  tensor([  5.3455, -17.1790])\n",
      "Grad:  tensor([-0.0038,  0.0214])\n",
      "------\n",
      "------\n",
      "Epoch 2905, Loss 2.929031, \n",
      "Params:  tensor([  5.3455, -17.1793])\n",
      "Grad:  tensor([-0.0038,  0.0214])\n",
      "------\n",
      "------\n",
      "Epoch 2906, Loss 2.929025, \n",
      "Params:  tensor([  5.3456, -17.1795])\n",
      "Grad:  tensor([-0.0038,  0.0213])\n",
      "------\n",
      "------\n",
      "Epoch 2907, Loss 2.929021, \n",
      "Params:  tensor([  5.3456, -17.1797])\n",
      "Grad:  tensor([-0.0038,  0.0213])\n",
      "------\n",
      "------\n",
      "Epoch 2908, Loss 2.929017, \n",
      "Params:  tensor([  5.3457, -17.1799])\n",
      "Grad:  tensor([-0.0037,  0.0213])\n",
      "------\n",
      "------\n",
      "Epoch 2909, Loss 2.929012, \n",
      "Params:  tensor([  5.3457, -17.1801])\n",
      "Grad:  tensor([-0.0037,  0.0212])\n",
      "------\n",
      "------\n",
      "Epoch 2910, Loss 2.929007, \n",
      "Params:  tensor([  5.3457, -17.1803])\n",
      "Grad:  tensor([-0.0037,  0.0212])\n",
      "------\n",
      "------\n",
      "Epoch 2911, Loss 2.929003, \n",
      "Params:  tensor([  5.3458, -17.1805])\n",
      "Grad:  tensor([-0.0037,  0.0211])\n",
      "------\n",
      "------\n",
      "Epoch 2912, Loss 2.928999, \n",
      "Params:  tensor([  5.3458, -17.1807])\n",
      "Grad:  tensor([-0.0037,  0.0211])\n",
      "------\n",
      "------\n",
      "Epoch 2913, Loss 2.928993, \n",
      "Params:  tensor([  5.3458, -17.1809])\n",
      "Grad:  tensor([-0.0037,  0.0211])\n",
      "------\n",
      "------\n",
      "Epoch 2914, Loss 2.928989, \n",
      "Params:  tensor([  5.3459, -17.1812])\n",
      "Grad:  tensor([-0.0037,  0.0210])\n",
      "------\n",
      "------\n",
      "Epoch 2915, Loss 2.928985, \n",
      "Params:  tensor([  5.3459, -17.1814])\n",
      "Grad:  tensor([-0.0037,  0.0210])\n",
      "------\n",
      "------\n",
      "Epoch 2916, Loss 2.928980, \n",
      "Params:  tensor([  5.3460, -17.1816])\n",
      "Grad:  tensor([-0.0037,  0.0210])\n",
      "------\n",
      "------\n",
      "Epoch 2917, Loss 2.928976, \n",
      "Params:  tensor([  5.3460, -17.1818])\n",
      "Grad:  tensor([-0.0037,  0.0209])\n",
      "------\n",
      "------\n",
      "Epoch 2918, Loss 2.928971, \n",
      "Params:  tensor([  5.3460, -17.1820])\n",
      "Grad:  tensor([-0.0037,  0.0209])\n",
      "------\n",
      "------\n",
      "Epoch 2919, Loss 2.928967, \n",
      "Params:  tensor([  5.3461, -17.1822])\n",
      "Grad:  tensor([-0.0037,  0.0209])\n",
      "------\n",
      "------\n",
      "Epoch 2920, Loss 2.928962, \n",
      "Params:  tensor([  5.3461, -17.1824])\n",
      "Grad:  tensor([-0.0037,  0.0208])\n",
      "------\n",
      "------\n",
      "Epoch 2921, Loss 2.928958, \n",
      "Params:  tensor([  5.3461, -17.1826])\n",
      "Grad:  tensor([-0.0037,  0.0208])\n",
      "------\n",
      "------\n",
      "Epoch 2922, Loss 2.928953, \n",
      "Params:  tensor([  5.3462, -17.1828])\n",
      "Grad:  tensor([-0.0037,  0.0208])\n",
      "------\n",
      "------\n",
      "Epoch 2923, Loss 2.928947, \n",
      "Params:  tensor([  5.3462, -17.1830])\n",
      "Grad:  tensor([-0.0036,  0.0207])\n",
      "------\n",
      "------\n",
      "Epoch 2924, Loss 2.928943, \n",
      "Params:  tensor([  5.3462, -17.1832])\n",
      "Grad:  tensor([-0.0037,  0.0207])\n",
      "------\n",
      "------\n",
      "Epoch 2925, Loss 2.928940, \n",
      "Params:  tensor([  5.3463, -17.1834])\n",
      "Grad:  tensor([-0.0036,  0.0206])\n",
      "------\n",
      "------\n",
      "Epoch 2926, Loss 2.928935, \n",
      "Params:  tensor([  5.3463, -17.1837])\n",
      "Grad:  tensor([-0.0036,  0.0206])\n",
      "------\n",
      "------\n",
      "Epoch 2927, Loss 2.928932, \n",
      "Params:  tensor([  5.3464, -17.1839])\n",
      "Grad:  tensor([-0.0036,  0.0206])\n",
      "------\n",
      "------\n",
      "Epoch 2928, Loss 2.928926, \n",
      "Params:  tensor([  5.3464, -17.1841])\n",
      "Grad:  tensor([-0.0036,  0.0205])\n",
      "------\n",
      "------\n",
      "Epoch 2929, Loss 2.928923, \n",
      "Params:  tensor([  5.3464, -17.1843])\n",
      "Grad:  tensor([-0.0036,  0.0205])\n",
      "------\n",
      "------\n",
      "Epoch 2930, Loss 2.928919, \n",
      "Params:  tensor([  5.3465, -17.1845])\n",
      "Grad:  tensor([-0.0036,  0.0205])\n",
      "------\n",
      "------\n",
      "Epoch 2931, Loss 2.928913, \n",
      "Params:  tensor([  5.3465, -17.1847])\n",
      "Grad:  tensor([-0.0036,  0.0204])\n",
      "------\n",
      "------\n",
      "Epoch 2932, Loss 2.928909, \n",
      "Params:  tensor([  5.3465, -17.1849])\n",
      "Grad:  tensor([-0.0036,  0.0204])\n",
      "------\n",
      "------\n",
      "Epoch 2933, Loss 2.928904, \n",
      "Params:  tensor([  5.3466, -17.1851])\n",
      "Grad:  tensor([-0.0036,  0.0204])\n",
      "------\n",
      "------\n",
      "Epoch 2934, Loss 2.928902, \n",
      "Params:  tensor([  5.3466, -17.1853])\n",
      "Grad:  tensor([-0.0036,  0.0203])\n",
      "------\n",
      "------\n",
      "Epoch 2935, Loss 2.928897, \n",
      "Params:  tensor([  5.3466, -17.1855])\n",
      "Grad:  tensor([-0.0036,  0.0203])\n",
      "------\n",
      "------\n",
      "Epoch 2936, Loss 2.928893, \n",
      "Params:  tensor([  5.3467, -17.1857])\n",
      "Grad:  tensor([-0.0036,  0.0203])\n",
      "------\n",
      "------\n",
      "Epoch 2937, Loss 2.928887, \n",
      "Params:  tensor([  5.3467, -17.1859])\n",
      "Grad:  tensor([-0.0036,  0.0202])\n",
      "------\n",
      "------\n",
      "Epoch 2938, Loss 2.928883, \n",
      "Params:  tensor([  5.3468, -17.1861])\n",
      "Grad:  tensor([-0.0035,  0.0202])\n",
      "------\n",
      "------\n",
      "Epoch 2939, Loss 2.928880, \n",
      "Params:  tensor([  5.3468, -17.1863])\n",
      "Grad:  tensor([-0.0036,  0.0202])\n",
      "------\n",
      "------\n",
      "Epoch 2940, Loss 2.928878, \n",
      "Params:  tensor([  5.3468, -17.1865])\n",
      "Grad:  tensor([-0.0036,  0.0201])\n",
      "------\n",
      "------\n",
      "Epoch 2941, Loss 2.928871, \n",
      "Params:  tensor([  5.3469, -17.1867])\n",
      "Grad:  tensor([-0.0035,  0.0201])\n",
      "------\n",
      "------\n",
      "Epoch 2942, Loss 2.928867, \n",
      "Params:  tensor([  5.3469, -17.1869])\n",
      "Grad:  tensor([-0.0035,  0.0201])\n",
      "------\n",
      "------\n",
      "Epoch 2943, Loss 2.928864, \n",
      "Params:  tensor([  5.3469, -17.1871])\n",
      "Grad:  tensor([-0.0035,  0.0200])\n",
      "------\n",
      "------\n",
      "Epoch 2944, Loss 2.928860, \n",
      "Params:  tensor([  5.3470, -17.1873])\n",
      "Grad:  tensor([-0.0035,  0.0200])\n",
      "------\n",
      "------\n",
      "Epoch 2945, Loss 2.928855, \n",
      "Params:  tensor([  5.3470, -17.1875])\n",
      "Grad:  tensor([-0.0035,  0.0200])\n",
      "------\n",
      "------\n",
      "Epoch 2946, Loss 2.928850, \n",
      "Params:  tensor([  5.3470, -17.1877])\n",
      "Grad:  tensor([-0.0035,  0.0199])\n",
      "------\n",
      "------\n",
      "Epoch 2947, Loss 2.928845, \n",
      "Params:  tensor([  5.3471, -17.1879])\n",
      "Grad:  tensor([-0.0035,  0.0199])\n",
      "------\n",
      "------\n",
      "Epoch 2948, Loss 2.928843, \n",
      "Params:  tensor([  5.3471, -17.1881])\n",
      "Grad:  tensor([-0.0035,  0.0199])\n",
      "------\n",
      "------\n",
      "Epoch 2949, Loss 2.928838, \n",
      "Params:  tensor([  5.3471, -17.1883])\n",
      "Grad:  tensor([-0.0035,  0.0198])\n",
      "------\n",
      "------\n",
      "Epoch 2950, Loss 2.928833, \n",
      "Params:  tensor([  5.3472, -17.1885])\n",
      "Grad:  tensor([-0.0035,  0.0198])\n",
      "------\n",
      "------\n",
      "Epoch 2951, Loss 2.928830, \n",
      "Params:  tensor([  5.3472, -17.1887])\n",
      "Grad:  tensor([-0.0035,  0.0198])\n",
      "------\n",
      "------\n",
      "Epoch 2952, Loss 2.928826, \n",
      "Params:  tensor([  5.3472, -17.1889])\n",
      "Grad:  tensor([-0.0035,  0.0197])\n",
      "------\n",
      "------\n",
      "Epoch 2953, Loss 2.928823, \n",
      "Params:  tensor([  5.3473, -17.1891])\n",
      "Grad:  tensor([-0.0035,  0.0197])\n",
      "------\n",
      "------\n",
      "Epoch 2954, Loss 2.928818, \n",
      "Params:  tensor([  5.3473, -17.1893])\n",
      "Grad:  tensor([-0.0035,  0.0197])\n",
      "------\n",
      "------\n",
      "Epoch 2955, Loss 2.928816, \n",
      "Params:  tensor([  5.3474, -17.1895])\n",
      "Grad:  tensor([-0.0035,  0.0196])\n",
      "------\n",
      "------\n",
      "Epoch 2956, Loss 2.928811, \n",
      "Params:  tensor([  5.3474, -17.1897])\n",
      "Grad:  tensor([-0.0035,  0.0196])\n",
      "------\n",
      "------\n",
      "Epoch 2957, Loss 2.928805, \n",
      "Params:  tensor([  5.3474, -17.1899])\n",
      "Grad:  tensor([-0.0034,  0.0196])\n",
      "------\n",
      "------\n",
      "Epoch 2958, Loss 2.928802, \n",
      "Params:  tensor([  5.3475, -17.1901])\n",
      "Grad:  tensor([-0.0035,  0.0195])\n",
      "------\n",
      "------\n",
      "Epoch 2959, Loss 2.928799, \n",
      "Params:  tensor([  5.3475, -17.1903])\n",
      "Grad:  tensor([-0.0034,  0.0195])\n",
      "------\n",
      "------\n",
      "Epoch 2960, Loss 2.928795, \n",
      "Params:  tensor([  5.3475, -17.1905])\n",
      "Grad:  tensor([-0.0034,  0.0195])\n",
      "------\n",
      "------\n",
      "Epoch 2961, Loss 2.928789, \n",
      "Params:  tensor([  5.3476, -17.1907])\n",
      "Grad:  tensor([-0.0034,  0.0194])\n",
      "------\n",
      "------\n",
      "Epoch 2962, Loss 2.928789, \n",
      "Params:  tensor([  5.3476, -17.1908])\n",
      "Grad:  tensor([-0.0034,  0.0194])\n",
      "------\n",
      "------\n",
      "Epoch 2963, Loss 2.928783, \n",
      "Params:  tensor([  5.3476, -17.1910])\n",
      "Grad:  tensor([-0.0034,  0.0194])\n",
      "------\n",
      "------\n",
      "Epoch 2964, Loss 2.928779, \n",
      "Params:  tensor([  5.3477, -17.1912])\n",
      "Grad:  tensor([-0.0034,  0.0193])\n",
      "------\n",
      "------\n",
      "Epoch 2965, Loss 2.928775, \n",
      "Params:  tensor([  5.3477, -17.1914])\n",
      "Grad:  tensor([-0.0034,  0.0193])\n",
      "------\n",
      "------\n",
      "Epoch 2966, Loss 2.928771, \n",
      "Params:  tensor([  5.3477, -17.1916])\n",
      "Grad:  tensor([-0.0034,  0.0193])\n",
      "------\n",
      "------\n",
      "Epoch 2967, Loss 2.928767, \n",
      "Params:  tensor([  5.3478, -17.1918])\n",
      "Grad:  tensor([-0.0034,  0.0192])\n",
      "------\n",
      "------\n",
      "Epoch 2968, Loss 2.928765, \n",
      "Params:  tensor([  5.3478, -17.1920])\n",
      "Grad:  tensor([-0.0034,  0.0192])\n",
      "------\n",
      "------\n",
      "Epoch 2969, Loss 2.928761, \n",
      "Params:  tensor([  5.3478, -17.1922])\n",
      "Grad:  tensor([-0.0034,  0.0192])\n",
      "------\n",
      "------\n",
      "Epoch 2970, Loss 2.928758, \n",
      "Params:  tensor([  5.3479, -17.1924])\n",
      "Grad:  tensor([-0.0034,  0.0191])\n",
      "------\n",
      "------\n",
      "Epoch 2971, Loss 2.928752, \n",
      "Params:  tensor([  5.3479, -17.1926])\n",
      "Grad:  tensor([-0.0034,  0.0191])\n",
      "------\n",
      "------\n",
      "Epoch 2972, Loss 2.928750, \n",
      "Params:  tensor([  5.3479, -17.1928])\n",
      "Grad:  tensor([-0.0034,  0.0191])\n",
      "------\n",
      "------\n",
      "Epoch 2973, Loss 2.928745, \n",
      "Params:  tensor([  5.3480, -17.1930])\n",
      "Grad:  tensor([-0.0034,  0.0190])\n",
      "------\n",
      "------\n",
      "Epoch 2974, Loss 2.928741, \n",
      "Params:  tensor([  5.3480, -17.1931])\n",
      "Grad:  tensor([-0.0034,  0.0190])\n",
      "------\n",
      "------\n",
      "Epoch 2975, Loss 2.928737, \n",
      "Params:  tensor([  5.3480, -17.1933])\n",
      "Grad:  tensor([-0.0034,  0.0190])\n",
      "------\n",
      "------\n",
      "Epoch 2976, Loss 2.928735, \n",
      "Params:  tensor([  5.3481, -17.1935])\n",
      "Grad:  tensor([-0.0033,  0.0189])\n",
      "------\n",
      "------\n",
      "Epoch 2977, Loss 2.928730, \n",
      "Params:  tensor([  5.3481, -17.1937])\n",
      "Grad:  tensor([-0.0033,  0.0189])\n",
      "------\n",
      "------\n",
      "Epoch 2978, Loss 2.928727, \n",
      "Params:  tensor([  5.3481, -17.1939])\n",
      "Grad:  tensor([-0.0033,  0.0189])\n",
      "------\n",
      "------\n",
      "Epoch 2979, Loss 2.928723, \n",
      "Params:  tensor([  5.3482, -17.1941])\n",
      "Grad:  tensor([-0.0033,  0.0188])\n",
      "------\n",
      "------\n",
      "Epoch 2980, Loss 2.928719, \n",
      "Params:  tensor([  5.3482, -17.1943])\n",
      "Grad:  tensor([-0.0033,  0.0188])\n",
      "------\n",
      "------\n",
      "Epoch 2981, Loss 2.928716, \n",
      "Params:  tensor([  5.3482, -17.1945])\n",
      "Grad:  tensor([-0.0033,  0.0188])\n",
      "------\n",
      "------\n",
      "Epoch 2982, Loss 2.928712, \n",
      "Params:  tensor([  5.3483, -17.1947])\n",
      "Grad:  tensor([-0.0033,  0.0187])\n",
      "------\n",
      "------\n",
      "Epoch 2983, Loss 2.928708, \n",
      "Params:  tensor([  5.3483, -17.1948])\n",
      "Grad:  tensor([-0.0033,  0.0187])\n",
      "------\n",
      "------\n",
      "Epoch 2984, Loss 2.928705, \n",
      "Params:  tensor([  5.3483, -17.1950])\n",
      "Grad:  tensor([-0.0033,  0.0187])\n",
      "------\n",
      "------\n",
      "Epoch 2985, Loss 2.928700, \n",
      "Params:  tensor([  5.3484, -17.1952])\n",
      "Grad:  tensor([-0.0033,  0.0186])\n",
      "------\n",
      "------\n",
      "Epoch 2986, Loss 2.928698, \n",
      "Params:  tensor([  5.3484, -17.1954])\n",
      "Grad:  tensor([-0.0033,  0.0186])\n",
      "------\n",
      "------\n",
      "Epoch 2987, Loss 2.928695, \n",
      "Params:  tensor([  5.3484, -17.1956])\n",
      "Grad:  tensor([-0.0033,  0.0186])\n",
      "------\n",
      "------\n",
      "Epoch 2988, Loss 2.928690, \n",
      "Params:  tensor([  5.3485, -17.1958])\n",
      "Grad:  tensor([-0.0033,  0.0186])\n",
      "------\n",
      "------\n",
      "Epoch 2989, Loss 2.928687, \n",
      "Params:  tensor([  5.3485, -17.1960])\n",
      "Grad:  tensor([-0.0033,  0.0185])\n",
      "------\n",
      "------\n",
      "Epoch 2990, Loss 2.928684, \n",
      "Params:  tensor([  5.3485, -17.1961])\n",
      "Grad:  tensor([-0.0033,  0.0185])\n",
      "------\n",
      "------\n",
      "Epoch 2991, Loss 2.928679, \n",
      "Params:  tensor([  5.3486, -17.1963])\n",
      "Grad:  tensor([-0.0032,  0.0185])\n",
      "------\n",
      "------\n",
      "Epoch 2992, Loss 2.928677, \n",
      "Params:  tensor([  5.3486, -17.1965])\n",
      "Grad:  tensor([-0.0033,  0.0184])\n",
      "------\n",
      "------\n",
      "Epoch 2993, Loss 2.928673, \n",
      "Params:  tensor([  5.3486, -17.1967])\n",
      "Grad:  tensor([-0.0033,  0.0184])\n",
      "------\n",
      "------\n",
      "Epoch 2994, Loss 2.928669, \n",
      "Params:  tensor([  5.3487, -17.1969])\n",
      "Grad:  tensor([-0.0033,  0.0184])\n",
      "------\n",
      "------\n",
      "Epoch 2995, Loss 2.928666, \n",
      "Params:  tensor([  5.3487, -17.1971])\n",
      "Grad:  tensor([-0.0032,  0.0183])\n",
      "------\n",
      "------\n",
      "Epoch 2996, Loss 2.928662, \n",
      "Params:  tensor([  5.3487, -17.1972])\n",
      "Grad:  tensor([-0.0032,  0.0183])\n",
      "------\n",
      "------\n",
      "Epoch 2997, Loss 2.928660, \n",
      "Params:  tensor([  5.3488, -17.1974])\n",
      "Grad:  tensor([-0.0032,  0.0183])\n",
      "------\n",
      "------\n",
      "Epoch 2998, Loss 2.928656, \n",
      "Params:  tensor([  5.3488, -17.1976])\n",
      "Grad:  tensor([-0.0032,  0.0182])\n",
      "------\n",
      "------\n",
      "Epoch 2999, Loss 2.928651, \n",
      "Params:  tensor([  5.3488, -17.1978])\n",
      "Grad:  tensor([-0.0032,  0.0182])\n",
      "------\n",
      "------\n",
      "Epoch 3000, Loss 2.928648, \n",
      "Params:  tensor([  5.3489, -17.1980])\n",
      "Grad:  tensor([-0.0032,  0.0182])\n",
      "------\n",
      "------\n",
      "Epoch 3001, Loss 2.928646, \n",
      "Params:  tensor([  5.3489, -17.1982])\n",
      "Grad:  tensor([-0.0032,  0.0181])\n",
      "------\n",
      "------\n",
      "Epoch 3002, Loss 2.928643, \n",
      "Params:  tensor([  5.3489, -17.1983])\n",
      "Grad:  tensor([-0.0032,  0.0181])\n",
      "------\n",
      "------\n",
      "Epoch 3003, Loss 2.928638, \n",
      "Params:  tensor([  5.3489, -17.1985])\n",
      "Grad:  tensor([-0.0032,  0.0181])\n",
      "------\n",
      "------\n",
      "Epoch 3004, Loss 2.928635, \n",
      "Params:  tensor([  5.3490, -17.1987])\n",
      "Grad:  tensor([-0.0032,  0.0181])\n",
      "------\n",
      "------\n",
      "Epoch 3005, Loss 2.928632, \n",
      "Params:  tensor([  5.3490, -17.1989])\n",
      "Grad:  tensor([-0.0032,  0.0180])\n",
      "------\n",
      "------\n",
      "Epoch 3006, Loss 2.928629, \n",
      "Params:  tensor([  5.3490, -17.1991])\n",
      "Grad:  tensor([-0.0032,  0.0180])\n",
      "------\n",
      "------\n",
      "Epoch 3007, Loss 2.928625, \n",
      "Params:  tensor([  5.3491, -17.1992])\n",
      "Grad:  tensor([-0.0032,  0.0180])\n",
      "------\n",
      "------\n",
      "Epoch 3008, Loss 2.928621, \n",
      "Params:  tensor([  5.3491, -17.1994])\n",
      "Grad:  tensor([-0.0032,  0.0179])\n",
      "------\n",
      "------\n",
      "Epoch 3009, Loss 2.928617, \n",
      "Params:  tensor([  5.3491, -17.1996])\n",
      "Grad:  tensor([-0.0032,  0.0179])\n",
      "------\n",
      "------\n",
      "Epoch 3010, Loss 2.928616, \n",
      "Params:  tensor([  5.3492, -17.1998])\n",
      "Grad:  tensor([-0.0032,  0.0179])\n",
      "------\n",
      "------\n",
      "Epoch 3011, Loss 2.928612, \n",
      "Params:  tensor([  5.3492, -17.2000])\n",
      "Grad:  tensor([-0.0032,  0.0178])\n",
      "------\n",
      "------\n",
      "Epoch 3012, Loss 2.928608, \n",
      "Params:  tensor([  5.3492, -17.2001])\n",
      "Grad:  tensor([-0.0032,  0.0178])\n",
      "------\n",
      "------\n",
      "Epoch 3013, Loss 2.928604, \n",
      "Params:  tensor([  5.3493, -17.2003])\n",
      "Grad:  tensor([-0.0031,  0.0178])\n",
      "------\n",
      "------\n",
      "Epoch 3014, Loss 2.928601, \n",
      "Params:  tensor([  5.3493, -17.2005])\n",
      "Grad:  tensor([-0.0031,  0.0177])\n",
      "------\n",
      "------\n",
      "Epoch 3015, Loss 2.928599, \n",
      "Params:  tensor([  5.3493, -17.2007])\n",
      "Grad:  tensor([-0.0031,  0.0177])\n",
      "------\n",
      "------\n",
      "Epoch 3016, Loss 2.928596, \n",
      "Params:  tensor([  5.3494, -17.2008])\n",
      "Grad:  tensor([-0.0031,  0.0177])\n",
      "------\n",
      "------\n",
      "Epoch 3017, Loss 2.928592, \n",
      "Params:  tensor([  5.3494, -17.2010])\n",
      "Grad:  tensor([-0.0031,  0.0177])\n",
      "------\n",
      "------\n",
      "Epoch 3018, Loss 2.928588, \n",
      "Params:  tensor([  5.3494, -17.2012])\n",
      "Grad:  tensor([-0.0031,  0.0176])\n",
      "------\n",
      "------\n",
      "Epoch 3019, Loss 2.928586, \n",
      "Params:  tensor([  5.3495, -17.2014])\n",
      "Grad:  tensor([-0.0031,  0.0176])\n",
      "------\n",
      "------\n",
      "Epoch 3020, Loss 2.928583, \n",
      "Params:  tensor([  5.3495, -17.2015])\n",
      "Grad:  tensor([-0.0031,  0.0176])\n",
      "------\n",
      "------\n",
      "Epoch 3021, Loss 2.928580, \n",
      "Params:  tensor([  5.3495, -17.2017])\n",
      "Grad:  tensor([-0.0031,  0.0175])\n",
      "------\n",
      "------\n",
      "Epoch 3022, Loss 2.928576, \n",
      "Params:  tensor([  5.3495, -17.2019])\n",
      "Grad:  tensor([-0.0031,  0.0175])\n",
      "------\n",
      "------\n",
      "Epoch 3023, Loss 2.928574, \n",
      "Params:  tensor([  5.3496, -17.2021])\n",
      "Grad:  tensor([-0.0031,  0.0175])\n",
      "------\n",
      "------\n",
      "Epoch 3024, Loss 2.928570, \n",
      "Params:  tensor([  5.3496, -17.2022])\n",
      "Grad:  tensor([-0.0031,  0.0175])\n",
      "------\n",
      "------\n",
      "Epoch 3025, Loss 2.928567, \n",
      "Params:  tensor([  5.3496, -17.2024])\n",
      "Grad:  tensor([-0.0031,  0.0174])\n",
      "------\n",
      "------\n",
      "Epoch 3026, Loss 2.928564, \n",
      "Params:  tensor([  5.3497, -17.2026])\n",
      "Grad:  tensor([-0.0031,  0.0174])\n",
      "------\n",
      "------\n",
      "Epoch 3027, Loss 2.928561, \n",
      "Params:  tensor([  5.3497, -17.2028])\n",
      "Grad:  tensor([-0.0030,  0.0174])\n",
      "------\n",
      "------\n",
      "Epoch 3028, Loss 2.928557, \n",
      "Params:  tensor([  5.3497, -17.2029])\n",
      "Grad:  tensor([-0.0031,  0.0173])\n",
      "------\n",
      "------\n",
      "Epoch 3029, Loss 2.928555, \n",
      "Params:  tensor([  5.3498, -17.2031])\n",
      "Grad:  tensor([-0.0031,  0.0173])\n",
      "------\n",
      "------\n",
      "Epoch 3030, Loss 2.928551, \n",
      "Params:  tensor([  5.3498, -17.2033])\n",
      "Grad:  tensor([-0.0031,  0.0173])\n",
      "------\n",
      "------\n",
      "Epoch 3031, Loss 2.928548, \n",
      "Params:  tensor([  5.3498, -17.2035])\n",
      "Grad:  tensor([-0.0031,  0.0172])\n",
      "------\n",
      "------\n",
      "Epoch 3032, Loss 2.928545, \n",
      "Params:  tensor([  5.3498, -17.2036])\n",
      "Grad:  tensor([-0.0030,  0.0172])\n",
      "------\n",
      "------\n",
      "Epoch 3033, Loss 2.928543, \n",
      "Params:  tensor([  5.3499, -17.2038])\n",
      "Grad:  tensor([-0.0030,  0.0172])\n",
      "------\n",
      "------\n",
      "Epoch 3034, Loss 2.928539, \n",
      "Params:  tensor([  5.3499, -17.2040])\n",
      "Grad:  tensor([-0.0030,  0.0172])\n",
      "------\n",
      "------\n",
      "Epoch 3035, Loss 2.928536, \n",
      "Params:  tensor([  5.3499, -17.2041])\n",
      "Grad:  tensor([-0.0030,  0.0171])\n",
      "------\n",
      "------\n",
      "Epoch 3036, Loss 2.928532, \n",
      "Params:  tensor([  5.3500, -17.2043])\n",
      "Grad:  tensor([-0.0030,  0.0171])\n",
      "------\n",
      "------\n",
      "Epoch 3037, Loss 2.928531, \n",
      "Params:  tensor([  5.3500, -17.2045])\n",
      "Grad:  tensor([-0.0030,  0.0171])\n",
      "------\n",
      "------\n",
      "Epoch 3038, Loss 2.928528, \n",
      "Params:  tensor([  5.3500, -17.2047])\n",
      "Grad:  tensor([-0.0030,  0.0170])\n",
      "------\n",
      "------\n",
      "Epoch 3039, Loss 2.928524, \n",
      "Params:  tensor([  5.3501, -17.2048])\n",
      "Grad:  tensor([-0.0030,  0.0170])\n",
      "------\n",
      "------\n",
      "Epoch 3040, Loss 2.928521, \n",
      "Params:  tensor([  5.3501, -17.2050])\n",
      "Grad:  tensor([-0.0030,  0.0170])\n",
      "------\n",
      "------\n",
      "Epoch 3041, Loss 2.928519, \n",
      "Params:  tensor([  5.3501, -17.2052])\n",
      "Grad:  tensor([-0.0030,  0.0170])\n",
      "------\n",
      "------\n",
      "Epoch 3042, Loss 2.928514, \n",
      "Params:  tensor([  5.3502, -17.2053])\n",
      "Grad:  tensor([-0.0030,  0.0169])\n",
      "------\n",
      "------\n",
      "Epoch 3043, Loss 2.928512, \n",
      "Params:  tensor([  5.3502, -17.2055])\n",
      "Grad:  tensor([-0.0030,  0.0169])\n",
      "------\n",
      "------\n",
      "Epoch 3044, Loss 2.928509, \n",
      "Params:  tensor([  5.3502, -17.2057])\n",
      "Grad:  tensor([-0.0030,  0.0169])\n",
      "------\n",
      "------\n",
      "Epoch 3045, Loss 2.928505, \n",
      "Params:  tensor([  5.3502, -17.2058])\n",
      "Grad:  tensor([-0.0030,  0.0168])\n",
      "------\n",
      "------\n",
      "Epoch 3046, Loss 2.928503, \n",
      "Params:  tensor([  5.3503, -17.2060])\n",
      "Grad:  tensor([-0.0030,  0.0168])\n",
      "------\n",
      "------\n",
      "Epoch 3047, Loss 2.928500, \n",
      "Params:  tensor([  5.3503, -17.2062])\n",
      "Grad:  tensor([-0.0030,  0.0168])\n",
      "------\n",
      "------\n",
      "Epoch 3048, Loss 2.928498, \n",
      "Params:  tensor([  5.3503, -17.2063])\n",
      "Grad:  tensor([-0.0030,  0.0168])\n",
      "------\n",
      "------\n",
      "Epoch 3049, Loss 2.928495, \n",
      "Params:  tensor([  5.3504, -17.2065])\n",
      "Grad:  tensor([-0.0030,  0.0167])\n",
      "------\n",
      "------\n",
      "Epoch 3050, Loss 2.928491, \n",
      "Params:  tensor([  5.3504, -17.2067])\n",
      "Grad:  tensor([-0.0030,  0.0167])\n",
      "------\n",
      "------\n",
      "Epoch 3051, Loss 2.928489, \n",
      "Params:  tensor([  5.3504, -17.2068])\n",
      "Grad:  tensor([-0.0030,  0.0167])\n",
      "------\n",
      "------\n",
      "Epoch 3052, Loss 2.928486, \n",
      "Params:  tensor([  5.3504, -17.2070])\n",
      "Grad:  tensor([-0.0029,  0.0166])\n",
      "------\n",
      "------\n",
      "Epoch 3053, Loss 2.928484, \n",
      "Params:  tensor([  5.3505, -17.2072])\n",
      "Grad:  tensor([-0.0029,  0.0166])\n",
      "------\n",
      "------\n",
      "Epoch 3054, Loss 2.928481, \n",
      "Params:  tensor([  5.3505, -17.2073])\n",
      "Grad:  tensor([-0.0029,  0.0166])\n",
      "------\n",
      "------\n",
      "Epoch 3055, Loss 2.928477, \n",
      "Params:  tensor([  5.3505, -17.2075])\n",
      "Grad:  tensor([-0.0029,  0.0165])\n",
      "------\n",
      "------\n",
      "Epoch 3056, Loss 2.928474, \n",
      "Params:  tensor([  5.3506, -17.2077])\n",
      "Grad:  tensor([-0.0029,  0.0165])\n",
      "------\n",
      "------\n",
      "Epoch 3057, Loss 2.928472, \n",
      "Params:  tensor([  5.3506, -17.2078])\n",
      "Grad:  tensor([-0.0029,  0.0165])\n",
      "------\n",
      "------\n",
      "Epoch 3058, Loss 2.928469, \n",
      "Params:  tensor([  5.3506, -17.2080])\n",
      "Grad:  tensor([-0.0029,  0.0165])\n",
      "------\n",
      "------\n",
      "Epoch 3059, Loss 2.928468, \n",
      "Params:  tensor([  5.3507, -17.2082])\n",
      "Grad:  tensor([-0.0029,  0.0164])\n",
      "------\n",
      "------\n",
      "Epoch 3060, Loss 2.928463, \n",
      "Params:  tensor([  5.3507, -17.2083])\n",
      "Grad:  tensor([-0.0029,  0.0164])\n",
      "------\n",
      "------\n",
      "Epoch 3061, Loss 2.928460, \n",
      "Params:  tensor([  5.3507, -17.2085])\n",
      "Grad:  tensor([-0.0029,  0.0164])\n",
      "------\n",
      "------\n",
      "Epoch 3062, Loss 2.928458, \n",
      "Params:  tensor([  5.3507, -17.2087])\n",
      "Grad:  tensor([-0.0029,  0.0164])\n",
      "------\n",
      "------\n",
      "Epoch 3063, Loss 2.928456, \n",
      "Params:  tensor([  5.3508, -17.2088])\n",
      "Grad:  tensor([-0.0029,  0.0163])\n",
      "------\n",
      "------\n",
      "Epoch 3064, Loss 2.928452, \n",
      "Params:  tensor([  5.3508, -17.2090])\n",
      "Grad:  tensor([-0.0029,  0.0163])\n",
      "------\n",
      "------\n",
      "Epoch 3065, Loss 2.928449, \n",
      "Params:  tensor([  5.3508, -17.2091])\n",
      "Grad:  tensor([-0.0029,  0.0163])\n",
      "------\n",
      "------\n",
      "Epoch 3066, Loss 2.928447, \n",
      "Params:  tensor([  5.3509, -17.2093])\n",
      "Grad:  tensor([-0.0029,  0.0162])\n",
      "------\n",
      "------\n",
      "Epoch 3067, Loss 2.928443, \n",
      "Params:  tensor([  5.3509, -17.2095])\n",
      "Grad:  tensor([-0.0029,  0.0162])\n",
      "------\n",
      "------\n",
      "Epoch 3068, Loss 2.928444, \n",
      "Params:  tensor([  5.3509, -17.2096])\n",
      "Grad:  tensor([-0.0029,  0.0162])\n",
      "------\n",
      "------\n",
      "Epoch 3069, Loss 2.928440, \n",
      "Params:  tensor([  5.3509, -17.2098])\n",
      "Grad:  tensor([-0.0029,  0.0162])\n",
      "------\n",
      "------\n",
      "Epoch 3070, Loss 2.928435, \n",
      "Params:  tensor([  5.3510, -17.2100])\n",
      "Grad:  tensor([-0.0029,  0.0161])\n",
      "------\n",
      "------\n",
      "Epoch 3071, Loss 2.928435, \n",
      "Params:  tensor([  5.3510, -17.2101])\n",
      "Grad:  tensor([-0.0029,  0.0161])\n",
      "------\n",
      "------\n",
      "Epoch 3072, Loss 2.928430, \n",
      "Params:  tensor([  5.3510, -17.2103])\n",
      "Grad:  tensor([-0.0028,  0.0161])\n",
      "------\n",
      "------\n",
      "Epoch 3073, Loss 2.928428, \n",
      "Params:  tensor([  5.3511, -17.2104])\n",
      "Grad:  tensor([-0.0028,  0.0161])\n",
      "------\n",
      "------\n",
      "Epoch 3074, Loss 2.928426, \n",
      "Params:  tensor([  5.3511, -17.2106])\n",
      "Grad:  tensor([-0.0028,  0.0160])\n",
      "------\n",
      "------\n",
      "Epoch 3075, Loss 2.928423, \n",
      "Params:  tensor([  5.3511, -17.2108])\n",
      "Grad:  tensor([-0.0028,  0.0160])\n",
      "------\n",
      "------\n",
      "Epoch 3076, Loss 2.928421, \n",
      "Params:  tensor([  5.3511, -17.2109])\n",
      "Grad:  tensor([-0.0028,  0.0160])\n",
      "------\n",
      "------\n",
      "Epoch 3077, Loss 2.928417, \n",
      "Params:  tensor([  5.3512, -17.2111])\n",
      "Grad:  tensor([-0.0028,  0.0159])\n",
      "------\n",
      "------\n",
      "Epoch 3078, Loss 2.928416, \n",
      "Params:  tensor([  5.3512, -17.2112])\n",
      "Grad:  tensor([-0.0028,  0.0159])\n",
      "------\n",
      "------\n",
      "Epoch 3079, Loss 2.928411, \n",
      "Params:  tensor([  5.3512, -17.2114])\n",
      "Grad:  tensor([-0.0028,  0.0159])\n",
      "------\n",
      "------\n",
      "Epoch 3080, Loss 2.928410, \n",
      "Params:  tensor([  5.3512, -17.2116])\n",
      "Grad:  tensor([-0.0028,  0.0159])\n",
      "------\n",
      "------\n",
      "Epoch 3081, Loss 2.928407, \n",
      "Params:  tensor([  5.3513, -17.2117])\n",
      "Grad:  tensor([-0.0028,  0.0158])\n",
      "------\n",
      "------\n",
      "Epoch 3082, Loss 2.928404, \n",
      "Params:  tensor([  5.3513, -17.2119])\n",
      "Grad:  tensor([-0.0028,  0.0158])\n",
      "------\n",
      "------\n",
      "Epoch 3083, Loss 2.928402, \n",
      "Params:  tensor([  5.3513, -17.2120])\n",
      "Grad:  tensor([-0.0028,  0.0158])\n",
      "------\n",
      "------\n",
      "Epoch 3084, Loss 2.928399, \n",
      "Params:  tensor([  5.3514, -17.2122])\n",
      "Grad:  tensor([-0.0028,  0.0158])\n",
      "------\n",
      "------\n",
      "Epoch 3085, Loss 2.928396, \n",
      "Params:  tensor([  5.3514, -17.2123])\n",
      "Grad:  tensor([-0.0028,  0.0157])\n",
      "------\n",
      "------\n",
      "Epoch 3086, Loss 2.928395, \n",
      "Params:  tensor([  5.3514, -17.2125])\n",
      "Grad:  tensor([-0.0028,  0.0157])\n",
      "------\n",
      "------\n",
      "Epoch 3087, Loss 2.928392, \n",
      "Params:  tensor([  5.3514, -17.2127])\n",
      "Grad:  tensor([-0.0027,  0.0157])\n",
      "------\n",
      "------\n",
      "Epoch 3088, Loss 2.928389, \n",
      "Params:  tensor([  5.3515, -17.2128])\n",
      "Grad:  tensor([-0.0027,  0.0157])\n",
      "------\n",
      "------\n",
      "Epoch 3089, Loss 2.928386, \n",
      "Params:  tensor([  5.3515, -17.2130])\n",
      "Grad:  tensor([-0.0027,  0.0156])\n",
      "------\n",
      "------\n",
      "Epoch 3090, Loss 2.928383, \n",
      "Params:  tensor([  5.3515, -17.2131])\n",
      "Grad:  tensor([-0.0028,  0.0156])\n",
      "------\n",
      "------\n",
      "Epoch 3091, Loss 2.928382, \n",
      "Params:  tensor([  5.3516, -17.2133])\n",
      "Grad:  tensor([-0.0028,  0.0156])\n",
      "------\n",
      "------\n",
      "Epoch 3092, Loss 2.928379, \n",
      "Params:  tensor([  5.3516, -17.2134])\n",
      "Grad:  tensor([-0.0027,  0.0155])\n",
      "------\n",
      "------\n",
      "Epoch 3093, Loss 2.928378, \n",
      "Params:  tensor([  5.3516, -17.2136])\n",
      "Grad:  tensor([-0.0027,  0.0155])\n",
      "------\n",
      "------\n",
      "Epoch 3094, Loss 2.928375, \n",
      "Params:  tensor([  5.3516, -17.2137])\n",
      "Grad:  tensor([-0.0027,  0.0155])\n",
      "------\n",
      "------\n",
      "Epoch 3095, Loss 2.928372, \n",
      "Params:  tensor([  5.3517, -17.2139])\n",
      "Grad:  tensor([-0.0027,  0.0155])\n",
      "------\n",
      "------\n",
      "Epoch 3096, Loss 2.928370, \n",
      "Params:  tensor([  5.3517, -17.2141])\n",
      "Grad:  tensor([-0.0027,  0.0154])\n",
      "------\n",
      "------\n",
      "Epoch 3097, Loss 2.928368, \n",
      "Params:  tensor([  5.3517, -17.2142])\n",
      "Grad:  tensor([-0.0027,  0.0154])\n",
      "------\n",
      "------\n",
      "Epoch 3098, Loss 2.928364, \n",
      "Params:  tensor([  5.3517, -17.2144])\n",
      "Grad:  tensor([-0.0027,  0.0154])\n",
      "------\n",
      "------\n",
      "Epoch 3099, Loss 2.928362, \n",
      "Params:  tensor([  5.3518, -17.2145])\n",
      "Grad:  tensor([-0.0027,  0.0154])\n",
      "------\n",
      "------\n",
      "Epoch 3100, Loss 2.928361, \n",
      "Params:  tensor([  5.3518, -17.2147])\n",
      "Grad:  tensor([-0.0027,  0.0153])\n",
      "------\n",
      "------\n",
      "Epoch 3101, Loss 2.928356, \n",
      "Params:  tensor([  5.3518, -17.2148])\n",
      "Grad:  tensor([-0.0027,  0.0153])\n",
      "------\n",
      "------\n",
      "Epoch 3102, Loss 2.928355, \n",
      "Params:  tensor([  5.3519, -17.2150])\n",
      "Grad:  tensor([-0.0027,  0.0153])\n",
      "------\n",
      "------\n",
      "Epoch 3103, Loss 2.928353, \n",
      "Params:  tensor([  5.3519, -17.2151])\n",
      "Grad:  tensor([-0.0027,  0.0153])\n",
      "------\n",
      "------\n",
      "Epoch 3104, Loss 2.928349, \n",
      "Params:  tensor([  5.3519, -17.2153])\n",
      "Grad:  tensor([-0.0027,  0.0152])\n",
      "------\n",
      "------\n",
      "Epoch 3105, Loss 2.928348, \n",
      "Params:  tensor([  5.3519, -17.2154])\n",
      "Grad:  tensor([-0.0027,  0.0152])\n",
      "------\n",
      "------\n",
      "Epoch 3106, Loss 2.928345, \n",
      "Params:  tensor([  5.3520, -17.2156])\n",
      "Grad:  tensor([-0.0027,  0.0152])\n",
      "------\n",
      "------\n",
      "Epoch 3107, Loss 2.928343, \n",
      "Params:  tensor([  5.3520, -17.2157])\n",
      "Grad:  tensor([-0.0027,  0.0152])\n",
      "------\n",
      "------\n",
      "Epoch 3108, Loss 2.928340, \n",
      "Params:  tensor([  5.3520, -17.2159])\n",
      "Grad:  tensor([-0.0027,  0.0151])\n",
      "------\n",
      "------\n",
      "Epoch 3109, Loss 2.928339, \n",
      "Params:  tensor([  5.3520, -17.2160])\n",
      "Grad:  tensor([-0.0027,  0.0151])\n",
      "------\n",
      "------\n",
      "Epoch 3110, Loss 2.928337, \n",
      "Params:  tensor([  5.3521, -17.2162])\n",
      "Grad:  tensor([-0.0027,  0.0151])\n",
      "------\n",
      "------\n",
      "Epoch 3111, Loss 2.928333, \n",
      "Params:  tensor([  5.3521, -17.2163])\n",
      "Grad:  tensor([-0.0027,  0.0151])\n",
      "------\n",
      "------\n",
      "Epoch 3112, Loss 2.928332, \n",
      "Params:  tensor([  5.3521, -17.2165])\n",
      "Grad:  tensor([-0.0027,  0.0150])\n",
      "------\n",
      "------\n",
      "Epoch 3113, Loss 2.928328, \n",
      "Params:  tensor([  5.3521, -17.2166])\n",
      "Grad:  tensor([-0.0026,  0.0150])\n",
      "------\n",
      "------\n",
      "Epoch 3114, Loss 2.928329, \n",
      "Params:  tensor([  5.3522, -17.2168])\n",
      "Grad:  tensor([-0.0027,  0.0150])\n",
      "------\n",
      "------\n",
      "Epoch 3115, Loss 2.928324, \n",
      "Params:  tensor([  5.3522, -17.2169])\n",
      "Grad:  tensor([-0.0026,  0.0149])\n",
      "------\n",
      "------\n",
      "Epoch 3116, Loss 2.928323, \n",
      "Params:  tensor([  5.3522, -17.2171])\n",
      "Grad:  tensor([-0.0026,  0.0149])\n",
      "------\n",
      "------\n",
      "Epoch 3117, Loss 2.928320, \n",
      "Params:  tensor([  5.3523, -17.2172])\n",
      "Grad:  tensor([-0.0026,  0.0149])\n",
      "------\n",
      "------\n",
      "Epoch 3118, Loss 2.928319, \n",
      "Params:  tensor([  5.3523, -17.2174])\n",
      "Grad:  tensor([-0.0026,  0.0149])\n",
      "------\n",
      "------\n",
      "Epoch 3119, Loss 2.928315, \n",
      "Params:  tensor([  5.3523, -17.2175])\n",
      "Grad:  tensor([-0.0026,  0.0148])\n",
      "------\n",
      "------\n",
      "Epoch 3120, Loss 2.928313, \n",
      "Params:  tensor([  5.3523, -17.2177])\n",
      "Grad:  tensor([-0.0026,  0.0148])\n",
      "------\n",
      "------\n",
      "Epoch 3121, Loss 2.928310, \n",
      "Params:  tensor([  5.3524, -17.2178])\n",
      "Grad:  tensor([-0.0026,  0.0148])\n",
      "------\n",
      "------\n",
      "Epoch 3122, Loss 2.928308, \n",
      "Params:  tensor([  5.3524, -17.2180])\n",
      "Grad:  tensor([-0.0026,  0.0148])\n",
      "------\n",
      "------\n",
      "Epoch 3123, Loss 2.928306, \n",
      "Params:  tensor([  5.3524, -17.2181])\n",
      "Grad:  tensor([-0.0026,  0.0147])\n",
      "------\n",
      "------\n",
      "Epoch 3124, Loss 2.928304, \n",
      "Params:  tensor([  5.3524, -17.2183])\n",
      "Grad:  tensor([-0.0026,  0.0147])\n",
      "------\n",
      "------\n",
      "Epoch 3125, Loss 2.928303, \n",
      "Params:  tensor([  5.3525, -17.2184])\n",
      "Grad:  tensor([-0.0026,  0.0147])\n",
      "------\n",
      "------\n",
      "Epoch 3126, Loss 2.928299, \n",
      "Params:  tensor([  5.3525, -17.2186])\n",
      "Grad:  tensor([-0.0026,  0.0147])\n",
      "------\n",
      "------\n",
      "Epoch 3127, Loss 2.928296, \n",
      "Params:  tensor([  5.3525, -17.2187])\n",
      "Grad:  tensor([-0.0026,  0.0146])\n",
      "------\n",
      "------\n",
      "Epoch 3128, Loss 2.928295, \n",
      "Params:  tensor([  5.3525, -17.2189])\n",
      "Grad:  tensor([-0.0026,  0.0146])\n",
      "------\n",
      "------\n",
      "Epoch 3129, Loss 2.928293, \n",
      "Params:  tensor([  5.3526, -17.2190])\n",
      "Grad:  tensor([-0.0026,  0.0146])\n",
      "------\n",
      "------\n",
      "Epoch 3130, Loss 2.928291, \n",
      "Params:  tensor([  5.3526, -17.2192])\n",
      "Grad:  tensor([-0.0026,  0.0146])\n",
      "------\n",
      "------\n",
      "Epoch 3131, Loss 2.928288, \n",
      "Params:  tensor([  5.3526, -17.2193])\n",
      "Grad:  tensor([-0.0026,  0.0145])\n",
      "------\n",
      "------\n",
      "Epoch 3132, Loss 2.928287, \n",
      "Params:  tensor([  5.3526, -17.2194])\n",
      "Grad:  tensor([-0.0026,  0.0145])\n",
      "------\n",
      "------\n",
      "Epoch 3133, Loss 2.928285, \n",
      "Params:  tensor([  5.3527, -17.2196])\n",
      "Grad:  tensor([-0.0025,  0.0145])\n",
      "------\n",
      "------\n",
      "Epoch 3134, Loss 2.928282, \n",
      "Params:  tensor([  5.3527, -17.2197])\n",
      "Grad:  tensor([-0.0026,  0.0145])\n",
      "------\n",
      "------\n",
      "Epoch 3135, Loss 2.928280, \n",
      "Params:  tensor([  5.3527, -17.2199])\n",
      "Grad:  tensor([-0.0026,  0.0144])\n",
      "------\n",
      "------\n",
      "Epoch 3136, Loss 2.928276, \n",
      "Params:  tensor([  5.3527, -17.2200])\n",
      "Grad:  tensor([-0.0025,  0.0144])\n",
      "------\n",
      "------\n",
      "Epoch 3137, Loss 2.928275, \n",
      "Params:  tensor([  5.3528, -17.2202])\n",
      "Grad:  tensor([-0.0026,  0.0144])\n",
      "------\n",
      "------\n",
      "Epoch 3138, Loss 2.928273, \n",
      "Params:  tensor([  5.3528, -17.2203])\n",
      "Grad:  tensor([-0.0025,  0.0144])\n",
      "------\n",
      "------\n",
      "Epoch 3139, Loss 2.928271, \n",
      "Params:  tensor([  5.3528, -17.2205])\n",
      "Grad:  tensor([-0.0025,  0.0144])\n",
      "------\n",
      "------\n",
      "Epoch 3140, Loss 2.928268, \n",
      "Params:  tensor([  5.3528, -17.2206])\n",
      "Grad:  tensor([-0.0025,  0.0143])\n",
      "------\n",
      "------\n",
      "Epoch 3141, Loss 2.928267, \n",
      "Params:  tensor([  5.3529, -17.2207])\n",
      "Grad:  tensor([-0.0025,  0.0143])\n",
      "------\n",
      "------\n",
      "Epoch 3142, Loss 2.928264, \n",
      "Params:  tensor([  5.3529, -17.2209])\n",
      "Grad:  tensor([-0.0025,  0.0143])\n",
      "------\n",
      "------\n",
      "Epoch 3143, Loss 2.928263, \n",
      "Params:  tensor([  5.3529, -17.2210])\n",
      "Grad:  tensor([-0.0025,  0.0143])\n",
      "------\n",
      "------\n",
      "Epoch 3144, Loss 2.928260, \n",
      "Params:  tensor([  5.3529, -17.2212])\n",
      "Grad:  tensor([-0.0025,  0.0142])\n",
      "------\n",
      "------\n",
      "Epoch 3145, Loss 2.928259, \n",
      "Params:  tensor([  5.3530, -17.2213])\n",
      "Grad:  tensor([-0.0025,  0.0142])\n",
      "------\n",
      "------\n",
      "Epoch 3146, Loss 2.928256, \n",
      "Params:  tensor([  5.3530, -17.2214])\n",
      "Grad:  tensor([-0.0025,  0.0142])\n",
      "------\n",
      "------\n",
      "Epoch 3147, Loss 2.928255, \n",
      "Params:  tensor([  5.3530, -17.2216])\n",
      "Grad:  tensor([-0.0025,  0.0142])\n",
      "------\n",
      "------\n",
      "Epoch 3148, Loss 2.928252, \n",
      "Params:  tensor([  5.3530, -17.2217])\n",
      "Grad:  tensor([-0.0025,  0.0141])\n",
      "------\n",
      "------\n",
      "Epoch 3149, Loss 2.928250, \n",
      "Params:  tensor([  5.3531, -17.2219])\n",
      "Grad:  tensor([-0.0025,  0.0141])\n",
      "------\n",
      "------\n",
      "Epoch 3150, Loss 2.928249, \n",
      "Params:  tensor([  5.3531, -17.2220])\n",
      "Grad:  tensor([-0.0025,  0.0141])\n",
      "------\n",
      "------\n",
      "Epoch 3151, Loss 2.928246, \n",
      "Params:  tensor([  5.3531, -17.2222])\n",
      "Grad:  tensor([-0.0025,  0.0141])\n",
      "------\n",
      "------\n",
      "Epoch 3152, Loss 2.928245, \n",
      "Params:  tensor([  5.3531, -17.2223])\n",
      "Grad:  tensor([-0.0025,  0.0140])\n",
      "------\n",
      "------\n",
      "Epoch 3153, Loss 2.928242, \n",
      "Params:  tensor([  5.3532, -17.2224])\n",
      "Grad:  tensor([-0.0025,  0.0140])\n",
      "------\n",
      "------\n",
      "Epoch 3154, Loss 2.928239, \n",
      "Params:  tensor([  5.3532, -17.2226])\n",
      "Grad:  tensor([-0.0025,  0.0140])\n",
      "------\n",
      "------\n",
      "Epoch 3155, Loss 2.928236, \n",
      "Params:  tensor([  5.3532, -17.2227])\n",
      "Grad:  tensor([-0.0025,  0.0140])\n",
      "------\n",
      "------\n",
      "Epoch 3156, Loss 2.928236, \n",
      "Params:  tensor([  5.3532, -17.2229])\n",
      "Grad:  tensor([-0.0024,  0.0139])\n",
      "------\n",
      "------\n",
      "Epoch 3157, Loss 2.928233, \n",
      "Params:  tensor([  5.3533, -17.2230])\n",
      "Grad:  tensor([-0.0025,  0.0139])\n",
      "------\n",
      "------\n",
      "Epoch 3158, Loss 2.928231, \n",
      "Params:  tensor([  5.3533, -17.2231])\n",
      "Grad:  tensor([-0.0024,  0.0139])\n",
      "------\n",
      "------\n",
      "Epoch 3159, Loss 2.928230, \n",
      "Params:  tensor([  5.3533, -17.2233])\n",
      "Grad:  tensor([-0.0025,  0.0139])\n",
      "------\n",
      "------\n",
      "Epoch 3160, Loss 2.928227, \n",
      "Params:  tensor([  5.3533, -17.2234])\n",
      "Grad:  tensor([-0.0024,  0.0138])\n",
      "------\n",
      "------\n",
      "Epoch 3161, Loss 2.928226, \n",
      "Params:  tensor([  5.3534, -17.2235])\n",
      "Grad:  tensor([-0.0025,  0.0138])\n",
      "------\n",
      "------\n",
      "Epoch 3162, Loss 2.928225, \n",
      "Params:  tensor([  5.3534, -17.2237])\n",
      "Grad:  tensor([-0.0024,  0.0138])\n",
      "------\n",
      "------\n",
      "Epoch 3163, Loss 2.928222, \n",
      "Params:  tensor([  5.3534, -17.2238])\n",
      "Grad:  tensor([-0.0024,  0.0138])\n",
      "------\n",
      "------\n",
      "Epoch 3164, Loss 2.928219, \n",
      "Params:  tensor([  5.3534, -17.2240])\n",
      "Grad:  tensor([-0.0024,  0.0138])\n",
      "------\n",
      "------\n",
      "Epoch 3165, Loss 2.928218, \n",
      "Params:  tensor([  5.3535, -17.2241])\n",
      "Grad:  tensor([-0.0024,  0.0137])\n",
      "------\n",
      "------\n",
      "Epoch 3166, Loss 2.928216, \n",
      "Params:  tensor([  5.3535, -17.2242])\n",
      "Grad:  tensor([-0.0024,  0.0137])\n",
      "------\n",
      "------\n",
      "Epoch 3167, Loss 2.928215, \n",
      "Params:  tensor([  5.3535, -17.2244])\n",
      "Grad:  tensor([-0.0024,  0.0137])\n",
      "------\n",
      "------\n",
      "Epoch 3168, Loss 2.928212, \n",
      "Params:  tensor([  5.3535, -17.2245])\n",
      "Grad:  tensor([-0.0024,  0.0137])\n",
      "------\n",
      "------\n",
      "Epoch 3169, Loss 2.928211, \n",
      "Params:  tensor([  5.3536, -17.2246])\n",
      "Grad:  tensor([-0.0024,  0.0136])\n",
      "------\n",
      "------\n",
      "Epoch 3170, Loss 2.928209, \n",
      "Params:  tensor([  5.3536, -17.2248])\n",
      "Grad:  tensor([-0.0024,  0.0136])\n",
      "------\n",
      "------\n",
      "Epoch 3171, Loss 2.928206, \n",
      "Params:  tensor([  5.3536, -17.2249])\n",
      "Grad:  tensor([-0.0024,  0.0136])\n",
      "------\n",
      "------\n",
      "Epoch 3172, Loss 2.928205, \n",
      "Params:  tensor([  5.3536, -17.2250])\n",
      "Grad:  tensor([-0.0024,  0.0136])\n",
      "------\n",
      "------\n",
      "Epoch 3173, Loss 2.928204, \n",
      "Params:  tensor([  5.3537, -17.2252])\n",
      "Grad:  tensor([-0.0024,  0.0135])\n",
      "------\n",
      "------\n",
      "Epoch 3174, Loss 2.928202, \n",
      "Params:  tensor([  5.3537, -17.2253])\n",
      "Grad:  tensor([-0.0024,  0.0135])\n",
      "------\n",
      "------\n",
      "Epoch 3175, Loss 2.928200, \n",
      "Params:  tensor([  5.3537, -17.2255])\n",
      "Grad:  tensor([-0.0024,  0.0135])\n",
      "------\n",
      "------\n",
      "Epoch 3176, Loss 2.928196, \n",
      "Params:  tensor([  5.3537, -17.2256])\n",
      "Grad:  tensor([-0.0024,  0.0135])\n",
      "------\n",
      "------\n",
      "Epoch 3177, Loss 2.928195, \n",
      "Params:  tensor([  5.3538, -17.2257])\n",
      "Grad:  tensor([-0.0024,  0.0134])\n",
      "------\n",
      "------\n",
      "Epoch 3178, Loss 2.928195, \n",
      "Params:  tensor([  5.3538, -17.2259])\n",
      "Grad:  tensor([-0.0024,  0.0134])\n",
      "------\n",
      "------\n",
      "Epoch 3179, Loss 2.928191, \n",
      "Params:  tensor([  5.3538, -17.2260])\n",
      "Grad:  tensor([-0.0024,  0.0134])\n",
      "------\n",
      "------\n",
      "Epoch 3180, Loss 2.928190, \n",
      "Params:  tensor([  5.3538, -17.2261])\n",
      "Grad:  tensor([-0.0024,  0.0134])\n",
      "------\n",
      "------\n",
      "Epoch 3181, Loss 2.928188, \n",
      "Params:  tensor([  5.3538, -17.2263])\n",
      "Grad:  tensor([-0.0023,  0.0134])\n",
      "------\n",
      "------\n",
      "Epoch 3182, Loss 2.928186, \n",
      "Params:  tensor([  5.3539, -17.2264])\n",
      "Grad:  tensor([-0.0023,  0.0133])\n",
      "------\n",
      "------\n",
      "Epoch 3183, Loss 2.928185, \n",
      "Params:  tensor([  5.3539, -17.2265])\n",
      "Grad:  tensor([-0.0024,  0.0133])\n",
      "------\n",
      "------\n",
      "Epoch 3184, Loss 2.928184, \n",
      "Params:  tensor([  5.3539, -17.2267])\n",
      "Grad:  tensor([-0.0023,  0.0133])\n",
      "------\n",
      "------\n",
      "Epoch 3185, Loss 2.928182, \n",
      "Params:  tensor([  5.3539, -17.2268])\n",
      "Grad:  tensor([-0.0024,  0.0133])\n",
      "------\n",
      "------\n",
      "Epoch 3186, Loss 2.928180, \n",
      "Params:  tensor([  5.3540, -17.2269])\n",
      "Grad:  tensor([-0.0024,  0.0132])\n",
      "------\n",
      "------\n",
      "Epoch 3187, Loss 2.928178, \n",
      "Params:  tensor([  5.3540, -17.2271])\n",
      "Grad:  tensor([-0.0023,  0.0132])\n",
      "------\n",
      "------\n",
      "Epoch 3188, Loss 2.928175, \n",
      "Params:  tensor([  5.3540, -17.2272])\n",
      "Grad:  tensor([-0.0023,  0.0132])\n",
      "------\n",
      "------\n",
      "Epoch 3189, Loss 2.928172, \n",
      "Params:  tensor([  5.3540, -17.2273])\n",
      "Grad:  tensor([-0.0023,  0.0132])\n",
      "------\n",
      "------\n",
      "Epoch 3190, Loss 2.928171, \n",
      "Params:  tensor([  5.3541, -17.2275])\n",
      "Grad:  tensor([-0.0023,  0.0132])\n",
      "------\n",
      "------\n",
      "Epoch 3191, Loss 2.928170, \n",
      "Params:  tensor([  5.3541, -17.2276])\n",
      "Grad:  tensor([-0.0023,  0.0131])\n",
      "------\n",
      "------\n",
      "Epoch 3192, Loss 2.928169, \n",
      "Params:  tensor([  5.3541, -17.2277])\n",
      "Grad:  tensor([-0.0023,  0.0131])\n",
      "------\n",
      "------\n",
      "Epoch 3193, Loss 2.928167, \n",
      "Params:  tensor([  5.3541, -17.2278])\n",
      "Grad:  tensor([-0.0023,  0.0131])\n",
      "------\n",
      "------\n",
      "Epoch 3194, Loss 2.928164, \n",
      "Params:  tensor([  5.3542, -17.2280])\n",
      "Grad:  tensor([-0.0023,  0.0131])\n",
      "------\n",
      "------\n",
      "Epoch 3195, Loss 2.928163, \n",
      "Params:  tensor([  5.3542, -17.2281])\n",
      "Grad:  tensor([-0.0023,  0.0130])\n",
      "------\n",
      "------\n",
      "Epoch 3196, Loss 2.928162, \n",
      "Params:  tensor([  5.3542, -17.2282])\n",
      "Grad:  tensor([-0.0023,  0.0130])\n",
      "------\n",
      "------\n",
      "Epoch 3197, Loss 2.928160, \n",
      "Params:  tensor([  5.3542, -17.2284])\n",
      "Grad:  tensor([-0.0023,  0.0130])\n",
      "------\n",
      "------\n",
      "Epoch 3198, Loss 2.928158, \n",
      "Params:  tensor([  5.3542, -17.2285])\n",
      "Grad:  tensor([-0.0023,  0.0130])\n",
      "------\n",
      "------\n",
      "Epoch 3199, Loss 2.928157, \n",
      "Params:  tensor([  5.3543, -17.2286])\n",
      "Grad:  tensor([-0.0023,  0.0130])\n",
      "------\n",
      "------\n",
      "Epoch 3200, Loss 2.928154, \n",
      "Params:  tensor([  5.3543, -17.2288])\n",
      "Grad:  tensor([-0.0023,  0.0129])\n",
      "------\n",
      "------\n",
      "Epoch 3201, Loss 2.928152, \n",
      "Params:  tensor([  5.3543, -17.2289])\n",
      "Grad:  tensor([-0.0023,  0.0129])\n",
      "------\n",
      "------\n",
      "Epoch 3202, Loss 2.928149, \n",
      "Params:  tensor([  5.3543, -17.2290])\n",
      "Grad:  tensor([-0.0023,  0.0129])\n",
      "------\n",
      "------\n",
      "Epoch 3203, Loss 2.928150, \n",
      "Params:  tensor([  5.3544, -17.2291])\n",
      "Grad:  tensor([-0.0023,  0.0129])\n",
      "------\n",
      "------\n",
      "Epoch 3204, Loss 2.928147, \n",
      "Params:  tensor([  5.3544, -17.2293])\n",
      "Grad:  tensor([-0.0022,  0.0129])\n",
      "------\n",
      "------\n",
      "Epoch 3205, Loss 2.928146, \n",
      "Params:  tensor([  5.3544, -17.2294])\n",
      "Grad:  tensor([-0.0023,  0.0128])\n",
      "------\n",
      "------\n",
      "Epoch 3206, Loss 2.928144, \n",
      "Params:  tensor([  5.3544, -17.2295])\n",
      "Grad:  tensor([-0.0023,  0.0128])\n",
      "------\n",
      "------\n",
      "Epoch 3207, Loss 2.928142, \n",
      "Params:  tensor([  5.3544, -17.2297])\n",
      "Grad:  tensor([-0.0023,  0.0128])\n",
      "------\n",
      "------\n",
      "Epoch 3208, Loss 2.928140, \n",
      "Params:  tensor([  5.3545, -17.2298])\n",
      "Grad:  tensor([-0.0022,  0.0128])\n",
      "------\n",
      "------\n",
      "Epoch 3209, Loss 2.928138, \n",
      "Params:  tensor([  5.3545, -17.2299])\n",
      "Grad:  tensor([-0.0022,  0.0127])\n",
      "------\n",
      "------\n",
      "Epoch 3210, Loss 2.928137, \n",
      "Params:  tensor([  5.3545, -17.2300])\n",
      "Grad:  tensor([-0.0023,  0.0127])\n",
      "------\n",
      "------\n",
      "Epoch 3211, Loss 2.928135, \n",
      "Params:  tensor([  5.3545, -17.2302])\n",
      "Grad:  tensor([-0.0023,  0.0127])\n",
      "------\n",
      "------\n",
      "Epoch 3212, Loss 2.928135, \n",
      "Params:  tensor([  5.3546, -17.2303])\n",
      "Grad:  tensor([-0.0023,  0.0127])\n",
      "------\n",
      "------\n",
      "Epoch 3213, Loss 2.928133, \n",
      "Params:  tensor([  5.3546, -17.2304])\n",
      "Grad:  tensor([-0.0022,  0.0127])\n",
      "------\n",
      "------\n",
      "Epoch 3214, Loss 2.928131, \n",
      "Params:  tensor([  5.3546, -17.2305])\n",
      "Grad:  tensor([-0.0022,  0.0126])\n",
      "------\n",
      "------\n",
      "Epoch 3215, Loss 2.928130, \n",
      "Params:  tensor([  5.3546, -17.2307])\n",
      "Grad:  tensor([-0.0022,  0.0126])\n",
      "------\n",
      "------\n",
      "Epoch 3216, Loss 2.928126, \n",
      "Params:  tensor([  5.3546, -17.2308])\n",
      "Grad:  tensor([-0.0022,  0.0126])\n",
      "------\n",
      "------\n",
      "Epoch 3217, Loss 2.928125, \n",
      "Params:  tensor([  5.3547, -17.2309])\n",
      "Grad:  tensor([-0.0022,  0.0126])\n",
      "------\n",
      "------\n",
      "Epoch 3218, Loss 2.928124, \n",
      "Params:  tensor([  5.3547, -17.2310])\n",
      "Grad:  tensor([-0.0022,  0.0125])\n",
      "------\n",
      "------\n",
      "Epoch 3219, Loss 2.928121, \n",
      "Params:  tensor([  5.3547, -17.2312])\n",
      "Grad:  tensor([-0.0022,  0.0125])\n",
      "------\n",
      "------\n",
      "Epoch 3220, Loss 2.928121, \n",
      "Params:  tensor([  5.3547, -17.2313])\n",
      "Grad:  tensor([-0.0022,  0.0125])\n",
      "------\n",
      "------\n",
      "Epoch 3221, Loss 2.928120, \n",
      "Params:  tensor([  5.3548, -17.2314])\n",
      "Grad:  tensor([-0.0022,  0.0125])\n",
      "------\n",
      "------\n",
      "Epoch 3222, Loss 2.928118, \n",
      "Params:  tensor([  5.3548, -17.2315])\n",
      "Grad:  tensor([-0.0022,  0.0125])\n",
      "------\n",
      "------\n",
      "Epoch 3223, Loss 2.928117, \n",
      "Params:  tensor([  5.3548, -17.2317])\n",
      "Grad:  tensor([-0.0022,  0.0124])\n",
      "------\n",
      "------\n",
      "Epoch 3224, Loss 2.928115, \n",
      "Params:  tensor([  5.3548, -17.2318])\n",
      "Grad:  tensor([-0.0022,  0.0124])\n",
      "------\n",
      "------\n",
      "Epoch 3225, Loss 2.928113, \n",
      "Params:  tensor([  5.3548, -17.2319])\n",
      "Grad:  tensor([-0.0022,  0.0124])\n",
      "------\n",
      "------\n",
      "Epoch 3226, Loss 2.928110, \n",
      "Params:  tensor([  5.3549, -17.2320])\n",
      "Grad:  tensor([-0.0022,  0.0124])\n",
      "------\n",
      "------\n",
      "Epoch 3227, Loss 2.928109, \n",
      "Params:  tensor([  5.3549, -17.2322])\n",
      "Grad:  tensor([-0.0022,  0.0124])\n",
      "------\n",
      "------\n",
      "Epoch 3228, Loss 2.928108, \n",
      "Params:  tensor([  5.3549, -17.2323])\n",
      "Grad:  tensor([-0.0022,  0.0123])\n",
      "------\n",
      "------\n",
      "Epoch 3229, Loss 2.928105, \n",
      "Params:  tensor([  5.3549, -17.2324])\n",
      "Grad:  tensor([-0.0022,  0.0123])\n",
      "------\n",
      "------\n",
      "Epoch 3230, Loss 2.928105, \n",
      "Params:  tensor([  5.3550, -17.2325])\n",
      "Grad:  tensor([-0.0022,  0.0123])\n",
      "------\n",
      "------\n",
      "Epoch 3231, Loss 2.928104, \n",
      "Params:  tensor([  5.3550, -17.2327])\n",
      "Grad:  tensor([-0.0022,  0.0123])\n",
      "------\n",
      "------\n",
      "Epoch 3232, Loss 2.928102, \n",
      "Params:  tensor([  5.3550, -17.2328])\n",
      "Grad:  tensor([-0.0021,  0.0123])\n",
      "------\n",
      "------\n",
      "Epoch 3233, Loss 2.928101, \n",
      "Params:  tensor([  5.3550, -17.2329])\n",
      "Grad:  tensor([-0.0022,  0.0122])\n",
      "------\n",
      "------\n",
      "Epoch 3234, Loss 2.928098, \n",
      "Params:  tensor([  5.3550, -17.2330])\n",
      "Grad:  tensor([-0.0022,  0.0122])\n",
      "------\n",
      "------\n",
      "Epoch 3235, Loss 2.928097, \n",
      "Params:  tensor([  5.3551, -17.2331])\n",
      "Grad:  tensor([-0.0022,  0.0122])\n",
      "------\n",
      "------\n",
      "Epoch 3236, Loss 2.928095, \n",
      "Params:  tensor([  5.3551, -17.2333])\n",
      "Grad:  tensor([-0.0022,  0.0122])\n",
      "------\n",
      "------\n",
      "Epoch 3237, Loss 2.928094, \n",
      "Params:  tensor([  5.3551, -17.2334])\n",
      "Grad:  tensor([-0.0022,  0.0121])\n",
      "------\n",
      "------\n",
      "Epoch 3238, Loss 2.928093, \n",
      "Params:  tensor([  5.3551, -17.2335])\n",
      "Grad:  tensor([-0.0022,  0.0121])\n",
      "------\n",
      "------\n",
      "Epoch 3239, Loss 2.928091, \n",
      "Params:  tensor([  5.3551, -17.2336])\n",
      "Grad:  tensor([-0.0022,  0.0121])\n",
      "------\n",
      "------\n",
      "Epoch 3240, Loss 2.928090, \n",
      "Params:  tensor([  5.3552, -17.2338])\n",
      "Grad:  tensor([-0.0021,  0.0121])\n",
      "------\n",
      "------\n",
      "Epoch 3241, Loss 2.928088, \n",
      "Params:  tensor([  5.3552, -17.2339])\n",
      "Grad:  tensor([-0.0021,  0.0121])\n",
      "------\n",
      "------\n",
      "Epoch 3242, Loss 2.928086, \n",
      "Params:  tensor([  5.3552, -17.2340])\n",
      "Grad:  tensor([-0.0021,  0.0120])\n",
      "------\n",
      "------\n",
      "Epoch 3243, Loss 2.928085, \n",
      "Params:  tensor([  5.3552, -17.2341])\n",
      "Grad:  tensor([-0.0021,  0.0120])\n",
      "------\n",
      "------\n",
      "Epoch 3244, Loss 2.928084, \n",
      "Params:  tensor([  5.3553, -17.2342])\n",
      "Grad:  tensor([-0.0021,  0.0120])\n",
      "------\n",
      "------\n",
      "Epoch 3245, Loss 2.928082, \n",
      "Params:  tensor([  5.3553, -17.2344])\n",
      "Grad:  tensor([-0.0021,  0.0120])\n",
      "------\n",
      "------\n",
      "Epoch 3246, Loss 2.928080, \n",
      "Params:  tensor([  5.3553, -17.2345])\n",
      "Grad:  tensor([-0.0021,  0.0120])\n",
      "------\n",
      "------\n",
      "Epoch 3247, Loss 2.928079, \n",
      "Params:  tensor([  5.3553, -17.2346])\n",
      "Grad:  tensor([-0.0021,  0.0119])\n",
      "------\n",
      "------\n",
      "Epoch 3248, Loss 2.928076, \n",
      "Params:  tensor([  5.3553, -17.2347])\n",
      "Grad:  tensor([-0.0021,  0.0119])\n",
      "------\n",
      "------\n",
      "Epoch 3249, Loss 2.928077, \n",
      "Params:  tensor([  5.3554, -17.2348])\n",
      "Grad:  tensor([-0.0021,  0.0119])\n",
      "------\n",
      "------\n",
      "Epoch 3250, Loss 2.928075, \n",
      "Params:  tensor([  5.3554, -17.2350])\n",
      "Grad:  tensor([-0.0021,  0.0119])\n",
      "------\n",
      "------\n",
      "Epoch 3251, Loss 2.928072, \n",
      "Params:  tensor([  5.3554, -17.2351])\n",
      "Grad:  tensor([-0.0021,  0.0119])\n",
      "------\n",
      "------\n",
      "Epoch 3252, Loss 2.928072, \n",
      "Params:  tensor([  5.3554, -17.2352])\n",
      "Grad:  tensor([-0.0021,  0.0118])\n",
      "------\n",
      "------\n",
      "Epoch 3253, Loss 2.928071, \n",
      "Params:  tensor([  5.3554, -17.2353])\n",
      "Grad:  tensor([-0.0021,  0.0118])\n",
      "------\n",
      "------\n",
      "Epoch 3254, Loss 2.928068, \n",
      "Params:  tensor([  5.3555, -17.2354])\n",
      "Grad:  tensor([-0.0021,  0.0118])\n",
      "------\n",
      "------\n",
      "Epoch 3255, Loss 2.928069, \n",
      "Params:  tensor([  5.3555, -17.2355])\n",
      "Grad:  tensor([-0.0021,  0.0118])\n",
      "------\n",
      "------\n",
      "Epoch 3256, Loss 2.928066, \n",
      "Params:  tensor([  5.3555, -17.2357])\n",
      "Grad:  tensor([-0.0021,  0.0118])\n",
      "------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Epoch 3257, Loss 2.928065, \n",
      "Params:  tensor([  5.3555, -17.2358])\n",
      "Grad:  tensor([-0.0021,  0.0117])\n",
      "------\n",
      "------\n",
      "Epoch 3258, Loss 2.928064, \n",
      "Params:  tensor([  5.3555, -17.2359])\n",
      "Grad:  tensor([-0.0021,  0.0117])\n",
      "------\n",
      "------\n",
      "Epoch 3259, Loss 2.928061, \n",
      "Params:  tensor([  5.3556, -17.2360])\n",
      "Grad:  tensor([-0.0021,  0.0117])\n",
      "------\n",
      "------\n",
      "Epoch 3260, Loss 2.928060, \n",
      "Params:  tensor([  5.3556, -17.2361])\n",
      "Grad:  tensor([-0.0021,  0.0117])\n",
      "------\n",
      "------\n",
      "Epoch 3261, Loss 2.928057, \n",
      "Params:  tensor([  5.3556, -17.2362])\n",
      "Grad:  tensor([-0.0021,  0.0117])\n",
      "------\n",
      "------\n",
      "Epoch 3262, Loss 2.928058, \n",
      "Params:  tensor([  5.3556, -17.2364])\n",
      "Grad:  tensor([-0.0021,  0.0116])\n",
      "------\n",
      "------\n",
      "Epoch 3263, Loss 2.928056, \n",
      "Params:  tensor([  5.3557, -17.2365])\n",
      "Grad:  tensor([-0.0021,  0.0116])\n",
      "------\n",
      "------\n",
      "Epoch 3264, Loss 2.928055, \n",
      "Params:  tensor([  5.3557, -17.2366])\n",
      "Grad:  tensor([-0.0021,  0.0116])\n",
      "------\n",
      "------\n",
      "Epoch 3265, Loss 2.928052, \n",
      "Params:  tensor([  5.3557, -17.2367])\n",
      "Grad:  tensor([-0.0021,  0.0116])\n",
      "------\n",
      "------\n",
      "Epoch 3266, Loss 2.928053, \n",
      "Params:  tensor([  5.3557, -17.2368])\n",
      "Grad:  tensor([-0.0021,  0.0116])\n",
      "------\n",
      "------\n",
      "Epoch 3267, Loss 2.928051, \n",
      "Params:  tensor([  5.3557, -17.2369])\n",
      "Grad:  tensor([-0.0021,  0.0115])\n",
      "------\n",
      "------\n",
      "Epoch 3268, Loss 2.928050, \n",
      "Params:  tensor([  5.3558, -17.2371])\n",
      "Grad:  tensor([-0.0021,  0.0115])\n",
      "------\n",
      "------\n",
      "Epoch 3269, Loss 2.928047, \n",
      "Params:  tensor([  5.3558, -17.2372])\n",
      "Grad:  tensor([-0.0020,  0.0115])\n",
      "------\n",
      "------\n",
      "Epoch 3270, Loss 2.928046, \n",
      "Params:  tensor([  5.3558, -17.2373])\n",
      "Grad:  tensor([-0.0020,  0.0115])\n",
      "------\n",
      "------\n",
      "Epoch 3271, Loss 2.928046, \n",
      "Params:  tensor([  5.3558, -17.2374])\n",
      "Grad:  tensor([-0.0020,  0.0115])\n",
      "------\n",
      "------\n",
      "Epoch 3272, Loss 2.928044, \n",
      "Params:  tensor([  5.3558, -17.2375])\n",
      "Grad:  tensor([-0.0020,  0.0115])\n",
      "------\n",
      "------\n",
      "Epoch 3273, Loss 2.928042, \n",
      "Params:  tensor([  5.3559, -17.2376])\n",
      "Grad:  tensor([-0.0020,  0.0114])\n",
      "------\n",
      "------\n",
      "Epoch 3274, Loss 2.928040, \n",
      "Params:  tensor([  5.3559, -17.2377])\n",
      "Grad:  tensor([-0.0020,  0.0114])\n",
      "------\n",
      "------\n",
      "Epoch 3275, Loss 2.928040, \n",
      "Params:  tensor([  5.3559, -17.2379])\n",
      "Grad:  tensor([-0.0020,  0.0114])\n",
      "------\n",
      "------\n",
      "Epoch 3276, Loss 2.928036, \n",
      "Params:  tensor([  5.3559, -17.2380])\n",
      "Grad:  tensor([-0.0020,  0.0114])\n",
      "------\n",
      "------\n",
      "Epoch 3277, Loss 2.928036, \n",
      "Params:  tensor([  5.3559, -17.2381])\n",
      "Grad:  tensor([-0.0020,  0.0113])\n",
      "------\n",
      "------\n",
      "Epoch 3278, Loss 2.928037, \n",
      "Params:  tensor([  5.3560, -17.2382])\n",
      "Grad:  tensor([-0.0020,  0.0113])\n",
      "------\n",
      "------\n",
      "Epoch 3279, Loss 2.928034, \n",
      "Params:  tensor([  5.3560, -17.2383])\n",
      "Grad:  tensor([-0.0020,  0.0113])\n",
      "------\n",
      "------\n",
      "Epoch 3280, Loss 2.928034, \n",
      "Params:  tensor([  5.3560, -17.2384])\n",
      "Grad:  tensor([-0.0020,  0.0113])\n",
      "------\n",
      "------\n",
      "Epoch 3281, Loss 2.928031, \n",
      "Params:  tensor([  5.3560, -17.2385])\n",
      "Grad:  tensor([-0.0020,  0.0113])\n",
      "------\n",
      "------\n",
      "Epoch 3282, Loss 2.928032, \n",
      "Params:  tensor([  5.3560, -17.2386])\n",
      "Grad:  tensor([-0.0020,  0.0113])\n",
      "------\n",
      "------\n",
      "Epoch 3283, Loss 2.928028, \n",
      "Params:  tensor([  5.3561, -17.2388])\n",
      "Grad:  tensor([-0.0020,  0.0112])\n",
      "------\n",
      "------\n",
      "Epoch 3284, Loss 2.928027, \n",
      "Params:  tensor([  5.3561, -17.2389])\n",
      "Grad:  tensor([-0.0020,  0.0112])\n",
      "------\n",
      "------\n",
      "Epoch 3285, Loss 2.928026, \n",
      "Params:  tensor([  5.3561, -17.2390])\n",
      "Grad:  tensor([-0.0020,  0.0112])\n",
      "------\n",
      "------\n",
      "Epoch 3286, Loss 2.928025, \n",
      "Params:  tensor([  5.3561, -17.2391])\n",
      "Grad:  tensor([-0.0020,  0.0112])\n",
      "------\n",
      "------\n",
      "Epoch 3287, Loss 2.928024, \n",
      "Params:  tensor([  5.3561, -17.2392])\n",
      "Grad:  tensor([-0.0020,  0.0112])\n",
      "------\n",
      "------\n",
      "Epoch 3288, Loss 2.928022, \n",
      "Params:  tensor([  5.3562, -17.2393])\n",
      "Grad:  tensor([-0.0020,  0.0111])\n",
      "------\n",
      "------\n",
      "Epoch 3289, Loss 2.928023, \n",
      "Params:  tensor([  5.3562, -17.2394])\n",
      "Grad:  tensor([-0.0020,  0.0111])\n",
      "------\n",
      "------\n",
      "Epoch 3290, Loss 2.928021, \n",
      "Params:  tensor([  5.3562, -17.2395])\n",
      "Grad:  tensor([-0.0020,  0.0111])\n",
      "------\n",
      "------\n",
      "Epoch 3291, Loss 2.928019, \n",
      "Params:  tensor([  5.3562, -17.2397])\n",
      "Grad:  tensor([-0.0020,  0.0111])\n",
      "------\n",
      "------\n",
      "Epoch 3292, Loss 2.928018, \n",
      "Params:  tensor([  5.3562, -17.2398])\n",
      "Grad:  tensor([-0.0020,  0.0111])\n",
      "------\n",
      "------\n",
      "Epoch 3293, Loss 2.928017, \n",
      "Params:  tensor([  5.3563, -17.2399])\n",
      "Grad:  tensor([-0.0020,  0.0110])\n",
      "------\n",
      "------\n",
      "Epoch 3294, Loss 2.928015, \n",
      "Params:  tensor([  5.3563, -17.2400])\n",
      "Grad:  tensor([-0.0020,  0.0110])\n",
      "------\n",
      "------\n",
      "Epoch 3295, Loss 2.928013, \n",
      "Params:  tensor([  5.3563, -17.2401])\n",
      "Grad:  tensor([-0.0020,  0.0110])\n",
      "------\n",
      "------\n",
      "Epoch 3296, Loss 2.928013, \n",
      "Params:  tensor([  5.3563, -17.2402])\n",
      "Grad:  tensor([-0.0019,  0.0110])\n",
      "------\n",
      "------\n",
      "Epoch 3297, Loss 2.928011, \n",
      "Params:  tensor([  5.3563, -17.2403])\n",
      "Grad:  tensor([-0.0019,  0.0110])\n",
      "------\n",
      "------\n",
      "Epoch 3298, Loss 2.928009, \n",
      "Params:  tensor([  5.3563, -17.2404])\n",
      "Grad:  tensor([-0.0019,  0.0110])\n",
      "------\n",
      "------\n",
      "Epoch 3299, Loss 2.928008, \n",
      "Params:  tensor([  5.3564, -17.2405])\n",
      "Grad:  tensor([-0.0019,  0.0109])\n",
      "------\n",
      "------\n",
      "Epoch 3300, Loss 2.928006, \n",
      "Params:  tensor([  5.3564, -17.2406])\n",
      "Grad:  tensor([-0.0019,  0.0109])\n",
      "------\n",
      "------\n",
      "Epoch 3301, Loss 2.928007, \n",
      "Params:  tensor([  5.3564, -17.2407])\n",
      "Grad:  tensor([-0.0019,  0.0109])\n",
      "------\n",
      "------\n",
      "Epoch 3302, Loss 2.928007, \n",
      "Params:  tensor([  5.3564, -17.2409])\n",
      "Grad:  tensor([-0.0019,  0.0109])\n",
      "------\n",
      "------\n",
      "Epoch 3303, Loss 2.928004, \n",
      "Params:  tensor([  5.3564, -17.2410])\n",
      "Grad:  tensor([-0.0019,  0.0109])\n",
      "------\n",
      "------\n",
      "Epoch 3304, Loss 2.928002, \n",
      "Params:  tensor([  5.3565, -17.2411])\n",
      "Grad:  tensor([-0.0019,  0.0108])\n",
      "------\n",
      "------\n",
      "Epoch 3305, Loss 2.928002, \n",
      "Params:  tensor([  5.3565, -17.2412])\n",
      "Grad:  tensor([-0.0019,  0.0108])\n",
      "------\n",
      "------\n",
      "Epoch 3306, Loss 2.928000, \n",
      "Params:  tensor([  5.3565, -17.2413])\n",
      "Grad:  tensor([-0.0019,  0.0108])\n",
      "------\n",
      "------\n",
      "Epoch 3307, Loss 2.928000, \n",
      "Params:  tensor([  5.3565, -17.2414])\n",
      "Grad:  tensor([-0.0019,  0.0108])\n",
      "------\n",
      "------\n",
      "Epoch 3308, Loss 2.927998, \n",
      "Params:  tensor([  5.3565, -17.2415])\n",
      "Grad:  tensor([-0.0019,  0.0108])\n",
      "------\n",
      "------\n",
      "Epoch 3309, Loss 2.927995, \n",
      "Params:  tensor([  5.3566, -17.2416])\n",
      "Grad:  tensor([-0.0019,  0.0107])\n",
      "------\n",
      "------\n",
      "Epoch 3310, Loss 2.927995, \n",
      "Params:  tensor([  5.3566, -17.2417])\n",
      "Grad:  tensor([-0.0019,  0.0107])\n",
      "------\n",
      "------\n",
      "Epoch 3311, Loss 2.927994, \n",
      "Params:  tensor([  5.3566, -17.2418])\n",
      "Grad:  tensor([-0.0019,  0.0107])\n",
      "------\n",
      "------\n",
      "Epoch 3312, Loss 2.927994, \n",
      "Params:  tensor([  5.3566, -17.2419])\n",
      "Grad:  tensor([-0.0019,  0.0107])\n",
      "------\n",
      "------\n",
      "Epoch 3313, Loss 2.927991, \n",
      "Params:  tensor([  5.3566, -17.2420])\n",
      "Grad:  tensor([-0.0019,  0.0107])\n",
      "------\n",
      "------\n",
      "Epoch 3314, Loss 2.927991, \n",
      "Params:  tensor([  5.3567, -17.2421])\n",
      "Grad:  tensor([-0.0019,  0.0107])\n",
      "------\n",
      "------\n",
      "Epoch 3315, Loss 2.927990, \n",
      "Params:  tensor([  5.3567, -17.2423])\n",
      "Grad:  tensor([-0.0019,  0.0106])\n",
      "------\n",
      "------\n",
      "Epoch 3316, Loss 2.927989, \n",
      "Params:  tensor([  5.3567, -17.2424])\n",
      "Grad:  tensor([-0.0019,  0.0106])\n",
      "------\n",
      "------\n",
      "Epoch 3317, Loss 2.927988, \n",
      "Params:  tensor([  5.3567, -17.2425])\n",
      "Grad:  tensor([-0.0019,  0.0106])\n",
      "------\n",
      "------\n",
      "Epoch 3318, Loss 2.927986, \n",
      "Params:  tensor([  5.3567, -17.2426])\n",
      "Grad:  tensor([-0.0019,  0.0106])\n",
      "------\n",
      "------\n",
      "Epoch 3319, Loss 2.927985, \n",
      "Params:  tensor([  5.3567, -17.2427])\n",
      "Grad:  tensor([-0.0019,  0.0106])\n",
      "------\n",
      "------\n",
      "Epoch 3320, Loss 2.927983, \n",
      "Params:  tensor([  5.3568, -17.2428])\n",
      "Grad:  tensor([-0.0018,  0.0106])\n",
      "------\n",
      "------\n",
      "Epoch 3321, Loss 2.927983, \n",
      "Params:  tensor([  5.3568, -17.2429])\n",
      "Grad:  tensor([-0.0018,  0.0105])\n",
      "------\n",
      "------\n",
      "Epoch 3322, Loss 2.927981, \n",
      "Params:  tensor([  5.3568, -17.2430])\n",
      "Grad:  tensor([-0.0018,  0.0105])\n",
      "------\n",
      "------\n",
      "Epoch 3323, Loss 2.927980, \n",
      "Params:  tensor([  5.3568, -17.2431])\n",
      "Grad:  tensor([-0.0018,  0.0105])\n",
      "------\n",
      "------\n",
      "Epoch 3324, Loss 2.927979, \n",
      "Params:  tensor([  5.3568, -17.2432])\n",
      "Grad:  tensor([-0.0018,  0.0105])\n",
      "------\n",
      "------\n",
      "Epoch 3325, Loss 2.927979, \n",
      "Params:  tensor([  5.3569, -17.2433])\n",
      "Grad:  tensor([-0.0018,  0.0105])\n",
      "------\n",
      "------\n",
      "Epoch 3326, Loss 2.927977, \n",
      "Params:  tensor([  5.3569, -17.2434])\n",
      "Grad:  tensor([-0.0018,  0.0104])\n",
      "------\n",
      "------\n",
      "Epoch 3327, Loss 2.927975, \n",
      "Params:  tensor([  5.3569, -17.2435])\n",
      "Grad:  tensor([-0.0019,  0.0104])\n",
      "------\n",
      "------\n",
      "Epoch 3328, Loss 2.927973, \n",
      "Params:  tensor([  5.3569, -17.2436])\n",
      "Grad:  tensor([-0.0018,  0.0104])\n",
      "------\n",
      "------\n",
      "Epoch 3329, Loss 2.927974, \n",
      "Params:  tensor([  5.3569, -17.2437])\n",
      "Grad:  tensor([-0.0018,  0.0104])\n",
      "------\n",
      "------\n",
      "Epoch 3330, Loss 2.927974, \n",
      "Params:  tensor([  5.3570, -17.2438])\n",
      "Grad:  tensor([-0.0018,  0.0104])\n",
      "------\n",
      "------\n",
      "Epoch 3331, Loss 2.927972, \n",
      "Params:  tensor([  5.3570, -17.2439])\n",
      "Grad:  tensor([-0.0018,  0.0104])\n",
      "------\n",
      "------\n",
      "Epoch 3332, Loss 2.927972, \n",
      "Params:  tensor([  5.3570, -17.2440])\n",
      "Grad:  tensor([-0.0018,  0.0103])\n",
      "------\n",
      "------\n",
      "Epoch 3333, Loss 2.927969, \n",
      "Params:  tensor([  5.3570, -17.2441])\n",
      "Grad:  tensor([-0.0018,  0.0103])\n",
      "------\n",
      "------\n",
      "Epoch 3334, Loss 2.927969, \n",
      "Params:  tensor([  5.3570, -17.2442])\n",
      "Grad:  tensor([-0.0018,  0.0103])\n",
      "------\n",
      "------\n",
      "Epoch 3335, Loss 2.927967, \n",
      "Params:  tensor([  5.3570, -17.2443])\n",
      "Grad:  tensor([-0.0018,  0.0103])\n",
      "------\n",
      "------\n",
      "Epoch 3336, Loss 2.927967, \n",
      "Params:  tensor([  5.3571, -17.2444])\n",
      "Grad:  tensor([-0.0018,  0.0103])\n",
      "------\n",
      "------\n",
      "Epoch 3337, Loss 2.927963, \n",
      "Params:  tensor([  5.3571, -17.2446])\n",
      "Grad:  tensor([-0.0018,  0.0102])\n",
      "------\n",
      "------\n",
      "Epoch 3338, Loss 2.927963, \n",
      "Params:  tensor([  5.3571, -17.2447])\n",
      "Grad:  tensor([-0.0018,  0.0102])\n",
      "------\n",
      "------\n",
      "Epoch 3339, Loss 2.927962, \n",
      "Params:  tensor([  5.3571, -17.2448])\n",
      "Grad:  tensor([-0.0018,  0.0102])\n",
      "------\n",
      "------\n",
      "Epoch 3340, Loss 2.927962, \n",
      "Params:  tensor([  5.3571, -17.2449])\n",
      "Grad:  tensor([-0.0018,  0.0102])\n",
      "------\n",
      "------\n",
      "Epoch 3341, Loss 2.927960, \n",
      "Params:  tensor([  5.3572, -17.2450])\n",
      "Grad:  tensor([-0.0018,  0.0102])\n",
      "------\n",
      "------\n",
      "Epoch 3342, Loss 2.927960, \n",
      "Params:  tensor([  5.3572, -17.2451])\n",
      "Grad:  tensor([-0.0018,  0.0102])\n",
      "------\n",
      "------\n",
      "Epoch 3343, Loss 2.927959, \n",
      "Params:  tensor([  5.3572, -17.2452])\n",
      "Grad:  tensor([-0.0018,  0.0101])\n",
      "------\n",
      "------\n",
      "Epoch 3344, Loss 2.927958, \n",
      "Params:  tensor([  5.3572, -17.2453])\n",
      "Grad:  tensor([-0.0018,  0.0101])\n",
      "------\n",
      "------\n",
      "Epoch 3345, Loss 2.927956, \n",
      "Params:  tensor([  5.3572, -17.2454])\n",
      "Grad:  tensor([-0.0018,  0.0101])\n",
      "------\n",
      "------\n",
      "Epoch 3346, Loss 2.927956, \n",
      "Params:  tensor([  5.3572, -17.2455])\n",
      "Grad:  tensor([-0.0018,  0.0101])\n",
      "------\n",
      "------\n",
      "Epoch 3347, Loss 2.927955, \n",
      "Params:  tensor([  5.3573, -17.2456])\n",
      "Grad:  tensor([-0.0018,  0.0101])\n",
      "------\n",
      "------\n",
      "Epoch 3348, Loss 2.927953, \n",
      "Params:  tensor([  5.3573, -17.2457])\n",
      "Grad:  tensor([-0.0018,  0.0101])\n",
      "------\n",
      "------\n",
      "Epoch 3349, Loss 2.927953, \n",
      "Params:  tensor([  5.3573, -17.2458])\n",
      "Grad:  tensor([-0.0018,  0.0100])\n",
      "------\n",
      "------\n",
      "Epoch 3350, Loss 2.927951, \n",
      "Params:  tensor([  5.3573, -17.2459])\n",
      "Grad:  tensor([-0.0018,  0.0100])\n",
      "------\n",
      "------\n",
      "Epoch 3351, Loss 2.927950, \n",
      "Params:  tensor([  5.3573, -17.2460])\n",
      "Grad:  tensor([-0.0018,  0.0100])\n",
      "------\n",
      "------\n",
      "Epoch 3352, Loss 2.927948, \n",
      "Params:  tensor([  5.3573, -17.2461])\n",
      "Grad:  tensor([-0.0018,  0.0100])\n",
      "------\n",
      "------\n",
      "Epoch 3353, Loss 2.927947, \n",
      "Params:  tensor([  5.3574, -17.2462])\n",
      "Grad:  tensor([-0.0017,  0.0100])\n",
      "------\n",
      "------\n",
      "Epoch 3354, Loss 2.927948, \n",
      "Params:  tensor([  5.3574, -17.2463])\n",
      "Grad:  tensor([-0.0018,  0.0100])\n",
      "------\n",
      "------\n",
      "Epoch 3355, Loss 2.927945, \n",
      "Params:  tensor([  5.3574, -17.2464])\n",
      "Grad:  tensor([-0.0017,  0.0099])\n",
      "------\n",
      "------\n",
      "Epoch 3356, Loss 2.927944, \n",
      "Params:  tensor([  5.3574, -17.2465])\n",
      "Grad:  tensor([-0.0017,  0.0099])\n",
      "------\n",
      "------\n",
      "Epoch 3357, Loss 2.927943, \n",
      "Params:  tensor([  5.3574, -17.2466])\n",
      "Grad:  tensor([-0.0018,  0.0099])\n",
      "------\n",
      "------\n",
      "Epoch 3358, Loss 2.927944, \n",
      "Params:  tensor([  5.3575, -17.2467])\n",
      "Grad:  tensor([-0.0017,  0.0099])\n",
      "------\n",
      "------\n",
      "Epoch 3359, Loss 2.927942, \n",
      "Params:  tensor([  5.3575, -17.2468])\n",
      "Grad:  tensor([-0.0017,  0.0099])\n",
      "------\n",
      "------\n",
      "Epoch 3360, Loss 2.927941, \n",
      "Params:  tensor([  5.3575, -17.2469])\n",
      "Grad:  tensor([-0.0018,  0.0099])\n",
      "------\n",
      "------\n",
      "Epoch 3361, Loss 2.927940, \n",
      "Params:  tensor([  5.3575, -17.2470])\n",
      "Grad:  tensor([-0.0017,  0.0098])\n",
      "------\n",
      "------\n",
      "Epoch 3362, Loss 2.927938, \n",
      "Params:  tensor([  5.3575, -17.2471])\n",
      "Grad:  tensor([-0.0017,  0.0098])\n",
      "------\n",
      "------\n",
      "Epoch 3363, Loss 2.927938, \n",
      "Params:  tensor([  5.3575, -17.2472])\n",
      "Grad:  tensor([-0.0018,  0.0098])\n",
      "------\n",
      "------\n",
      "Epoch 3364, Loss 2.927936, \n",
      "Params:  tensor([  5.3576, -17.2473])\n",
      "Grad:  tensor([-0.0017,  0.0098])\n",
      "------\n",
      "------\n",
      "Epoch 3365, Loss 2.927936, \n",
      "Params:  tensor([  5.3576, -17.2474])\n",
      "Grad:  tensor([-0.0017,  0.0098])\n",
      "------\n",
      "------\n",
      "Epoch 3366, Loss 2.927937, \n",
      "Params:  tensor([  5.3576, -17.2474])\n",
      "Grad:  tensor([-0.0017,  0.0098])\n",
      "------\n",
      "------\n",
      "Epoch 3367, Loss 2.927934, \n",
      "Params:  tensor([  5.3576, -17.2475])\n",
      "Grad:  tensor([-0.0017,  0.0097])\n",
      "------\n",
      "------\n",
      "Epoch 3368, Loss 2.927933, \n",
      "Params:  tensor([  5.3576, -17.2476])\n",
      "Grad:  tensor([-0.0017,  0.0097])\n",
      "------\n",
      "------\n",
      "Epoch 3369, Loss 2.927932, \n",
      "Params:  tensor([  5.3576, -17.2477])\n",
      "Grad:  tensor([-0.0017,  0.0097])\n",
      "------\n",
      "------\n",
      "Epoch 3370, Loss 2.927930, \n",
      "Params:  tensor([  5.3577, -17.2478])\n",
      "Grad:  tensor([-0.0017,  0.0097])\n",
      "------\n",
      "------\n",
      "Epoch 3371, Loss 2.927928, \n",
      "Params:  tensor([  5.3577, -17.2479])\n",
      "Grad:  tensor([-0.0017,  0.0097])\n",
      "------\n",
      "------\n",
      "Epoch 3372, Loss 2.927931, \n",
      "Params:  tensor([  5.3577, -17.2480])\n",
      "Grad:  tensor([-0.0017,  0.0097])\n",
      "------\n",
      "------\n",
      "Epoch 3373, Loss 2.927929, \n",
      "Params:  tensor([  5.3577, -17.2481])\n",
      "Grad:  tensor([-0.0017,  0.0096])\n",
      "------\n",
      "------\n",
      "Epoch 3374, Loss 2.927927, \n",
      "Params:  tensor([  5.3577, -17.2482])\n",
      "Grad:  tensor([-0.0017,  0.0096])\n",
      "------\n",
      "------\n",
      "Epoch 3375, Loss 2.927926, \n",
      "Params:  tensor([  5.3577, -17.2483])\n",
      "Grad:  tensor([-0.0017,  0.0096])\n",
      "------\n",
      "------\n",
      "Epoch 3376, Loss 2.927925, \n",
      "Params:  tensor([  5.3578, -17.2484])\n",
      "Grad:  tensor([-0.0017,  0.0096])\n",
      "------\n",
      "------\n",
      "Epoch 3377, Loss 2.927924, \n",
      "Params:  tensor([  5.3578, -17.2485])\n",
      "Grad:  tensor([-0.0017,  0.0096])\n",
      "------\n",
      "------\n",
      "Epoch 3378, Loss 2.927923, \n",
      "Params:  tensor([  5.3578, -17.2486])\n",
      "Grad:  tensor([-0.0017,  0.0096])\n",
      "------\n",
      "------\n",
      "Epoch 3379, Loss 2.927924, \n",
      "Params:  tensor([  5.3578, -17.2487])\n",
      "Grad:  tensor([-0.0017,  0.0095])\n",
      "------\n",
      "------\n",
      "Epoch 3380, Loss 2.927922, \n",
      "Params:  tensor([  5.3578, -17.2488])\n",
      "Grad:  tensor([-0.0017,  0.0095])\n",
      "------\n",
      "------\n",
      "Epoch 3381, Loss 2.927922, \n",
      "Params:  tensor([  5.3578, -17.2489])\n",
      "Grad:  tensor([-0.0017,  0.0095])\n",
      "------\n",
      "------\n",
      "Epoch 3382, Loss 2.927920, \n",
      "Params:  tensor([  5.3579, -17.2490])\n",
      "Grad:  tensor([-0.0017,  0.0095])\n",
      "------\n",
      "------\n",
      "Epoch 3383, Loss 2.927918, \n",
      "Params:  tensor([  5.3579, -17.2491])\n",
      "Grad:  tensor([-0.0017,  0.0095])\n",
      "------\n",
      "------\n",
      "Epoch 3384, Loss 2.927917, \n",
      "Params:  tensor([  5.3579, -17.2492])\n",
      "Grad:  tensor([-0.0017,  0.0095])\n",
      "------\n",
      "------\n",
      "Epoch 3385, Loss 2.927917, \n",
      "Params:  tensor([  5.3579, -17.2493])\n",
      "Grad:  tensor([-0.0017,  0.0094])\n",
      "------\n",
      "------\n",
      "Epoch 3386, Loss 2.927915, \n",
      "Params:  tensor([  5.3579, -17.2494])\n",
      "Grad:  tensor([-0.0017,  0.0094])\n",
      "------\n",
      "------\n",
      "Epoch 3387, Loss 2.927915, \n",
      "Params:  tensor([  5.3579, -17.2495])\n",
      "Grad:  tensor([-0.0017,  0.0094])\n",
      "------\n",
      "------\n",
      "Epoch 3388, Loss 2.927914, \n",
      "Params:  tensor([  5.3580, -17.2496])\n",
      "Grad:  tensor([-0.0016,  0.0094])\n",
      "------\n",
      "------\n",
      "Epoch 3389, Loss 2.927913, \n",
      "Params:  tensor([  5.3580, -17.2496])\n",
      "Grad:  tensor([-0.0017,  0.0094])\n",
      "------\n",
      "------\n",
      "Epoch 3390, Loss 2.927911, \n",
      "Params:  tensor([  5.3580, -17.2497])\n",
      "Grad:  tensor([-0.0016,  0.0094])\n",
      "------\n",
      "------\n",
      "Epoch 3391, Loss 2.927913, \n",
      "Params:  tensor([  5.3580, -17.2498])\n",
      "Grad:  tensor([-0.0017,  0.0093])\n",
      "------\n",
      "------\n",
      "Epoch 3392, Loss 2.927911, \n",
      "Params:  tensor([  5.3580, -17.2499])\n",
      "Grad:  tensor([-0.0016,  0.0093])\n",
      "------\n",
      "------\n",
      "Epoch 3393, Loss 2.927910, \n",
      "Params:  tensor([  5.3580, -17.2500])\n",
      "Grad:  tensor([-0.0016,  0.0093])\n",
      "------\n",
      "------\n",
      "Epoch 3394, Loss 2.927909, \n",
      "Params:  tensor([  5.3581, -17.2501])\n",
      "Grad:  tensor([-0.0016,  0.0093])\n",
      "------\n",
      "------\n",
      "Epoch 3395, Loss 2.927908, \n",
      "Params:  tensor([  5.3581, -17.2502])\n",
      "Grad:  tensor([-0.0016,  0.0093])\n",
      "------\n",
      "------\n",
      "Epoch 3396, Loss 2.927907, \n",
      "Params:  tensor([  5.3581, -17.2503])\n",
      "Grad:  tensor([-0.0017,  0.0093])\n",
      "------\n",
      "------\n",
      "Epoch 3397, Loss 2.927906, \n",
      "Params:  tensor([  5.3581, -17.2504])\n",
      "Grad:  tensor([-0.0016,  0.0093])\n",
      "------\n",
      "------\n",
      "Epoch 3398, Loss 2.927905, \n",
      "Params:  tensor([  5.3581, -17.2505])\n",
      "Grad:  tensor([-0.0017,  0.0092])\n",
      "------\n",
      "------\n",
      "Epoch 3399, Loss 2.927905, \n",
      "Params:  tensor([  5.3581, -17.2506])\n",
      "Grad:  tensor([-0.0016,  0.0092])\n",
      "------\n",
      "------\n",
      "Epoch 3400, Loss 2.927904, \n",
      "Params:  tensor([  5.3582, -17.2507])\n",
      "Grad:  tensor([-0.0016,  0.0092])\n",
      "------\n",
      "------\n",
      "Epoch 3401, Loss 2.927902, \n",
      "Params:  tensor([  5.3582, -17.2508])\n",
      "Grad:  tensor([-0.0016,  0.0092])\n",
      "------\n",
      "------\n",
      "Epoch 3402, Loss 2.927902, \n",
      "Params:  tensor([  5.3582, -17.2509])\n",
      "Grad:  tensor([-0.0016,  0.0092])\n",
      "------\n",
      "------\n",
      "Epoch 3403, Loss 2.927902, \n",
      "Params:  tensor([  5.3582, -17.2509])\n",
      "Grad:  tensor([-0.0016,  0.0092])\n",
      "------\n",
      "------\n",
      "Epoch 3404, Loss 2.927899, \n",
      "Params:  tensor([  5.3582, -17.2510])\n",
      "Grad:  tensor([-0.0016,  0.0091])\n",
      "------\n",
      "------\n",
      "Epoch 3405, Loss 2.927899, \n",
      "Params:  tensor([  5.3582, -17.2511])\n",
      "Grad:  tensor([-0.0016,  0.0091])\n",
      "------\n",
      "------\n",
      "Epoch 3406, Loss 2.927898, \n",
      "Params:  tensor([  5.3583, -17.2512])\n",
      "Grad:  tensor([-0.0016,  0.0091])\n",
      "------\n",
      "------\n",
      "Epoch 3407, Loss 2.927899, \n",
      "Params:  tensor([  5.3583, -17.2513])\n",
      "Grad:  tensor([-0.0016,  0.0091])\n",
      "------\n",
      "------\n",
      "Epoch 3408, Loss 2.927896, \n",
      "Params:  tensor([  5.3583, -17.2514])\n",
      "Grad:  tensor([-0.0016,  0.0091])\n",
      "------\n",
      "------\n",
      "Epoch 3409, Loss 2.927895, \n",
      "Params:  tensor([  5.3583, -17.2515])\n",
      "Grad:  tensor([-0.0016,  0.0091])\n",
      "------\n",
      "------\n",
      "Epoch 3410, Loss 2.927896, \n",
      "Params:  tensor([  5.3583, -17.2516])\n",
      "Grad:  tensor([-0.0016,  0.0091])\n",
      "------\n",
      "------\n",
      "Epoch 3411, Loss 2.927894, \n",
      "Params:  tensor([  5.3583, -17.2517])\n",
      "Grad:  tensor([-0.0016,  0.0090])\n",
      "------\n",
      "------\n",
      "Epoch 3412, Loss 2.927892, \n",
      "Params:  tensor([  5.3584, -17.2518])\n",
      "Grad:  tensor([-0.0016,  0.0090])\n",
      "------\n",
      "------\n",
      "Epoch 3413, Loss 2.927892, \n",
      "Params:  tensor([  5.3584, -17.2519])\n",
      "Grad:  tensor([-0.0016,  0.0090])\n",
      "------\n",
      "------\n",
      "Epoch 3414, Loss 2.927891, \n",
      "Params:  tensor([  5.3584, -17.2519])\n",
      "Grad:  tensor([-0.0016,  0.0090])\n",
      "------\n",
      "------\n",
      "Epoch 3415, Loss 2.927891, \n",
      "Params:  tensor([  5.3584, -17.2520])\n",
      "Grad:  tensor([-0.0016,  0.0090])\n",
      "------\n",
      "------\n",
      "Epoch 3416, Loss 2.927890, \n",
      "Params:  tensor([  5.3584, -17.2521])\n",
      "Grad:  tensor([-0.0016,  0.0090])\n",
      "------\n",
      "------\n",
      "Epoch 3417, Loss 2.927891, \n",
      "Params:  tensor([  5.3584, -17.2522])\n",
      "Grad:  tensor([-0.0016,  0.0089])\n",
      "------\n",
      "------\n",
      "Epoch 3418, Loss 2.927888, \n",
      "Params:  tensor([  5.3584, -17.2523])\n",
      "Grad:  tensor([-0.0016,  0.0089])\n",
      "------\n",
      "------\n",
      "Epoch 3419, Loss 2.927888, \n",
      "Params:  tensor([  5.3585, -17.2524])\n",
      "Grad:  tensor([-0.0016,  0.0089])\n",
      "------\n",
      "------\n",
      "Epoch 3420, Loss 2.927886, \n",
      "Params:  tensor([  5.3585, -17.2525])\n",
      "Grad:  tensor([-0.0016,  0.0089])\n",
      "------\n",
      "------\n",
      "Epoch 3421, Loss 2.927887, \n",
      "Params:  tensor([  5.3585, -17.2526])\n",
      "Grad:  tensor([-0.0016,  0.0089])\n",
      "------\n",
      "------\n",
      "Epoch 3422, Loss 2.927886, \n",
      "Params:  tensor([  5.3585, -17.2527])\n",
      "Grad:  tensor([-0.0016,  0.0089])\n",
      "------\n",
      "------\n",
      "Epoch 3423, Loss 2.927884, \n",
      "Params:  tensor([  5.3585, -17.2527])\n",
      "Grad:  tensor([-0.0016,  0.0089])\n",
      "------\n",
      "------\n",
      "Epoch 3424, Loss 2.927883, \n",
      "Params:  tensor([  5.3585, -17.2528])\n",
      "Grad:  tensor([-0.0015,  0.0088])\n",
      "------\n",
      "------\n",
      "Epoch 3425, Loss 2.927881, \n",
      "Params:  tensor([  5.3586, -17.2529])\n",
      "Grad:  tensor([-0.0015,  0.0088])\n",
      "------\n",
      "------\n",
      "Epoch 3426, Loss 2.927881, \n",
      "Params:  tensor([  5.3586, -17.2530])\n",
      "Grad:  tensor([-0.0016,  0.0088])\n",
      "------\n",
      "------\n",
      "Epoch 3427, Loss 2.927880, \n",
      "Params:  tensor([  5.3586, -17.2531])\n",
      "Grad:  tensor([-0.0015,  0.0088])\n",
      "------\n",
      "------\n",
      "Epoch 3428, Loss 2.927880, \n",
      "Params:  tensor([  5.3586, -17.2532])\n",
      "Grad:  tensor([-0.0016,  0.0088])\n",
      "------\n",
      "------\n",
      "Epoch 3429, Loss 2.927879, \n",
      "Params:  tensor([  5.3586, -17.2533])\n",
      "Grad:  tensor([-0.0015,  0.0088])\n",
      "------\n",
      "------\n",
      "Epoch 3430, Loss 2.927877, \n",
      "Params:  tensor([  5.3586, -17.2534])\n",
      "Grad:  tensor([-0.0016,  0.0087])\n",
      "------\n",
      "------\n",
      "Epoch 3431, Loss 2.927876, \n",
      "Params:  tensor([  5.3586, -17.2534])\n",
      "Grad:  tensor([-0.0015,  0.0087])\n",
      "------\n",
      "------\n",
      "Epoch 3432, Loss 2.927876, \n",
      "Params:  tensor([  5.3587, -17.2535])\n",
      "Grad:  tensor([-0.0015,  0.0087])\n",
      "------\n",
      "------\n",
      "Epoch 3433, Loss 2.927876, \n",
      "Params:  tensor([  5.3587, -17.2536])\n",
      "Grad:  tensor([-0.0015,  0.0087])\n",
      "------\n",
      "------\n",
      "Epoch 3434, Loss 2.927876, \n",
      "Params:  tensor([  5.3587, -17.2537])\n",
      "Grad:  tensor([-0.0016,  0.0087])\n",
      "------\n",
      "------\n",
      "Epoch 3435, Loss 2.927876, \n",
      "Params:  tensor([  5.3587, -17.2538])\n",
      "Grad:  tensor([-0.0016,  0.0087])\n",
      "------\n",
      "------\n",
      "Epoch 3436, Loss 2.927875, \n",
      "Params:  tensor([  5.3587, -17.2539])\n",
      "Grad:  tensor([-0.0015,  0.0087])\n",
      "------\n",
      "------\n",
      "Epoch 3437, Loss 2.927873, \n",
      "Params:  tensor([  5.3587, -17.2540])\n",
      "Grad:  tensor([-0.0015,  0.0087])\n",
      "------\n",
      "------\n",
      "Epoch 3438, Loss 2.927872, \n",
      "Params:  tensor([  5.3588, -17.2541])\n",
      "Grad:  tensor([-0.0015,  0.0086])\n",
      "------\n",
      "------\n",
      "Epoch 3439, Loss 2.927871, \n",
      "Params:  tensor([  5.3588, -17.2541])\n",
      "Grad:  tensor([-0.0015,  0.0086])\n",
      "------\n",
      "------\n",
      "Epoch 3440, Loss 2.927870, \n",
      "Params:  tensor([  5.3588, -17.2542])\n",
      "Grad:  tensor([-0.0015,  0.0086])\n",
      "------\n",
      "------\n",
      "Epoch 3441, Loss 2.927871, \n",
      "Params:  tensor([  5.3588, -17.2543])\n",
      "Grad:  tensor([-0.0015,  0.0086])\n",
      "------\n",
      "------\n",
      "Epoch 3442, Loss 2.927869, \n",
      "Params:  tensor([  5.3588, -17.2544])\n",
      "Grad:  tensor([-0.0015,  0.0086])\n",
      "------\n",
      "------\n",
      "Epoch 3443, Loss 2.927869, \n",
      "Params:  tensor([  5.3588, -17.2545])\n",
      "Grad:  tensor([-0.0015,  0.0086])\n",
      "------\n",
      "------\n",
      "Epoch 3444, Loss 2.927867, \n",
      "Params:  tensor([  5.3588, -17.2546])\n",
      "Grad:  tensor([-0.0015,  0.0085])\n",
      "------\n",
      "------\n",
      "Epoch 3445, Loss 2.927866, \n",
      "Params:  tensor([  5.3589, -17.2547])\n",
      "Grad:  tensor([-0.0015,  0.0085])\n",
      "------\n",
      "------\n",
      "Epoch 3446, Loss 2.927866, \n",
      "Params:  tensor([  5.3589, -17.2547])\n",
      "Grad:  tensor([-0.0015,  0.0085])\n",
      "------\n",
      "------\n",
      "Epoch 3447, Loss 2.927866, \n",
      "Params:  tensor([  5.3589, -17.2548])\n",
      "Grad:  tensor([-0.0015,  0.0085])\n",
      "------\n",
      "------\n",
      "Epoch 3448, Loss 2.927864, \n",
      "Params:  tensor([  5.3589, -17.2549])\n",
      "Grad:  tensor([-0.0015,  0.0085])\n",
      "------\n",
      "------\n",
      "Epoch 3449, Loss 2.927863, \n",
      "Params:  tensor([  5.3589, -17.2550])\n",
      "Grad:  tensor([-0.0015,  0.0085])\n",
      "------\n",
      "------\n",
      "Epoch 3450, Loss 2.927863, \n",
      "Params:  tensor([  5.3589, -17.2551])\n",
      "Grad:  tensor([-0.0015,  0.0085])\n",
      "------\n",
      "------\n",
      "Epoch 3451, Loss 2.927862, \n",
      "Params:  tensor([  5.3590, -17.2552])\n",
      "Grad:  tensor([-0.0015,  0.0084])\n",
      "------\n",
      "------\n",
      "Epoch 3452, Loss 2.927863, \n",
      "Params:  tensor([  5.3590, -17.2552])\n",
      "Grad:  tensor([-0.0015,  0.0084])\n",
      "------\n",
      "------\n",
      "Epoch 3453, Loss 2.927860, \n",
      "Params:  tensor([  5.3590, -17.2553])\n",
      "Grad:  tensor([-0.0015,  0.0084])\n",
      "------\n",
      "------\n",
      "Epoch 3454, Loss 2.927860, \n",
      "Params:  tensor([  5.3590, -17.2554])\n",
      "Grad:  tensor([-0.0015,  0.0084])\n",
      "------\n",
      "------\n",
      "Epoch 3455, Loss 2.927860, \n",
      "Params:  tensor([  5.3590, -17.2555])\n",
      "Grad:  tensor([-0.0015,  0.0084])\n",
      "------\n",
      "------\n",
      "Epoch 3456, Loss 2.927859, \n",
      "Params:  tensor([  5.3590, -17.2556])\n",
      "Grad:  tensor([-0.0015,  0.0084])\n",
      "------\n",
      "------\n",
      "Epoch 3457, Loss 2.927858, \n",
      "Params:  tensor([  5.3590, -17.2557])\n",
      "Grad:  tensor([-0.0015,  0.0084])\n",
      "------\n",
      "------\n",
      "Epoch 3458, Loss 2.927858, \n",
      "Params:  tensor([  5.3591, -17.2557])\n",
      "Grad:  tensor([-0.0015,  0.0083])\n",
      "------\n",
      "------\n",
      "Epoch 3459, Loss 2.927856, \n",
      "Params:  tensor([  5.3591, -17.2558])\n",
      "Grad:  tensor([-0.0015,  0.0083])\n",
      "------\n",
      "------\n",
      "Epoch 3460, Loss 2.927857, \n",
      "Params:  tensor([  5.3591, -17.2559])\n",
      "Grad:  tensor([-0.0015,  0.0083])\n",
      "------\n",
      "------\n",
      "Epoch 3461, Loss 2.927854, \n",
      "Params:  tensor([  5.3591, -17.2560])\n",
      "Grad:  tensor([-0.0015,  0.0083])\n",
      "------\n",
      "------\n",
      "Epoch 3462, Loss 2.927855, \n",
      "Params:  tensor([  5.3591, -17.2561])\n",
      "Grad:  tensor([-0.0015,  0.0083])\n",
      "------\n",
      "------\n",
      "Epoch 3463, Loss 2.927854, \n",
      "Params:  tensor([  5.3591, -17.2562])\n",
      "Grad:  tensor([-0.0015,  0.0083])\n",
      "------\n",
      "------\n",
      "Epoch 3464, Loss 2.927854, \n",
      "Params:  tensor([  5.3591, -17.2562])\n",
      "Grad:  tensor([-0.0015,  0.0083])\n",
      "------\n",
      "------\n",
      "Epoch 3465, Loss 2.927851, \n",
      "Params:  tensor([  5.3592, -17.2563])\n",
      "Grad:  tensor([-0.0014,  0.0082])\n",
      "------\n",
      "------\n",
      "Epoch 3466, Loss 2.927853, \n",
      "Params:  tensor([  5.3592, -17.2564])\n",
      "Grad:  tensor([-0.0015,  0.0082])\n",
      "------\n",
      "------\n",
      "Epoch 3467, Loss 2.927852, \n",
      "Params:  tensor([  5.3592, -17.2565])\n",
      "Grad:  tensor([-0.0014,  0.0082])\n",
      "------\n",
      "------\n",
      "Epoch 3468, Loss 2.927850, \n",
      "Params:  tensor([  5.3592, -17.2566])\n",
      "Grad:  tensor([-0.0014,  0.0082])\n",
      "------\n",
      "------\n",
      "Epoch 3469, Loss 2.927849, \n",
      "Params:  tensor([  5.3592, -17.2567])\n",
      "Grad:  tensor([-0.0015,  0.0082])\n",
      "------\n",
      "------\n",
      "Epoch 3470, Loss 2.927849, \n",
      "Params:  tensor([  5.3592, -17.2567])\n",
      "Grad:  tensor([-0.0015,  0.0082])\n",
      "------\n",
      "------\n",
      "Epoch 3471, Loss 2.927848, \n",
      "Params:  tensor([  5.3592, -17.2568])\n",
      "Grad:  tensor([-0.0014,  0.0082])\n",
      "------\n",
      "------\n",
      "Epoch 3472, Loss 2.927848, \n",
      "Params:  tensor([  5.3593, -17.2569])\n",
      "Grad:  tensor([-0.0014,  0.0081])\n",
      "------\n",
      "------\n",
      "Epoch 3473, Loss 2.927846, \n",
      "Params:  tensor([  5.3593, -17.2570])\n",
      "Grad:  tensor([-0.0015,  0.0081])\n",
      "------\n",
      "------\n",
      "Epoch 3474, Loss 2.927846, \n",
      "Params:  tensor([  5.3593, -17.2571])\n",
      "Grad:  tensor([-0.0015,  0.0081])\n",
      "------\n",
      "------\n",
      "Epoch 3475, Loss 2.927845, \n",
      "Params:  tensor([  5.3593, -17.2571])\n",
      "Grad:  tensor([-0.0014,  0.0081])\n",
      "------\n",
      "------\n",
      "Epoch 3476, Loss 2.927844, \n",
      "Params:  tensor([  5.3593, -17.2572])\n",
      "Grad:  tensor([-0.0014,  0.0081])\n",
      "------\n",
      "------\n",
      "Epoch 3477, Loss 2.927844, \n",
      "Params:  tensor([  5.3593, -17.2573])\n",
      "Grad:  tensor([-0.0014,  0.0081])\n",
      "------\n",
      "------\n",
      "Epoch 3478, Loss 2.927844, \n",
      "Params:  tensor([  5.3593, -17.2574])\n",
      "Grad:  tensor([-0.0014,  0.0081])\n",
      "------\n",
      "------\n",
      "Epoch 3479, Loss 2.927843, \n",
      "Params:  tensor([  5.3594, -17.2575])\n",
      "Grad:  tensor([-0.0014,  0.0081])\n",
      "------\n",
      "------\n",
      "Epoch 3480, Loss 2.927843, \n",
      "Params:  tensor([  5.3594, -17.2575])\n",
      "Grad:  tensor([-0.0014,  0.0080])\n",
      "------\n",
      "------\n",
      "Epoch 3481, Loss 2.927842, \n",
      "Params:  tensor([  5.3594, -17.2576])\n",
      "Grad:  tensor([-0.0014,  0.0080])\n",
      "------\n",
      "------\n",
      "Epoch 3482, Loss 2.927840, \n",
      "Params:  tensor([  5.3594, -17.2577])\n",
      "Grad:  tensor([-0.0014,  0.0080])\n",
      "------\n",
      "------\n",
      "Epoch 3483, Loss 2.927842, \n",
      "Params:  tensor([  5.3594, -17.2578])\n",
      "Grad:  tensor([-0.0014,  0.0080])\n",
      "------\n",
      "------\n",
      "Epoch 3484, Loss 2.927839, \n",
      "Params:  tensor([  5.3594, -17.2579])\n",
      "Grad:  tensor([-0.0014,  0.0080])\n",
      "------\n",
      "------\n",
      "Epoch 3485, Loss 2.927838, \n",
      "Params:  tensor([  5.3594, -17.2579])\n",
      "Grad:  tensor([-0.0014,  0.0080])\n",
      "------\n",
      "------\n",
      "Epoch 3486, Loss 2.927839, \n",
      "Params:  tensor([  5.3595, -17.2580])\n",
      "Grad:  tensor([-0.0014,  0.0080])\n",
      "------\n",
      "------\n",
      "Epoch 3487, Loss 2.927838, \n",
      "Params:  tensor([  5.3595, -17.2581])\n",
      "Grad:  tensor([-0.0014,  0.0079])\n",
      "------\n",
      "------\n",
      "Epoch 3488, Loss 2.927837, \n",
      "Params:  tensor([  5.3595, -17.2582])\n",
      "Grad:  tensor([-0.0014,  0.0079])\n",
      "------\n",
      "------\n",
      "Epoch 3489, Loss 2.927835, \n",
      "Params:  tensor([  5.3595, -17.2583])\n",
      "Grad:  tensor([-0.0014,  0.0079])\n",
      "------\n",
      "------\n",
      "Epoch 3490, Loss 2.927837, \n",
      "Params:  tensor([  5.3595, -17.2583])\n",
      "Grad:  tensor([-0.0014,  0.0079])\n",
      "------\n",
      "------\n",
      "Epoch 3491, Loss 2.927836, \n",
      "Params:  tensor([  5.3595, -17.2584])\n",
      "Grad:  tensor([-0.0014,  0.0079])\n",
      "------\n",
      "------\n",
      "Epoch 3492, Loss 2.927835, \n",
      "Params:  tensor([  5.3595, -17.2585])\n",
      "Grad:  tensor([-0.0014,  0.0079])\n",
      "------\n",
      "------\n",
      "Epoch 3493, Loss 2.927833, \n",
      "Params:  tensor([  5.3596, -17.2586])\n",
      "Grad:  tensor([-0.0014,  0.0079])\n",
      "------\n",
      "------\n",
      "Epoch 3494, Loss 2.927833, \n",
      "Params:  tensor([  5.3596, -17.2587])\n",
      "Grad:  tensor([-0.0014,  0.0079])\n",
      "------\n",
      "------\n",
      "Epoch 3495, Loss 2.927833, \n",
      "Params:  tensor([  5.3596, -17.2587])\n",
      "Grad:  tensor([-0.0014,  0.0078])\n",
      "------\n",
      "------\n",
      "Epoch 3496, Loss 2.927832, \n",
      "Params:  tensor([  5.3596, -17.2588])\n",
      "Grad:  tensor([-0.0014,  0.0078])\n",
      "------\n",
      "------\n",
      "Epoch 3497, Loss 2.927831, \n",
      "Params:  tensor([  5.3596, -17.2589])\n",
      "Grad:  tensor([-0.0014,  0.0078])\n",
      "------\n",
      "------\n",
      "Epoch 3498, Loss 2.927830, \n",
      "Params:  tensor([  5.3596, -17.2590])\n",
      "Grad:  tensor([-0.0014,  0.0078])\n",
      "------\n",
      "------\n",
      "Epoch 3499, Loss 2.927830, \n",
      "Params:  tensor([  5.3596, -17.2590])\n",
      "Grad:  tensor([-0.0014,  0.0078])\n",
      "------\n",
      "------\n",
      "Epoch 3500, Loss 2.927830, \n",
      "Params:  tensor([  5.3597, -17.2591])\n",
      "Grad:  tensor([-0.0014,  0.0078])\n",
      "------\n",
      "------\n",
      "Epoch 3501, Loss 2.927829, \n",
      "Params:  tensor([  5.3597, -17.2592])\n",
      "Grad:  tensor([-0.0014,  0.0078])\n",
      "------\n",
      "------\n",
      "Epoch 3502, Loss 2.927828, \n",
      "Params:  tensor([  5.3597, -17.2593])\n",
      "Grad:  tensor([-0.0014,  0.0077])\n",
      "------\n",
      "------\n",
      "Epoch 3503, Loss 2.927828, \n",
      "Params:  tensor([  5.3597, -17.2594])\n",
      "Grad:  tensor([-0.0014,  0.0077])\n",
      "------\n",
      "------\n",
      "Epoch 3504, Loss 2.927827, \n",
      "Params:  tensor([  5.3597, -17.2594])\n",
      "Grad:  tensor([-0.0014,  0.0077])\n",
      "------\n",
      "------\n",
      "Epoch 3505, Loss 2.927825, \n",
      "Params:  tensor([  5.3597, -17.2595])\n",
      "Grad:  tensor([-0.0014,  0.0077])\n",
      "------\n",
      "------\n",
      "Epoch 3506, Loss 2.927827, \n",
      "Params:  tensor([  5.3597, -17.2596])\n",
      "Grad:  tensor([-0.0014,  0.0077])\n",
      "------\n",
      "------\n",
      "Epoch 3507, Loss 2.927825, \n",
      "Params:  tensor([  5.3597, -17.2597])\n",
      "Grad:  tensor([-0.0014,  0.0077])\n",
      "------\n",
      "------\n",
      "Epoch 3508, Loss 2.927824, \n",
      "Params:  tensor([  5.3598, -17.2597])\n",
      "Grad:  tensor([-0.0013,  0.0077])\n",
      "------\n",
      "------\n",
      "Epoch 3509, Loss 2.927824, \n",
      "Params:  tensor([  5.3598, -17.2598])\n",
      "Grad:  tensor([-0.0013,  0.0077])\n",
      "------\n",
      "------\n",
      "Epoch 3510, Loss 2.927824, \n",
      "Params:  tensor([  5.3598, -17.2599])\n",
      "Grad:  tensor([-0.0013,  0.0076])\n",
      "------\n",
      "------\n",
      "Epoch 3511, Loss 2.927822, \n",
      "Params:  tensor([  5.3598, -17.2600])\n",
      "Grad:  tensor([-0.0014,  0.0076])\n",
      "------\n",
      "------\n",
      "Epoch 3512, Loss 2.927822, \n",
      "Params:  tensor([  5.3598, -17.2600])\n",
      "Grad:  tensor([-0.0014,  0.0076])\n",
      "------\n",
      "------\n",
      "Epoch 3513, Loss 2.927821, \n",
      "Params:  tensor([  5.3598, -17.2601])\n",
      "Grad:  tensor([-0.0013,  0.0076])\n",
      "------\n",
      "------\n",
      "Epoch 3514, Loss 2.927820, \n",
      "Params:  tensor([  5.3598, -17.2602])\n",
      "Grad:  tensor([-0.0013,  0.0076])\n",
      "------\n",
      "------\n",
      "Epoch 3515, Loss 2.927820, \n",
      "Params:  tensor([  5.3599, -17.2603])\n",
      "Grad:  tensor([-0.0014,  0.0076])\n",
      "------\n",
      "------\n",
      "Epoch 3516, Loss 2.927821, \n",
      "Params:  tensor([  5.3599, -17.2604])\n",
      "Grad:  tensor([-0.0014,  0.0076])\n",
      "------\n",
      "------\n",
      "Epoch 3517, Loss 2.927819, \n",
      "Params:  tensor([  5.3599, -17.2604])\n",
      "Grad:  tensor([-0.0014,  0.0075])\n",
      "------\n",
      "------\n",
      "Epoch 3518, Loss 2.927819, \n",
      "Params:  tensor([  5.3599, -17.2605])\n",
      "Grad:  tensor([-0.0014,  0.0075])\n",
      "------\n",
      "------\n",
      "Epoch 3519, Loss 2.927819, \n",
      "Params:  tensor([  5.3599, -17.2606])\n",
      "Grad:  tensor([-0.0013,  0.0075])\n",
      "------\n",
      "------\n",
      "Epoch 3520, Loss 2.927817, \n",
      "Params:  tensor([  5.3599, -17.2607])\n",
      "Grad:  tensor([-0.0013,  0.0075])\n",
      "------\n",
      "------\n",
      "Epoch 3521, Loss 2.927817, \n",
      "Params:  tensor([  5.3599, -17.2607])\n",
      "Grad:  tensor([-0.0013,  0.0075])\n",
      "------\n",
      "------\n",
      "Epoch 3522, Loss 2.927816, \n",
      "Params:  tensor([  5.3599, -17.2608])\n",
      "Grad:  tensor([-0.0013,  0.0075])\n",
      "------\n",
      "------\n",
      "Epoch 3523, Loss 2.927815, \n",
      "Params:  tensor([  5.3600, -17.2609])\n",
      "Grad:  tensor([-0.0013,  0.0075])\n",
      "------\n",
      "------\n",
      "Epoch 3524, Loss 2.927816, \n",
      "Params:  tensor([  5.3600, -17.2610])\n",
      "Grad:  tensor([-0.0013,  0.0075])\n",
      "------\n",
      "------\n",
      "Epoch 3525, Loss 2.927815, \n",
      "Params:  tensor([  5.3600, -17.2610])\n",
      "Grad:  tensor([-0.0013,  0.0074])\n",
      "------\n",
      "------\n",
      "Epoch 3526, Loss 2.927814, \n",
      "Params:  tensor([  5.3600, -17.2611])\n",
      "Grad:  tensor([-0.0013,  0.0074])\n",
      "------\n",
      "------\n",
      "Epoch 3527, Loss 2.927813, \n",
      "Params:  tensor([  5.3600, -17.2612])\n",
      "Grad:  tensor([-0.0013,  0.0074])\n",
      "------\n",
      "------\n",
      "Epoch 3528, Loss 2.927812, \n",
      "Params:  tensor([  5.3600, -17.2612])\n",
      "Grad:  tensor([-0.0013,  0.0074])\n",
      "------\n",
      "------\n",
      "Epoch 3529, Loss 2.927811, \n",
      "Params:  tensor([  5.3600, -17.2613])\n",
      "Grad:  tensor([-0.0013,  0.0074])\n",
      "------\n",
      "------\n",
      "Epoch 3530, Loss 2.927812, \n",
      "Params:  tensor([  5.3601, -17.2614])\n",
      "Grad:  tensor([-0.0013,  0.0074])\n",
      "------\n",
      "------\n",
      "Epoch 3531, Loss 2.927812, \n",
      "Params:  tensor([  5.3601, -17.2615])\n",
      "Grad:  tensor([-0.0013,  0.0074])\n",
      "------\n",
      "------\n",
      "Epoch 3532, Loss 2.927810, \n",
      "Params:  tensor([  5.3601, -17.2615])\n",
      "Grad:  tensor([-0.0013,  0.0074])\n",
      "------\n",
      "------\n",
      "Epoch 3533, Loss 2.927809, \n",
      "Params:  tensor([  5.3601, -17.2616])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3534, Loss 2.927810, \n",
      "Params:  tensor([  5.3601, -17.2617])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3535, Loss 2.927809, \n",
      "Params:  tensor([  5.3601, -17.2618])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3536, Loss 2.927808, \n",
      "Params:  tensor([  5.3601, -17.2618])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3537, Loss 2.927808, \n",
      "Params:  tensor([  5.3601, -17.2619])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3538, Loss 2.927806, \n",
      "Params:  tensor([  5.3602, -17.2620])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3539, Loss 2.927806, \n",
      "Params:  tensor([  5.3602, -17.2621])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3540, Loss 2.927805, \n",
      "Params:  tensor([  5.3602, -17.2621])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3541, Loss 2.927804, \n",
      "Params:  tensor([  5.3602, -17.2622])\n",
      "Grad:  tensor([-0.0013,  0.0073])\n",
      "------\n",
      "------\n",
      "Epoch 3542, Loss 2.927805, \n",
      "Params:  tensor([  5.3602, -17.2623])\n",
      "Grad:  tensor([-0.0013,  0.0072])\n",
      "------\n",
      "------\n",
      "Epoch 3543, Loss 2.927804, \n",
      "Params:  tensor([  5.3602, -17.2623])\n",
      "Grad:  tensor([-0.0013,  0.0072])\n",
      "------\n",
      "------\n",
      "Epoch 3544, Loss 2.927805, \n",
      "Params:  tensor([  5.3602, -17.2624])\n",
      "Grad:  tensor([-0.0013,  0.0072])\n",
      "------\n",
      "------\n",
      "Epoch 3545, Loss 2.927804, \n",
      "Params:  tensor([  5.3602, -17.2625])\n",
      "Grad:  tensor([-0.0013,  0.0072])\n",
      "------\n",
      "------\n",
      "Epoch 3546, Loss 2.927804, \n",
      "Params:  tensor([  5.3603, -17.2626])\n",
      "Grad:  tensor([-0.0013,  0.0072])\n",
      "------\n",
      "------\n",
      "Epoch 3547, Loss 2.927803, \n",
      "Params:  tensor([  5.3603, -17.2626])\n",
      "Grad:  tensor([-0.0013,  0.0072])\n",
      "------\n",
      "------\n",
      "Epoch 3548, Loss 2.927802, \n",
      "Params:  tensor([  5.3603, -17.2627])\n",
      "Grad:  tensor([-0.0013,  0.0072])\n",
      "------\n",
      "------\n",
      "Epoch 3549, Loss 2.927801, \n",
      "Params:  tensor([  5.3603, -17.2628])\n",
      "Grad:  tensor([-0.0013,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3550, Loss 2.927801, \n",
      "Params:  tensor([  5.3603, -17.2628])\n",
      "Grad:  tensor([-0.0013,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3551, Loss 2.927799, \n",
      "Params:  tensor([  5.3603, -17.2629])\n",
      "Grad:  tensor([-0.0012,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3552, Loss 2.927801, \n",
      "Params:  tensor([  5.3603, -17.2630])\n",
      "Grad:  tensor([-0.0012,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3553, Loss 2.927798, \n",
      "Params:  tensor([  5.3603, -17.2631])\n",
      "Grad:  tensor([-0.0012,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3554, Loss 2.927798, \n",
      "Params:  tensor([  5.3604, -17.2631])\n",
      "Grad:  tensor([-0.0012,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3555, Loss 2.927798, \n",
      "Params:  tensor([  5.3604, -17.2632])\n",
      "Grad:  tensor([-0.0013,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3556, Loss 2.927798, \n",
      "Params:  tensor([  5.3604, -17.2633])\n",
      "Grad:  tensor([-0.0013,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3557, Loss 2.927798, \n",
      "Params:  tensor([  5.3604, -17.2633])\n",
      "Grad:  tensor([-0.0013,  0.0071])\n",
      "------\n",
      "------\n",
      "Epoch 3558, Loss 2.927796, \n",
      "Params:  tensor([  5.3604, -17.2634])\n",
      "Grad:  tensor([-0.0013,  0.0070])\n",
      "------\n",
      "------\n",
      "Epoch 3559, Loss 2.927795, \n",
      "Params:  tensor([  5.3604, -17.2635])\n",
      "Grad:  tensor([-0.0013,  0.0070])\n",
      "------\n",
      "------\n",
      "Epoch 3560, Loss 2.927796, \n",
      "Params:  tensor([  5.3604, -17.2636])\n",
      "Grad:  tensor([-0.0013,  0.0070])\n",
      "------\n",
      "------\n",
      "Epoch 3561, Loss 2.927794, \n",
      "Params:  tensor([  5.3604, -17.2636])\n",
      "Grad:  tensor([-0.0013,  0.0070])\n",
      "------\n",
      "------\n",
      "Epoch 3562, Loss 2.927795, \n",
      "Params:  tensor([  5.3605, -17.2637])\n",
      "Grad:  tensor([-0.0013,  0.0070])\n",
      "------\n",
      "------\n",
      "Epoch 3563, Loss 2.927795, \n",
      "Params:  tensor([  5.3605, -17.2638])\n",
      "Grad:  tensor([-0.0013,  0.0070])\n",
      "------\n",
      "------\n",
      "Epoch 3564, Loss 2.927793, \n",
      "Params:  tensor([  5.3605, -17.2638])\n",
      "Grad:  tensor([-0.0013,  0.0070])\n",
      "------\n",
      "------\n",
      "Epoch 3565, Loss 2.927795, \n",
      "Params:  tensor([  5.3605, -17.2639])\n",
      "Grad:  tensor([-0.0012,  0.0070])\n",
      "------\n",
      "------\n",
      "Epoch 3566, Loss 2.927791, \n",
      "Params:  tensor([  5.3605, -17.2640])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3567, Loss 2.927791, \n",
      "Params:  tensor([  5.3605, -17.2640])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3568, Loss 2.927791, \n",
      "Params:  tensor([  5.3605, -17.2641])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3569, Loss 2.927790, \n",
      "Params:  tensor([  5.3605, -17.2642])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3570, Loss 2.927790, \n",
      "Params:  tensor([  5.3606, -17.2642])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3571, Loss 2.927789, \n",
      "Params:  tensor([  5.3606, -17.2643])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3572, Loss 2.927790, \n",
      "Params:  tensor([  5.3606, -17.2644])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3573, Loss 2.927789, \n",
      "Params:  tensor([  5.3606, -17.2645])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3574, Loss 2.927789, \n",
      "Params:  tensor([  5.3606, -17.2645])\n",
      "Grad:  tensor([-0.0012,  0.0069])\n",
      "------\n",
      "------\n",
      "Epoch 3575, Loss 2.927789, \n",
      "Params:  tensor([  5.3606, -17.2646])\n",
      "Grad:  tensor([-0.0012,  0.0068])\n",
      "------\n",
      "------\n",
      "Epoch 3576, Loss 2.927787, \n",
      "Params:  tensor([  5.3606, -17.2647])\n",
      "Grad:  tensor([-0.0012,  0.0068])\n",
      "------\n",
      "------\n",
      "Epoch 3577, Loss 2.927786, \n",
      "Params:  tensor([  5.3606, -17.2647])\n",
      "Grad:  tensor([-0.0012,  0.0068])\n",
      "------\n",
      "------\n",
      "Epoch 3578, Loss 2.927788, \n",
      "Params:  tensor([  5.3607, -17.2648])\n",
      "Grad:  tensor([-0.0012,  0.0068])\n",
      "------\n",
      "------\n",
      "Epoch 3579, Loss 2.927785, \n",
      "Params:  tensor([  5.3607, -17.2649])\n",
      "Grad:  tensor([-0.0012,  0.0068])\n",
      "------\n",
      "------\n",
      "Epoch 3580, Loss 2.927785, \n",
      "Params:  tensor([  5.3607, -17.2649])\n",
      "Grad:  tensor([-0.0012,  0.0068])\n",
      "------\n",
      "------\n",
      "Epoch 3581, Loss 2.927786, \n",
      "Params:  tensor([  5.3607, -17.2650])\n",
      "Grad:  tensor([-0.0012,  0.0068])\n",
      "------\n",
      "------\n",
      "Epoch 3582, Loss 2.927785, \n",
      "Params:  tensor([  5.3607, -17.2651])\n",
      "Grad:  tensor([-0.0012,  0.0068])\n",
      "------\n",
      "------\n",
      "Epoch 3583, Loss 2.927784, \n",
      "Params:  tensor([  5.3607, -17.2651])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3584, Loss 2.927784, \n",
      "Params:  tensor([  5.3607, -17.2652])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3585, Loss 2.927783, \n",
      "Params:  tensor([  5.3607, -17.2653])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3586, Loss 2.927783, \n",
      "Params:  tensor([  5.3607, -17.2653])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3587, Loss 2.927781, \n",
      "Params:  tensor([  5.3608, -17.2654])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3588, Loss 2.927782, \n",
      "Params:  tensor([  5.3608, -17.2655])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3589, Loss 2.927781, \n",
      "Params:  tensor([  5.3608, -17.2655])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3590, Loss 2.927781, \n",
      "Params:  tensor([  5.3608, -17.2656])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3591, Loss 2.927781, \n",
      "Params:  tensor([  5.3608, -17.2657])\n",
      "Grad:  tensor([-0.0012,  0.0067])\n",
      "------\n",
      "------\n",
      "Epoch 3592, Loss 2.927780, \n",
      "Params:  tensor([  5.3608, -17.2657])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3593, Loss 2.927780, \n",
      "Params:  tensor([  5.3608, -17.2658])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3594, Loss 2.927778, \n",
      "Params:  tensor([  5.3608, -17.2659])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3595, Loss 2.927779, \n",
      "Params:  tensor([  5.3609, -17.2659])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3596, Loss 2.927778, \n",
      "Params:  tensor([  5.3609, -17.2660])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3597, Loss 2.927778, \n",
      "Params:  tensor([  5.3609, -17.2661])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3598, Loss 2.927779, \n",
      "Params:  tensor([  5.3609, -17.2661])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3599, Loss 2.927777, \n",
      "Params:  tensor([  5.3609, -17.2662])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3600, Loss 2.927776, \n",
      "Params:  tensor([  5.3609, -17.2663])\n",
      "Grad:  tensor([-0.0012,  0.0066])\n",
      "------\n",
      "------\n",
      "Epoch 3601, Loss 2.927775, \n",
      "Params:  tensor([  5.3609, -17.2663])\n",
      "Grad:  tensor([-0.0012,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3602, Loss 2.927776, \n",
      "Params:  tensor([  5.3609, -17.2664])\n",
      "Grad:  tensor([-0.0012,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3603, Loss 2.927773, \n",
      "Params:  tensor([  5.3609, -17.2665])\n",
      "Grad:  tensor([-0.0012,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3604, Loss 2.927775, \n",
      "Params:  tensor([  5.3610, -17.2665])\n",
      "Grad:  tensor([-0.0012,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3605, Loss 2.927775, \n",
      "Params:  tensor([  5.3610, -17.2666])\n",
      "Grad:  tensor([-0.0011,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3606, Loss 2.927775, \n",
      "Params:  tensor([  5.3610, -17.2667])\n",
      "Grad:  tensor([-0.0011,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3607, Loss 2.927773, \n",
      "Params:  tensor([  5.3610, -17.2667])\n",
      "Grad:  tensor([-0.0011,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3608, Loss 2.927773, \n",
      "Params:  tensor([  5.3610, -17.2668])\n",
      "Grad:  tensor([-0.0011,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3609, Loss 2.927773, \n",
      "Params:  tensor([  5.3610, -17.2668])\n",
      "Grad:  tensor([-0.0011,  0.0065])\n",
      "------\n",
      "------\n",
      "Epoch 3610, Loss 2.927772, \n",
      "Params:  tensor([  5.3610, -17.2669])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3611, Loss 2.927772, \n",
      "Params:  tensor([  5.3610, -17.2670])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3612, Loss 2.927770, \n",
      "Params:  tensor([  5.3611, -17.2670])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3613, Loss 2.927772, \n",
      "Params:  tensor([  5.3611, -17.2671])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3614, Loss 2.927771, \n",
      "Params:  tensor([  5.3611, -17.2672])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3615, Loss 2.927770, \n",
      "Params:  tensor([  5.3611, -17.2672])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3616, Loss 2.927770, \n",
      "Params:  tensor([  5.3611, -17.2673])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3617, Loss 2.927769, \n",
      "Params:  tensor([  5.3611, -17.2674])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3618, Loss 2.927768, \n",
      "Params:  tensor([  5.3611, -17.2674])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3619, Loss 2.927769, \n",
      "Params:  tensor([  5.3611, -17.2675])\n",
      "Grad:  tensor([-0.0011,  0.0064])\n",
      "------\n",
      "------\n",
      "Epoch 3620, Loss 2.927768, \n",
      "Params:  tensor([  5.3611, -17.2675])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3621, Loss 2.927767, \n",
      "Params:  tensor([  5.3612, -17.2676])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3622, Loss 2.927767, \n",
      "Params:  tensor([  5.3612, -17.2677])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3623, Loss 2.927767, \n",
      "Params:  tensor([  5.3612, -17.2677])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3624, Loss 2.927765, \n",
      "Params:  tensor([  5.3612, -17.2678])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3625, Loss 2.927766, \n",
      "Params:  tensor([  5.3612, -17.2679])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3626, Loss 2.927765, \n",
      "Params:  tensor([  5.3612, -17.2679])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3627, Loss 2.927765, \n",
      "Params:  tensor([  5.3612, -17.2680])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3628, Loss 2.927764, \n",
      "Params:  tensor([  5.3612, -17.2681])\n",
      "Grad:  tensor([-0.0011,  0.0063])\n",
      "------\n",
      "------\n",
      "Epoch 3629, Loss 2.927764, \n",
      "Params:  tensor([  5.3612, -17.2681])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3630, Loss 2.927764, \n",
      "Params:  tensor([  5.3613, -17.2682])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3631, Loss 2.927762, \n",
      "Params:  tensor([  5.3613, -17.2682])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3632, Loss 2.927763, \n",
      "Params:  tensor([  5.3613, -17.2683])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3633, Loss 2.927763, \n",
      "Params:  tensor([  5.3613, -17.2684])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3634, Loss 2.927762, \n",
      "Params:  tensor([  5.3613, -17.2684])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3635, Loss 2.927761, \n",
      "Params:  tensor([  5.3613, -17.2685])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3636, Loss 2.927762, \n",
      "Params:  tensor([  5.3613, -17.2685])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3637, Loss 2.927759, \n",
      "Params:  tensor([  5.3613, -17.2686])\n",
      "Grad:  tensor([-0.0011,  0.0062])\n",
      "------\n",
      "------\n",
      "Epoch 3638, Loss 2.927761, \n",
      "Params:  tensor([  5.3613, -17.2687])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3639, Loss 2.927761, \n",
      "Params:  tensor([  5.3614, -17.2687])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3640, Loss 2.927760, \n",
      "Params:  tensor([  5.3614, -17.2688])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3641, Loss 2.927759, \n",
      "Params:  tensor([  5.3614, -17.2689])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3642, Loss 2.927758, \n",
      "Params:  tensor([  5.3614, -17.2689])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3643, Loss 2.927759, \n",
      "Params:  tensor([  5.3614, -17.2690])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3644, Loss 2.927757, \n",
      "Params:  tensor([  5.3614, -17.2690])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3645, Loss 2.927758, \n",
      "Params:  tensor([  5.3614, -17.2691])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3646, Loss 2.927757, \n",
      "Params:  tensor([  5.3614, -17.2692])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3647, Loss 2.927757, \n",
      "Params:  tensor([  5.3614, -17.2692])\n",
      "Grad:  tensor([-0.0011,  0.0061])\n",
      "------\n",
      "------\n",
      "Epoch 3648, Loss 2.927757, \n",
      "Params:  tensor([  5.3614, -17.2693])\n",
      "Grad:  tensor([-0.0011,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3649, Loss 2.927756, \n",
      "Params:  tensor([  5.3615, -17.2693])\n",
      "Grad:  tensor([-0.0011,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3650, Loss 2.927757, \n",
      "Params:  tensor([  5.3615, -17.2694])\n",
      "Grad:  tensor([-0.0011,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3651, Loss 2.927756, \n",
      "Params:  tensor([  5.3615, -17.2695])\n",
      "Grad:  tensor([-0.0011,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3652, Loss 2.927756, \n",
      "Params:  tensor([  5.3615, -17.2695])\n",
      "Grad:  tensor([-0.0010,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3653, Loss 2.927755, \n",
      "Params:  tensor([  5.3615, -17.2696])\n",
      "Grad:  tensor([-0.0010,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3654, Loss 2.927755, \n",
      "Params:  tensor([  5.3615, -17.2696])\n",
      "Grad:  tensor([-0.0010,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3655, Loss 2.927754, \n",
      "Params:  tensor([  5.3615, -17.2697])\n",
      "Grad:  tensor([-0.0010,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3656, Loss 2.927754, \n",
      "Params:  tensor([  5.3615, -17.2698])\n",
      "Grad:  tensor([-0.0010,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3657, Loss 2.927755, \n",
      "Params:  tensor([  5.3615, -17.2698])\n",
      "Grad:  tensor([-0.0010,  0.0060])\n",
      "------\n",
      "------\n",
      "Epoch 3658, Loss 2.927753, \n",
      "Params:  tensor([  5.3616, -17.2699])\n",
      "Grad:  tensor([-0.0010,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3659, Loss 2.927752, \n",
      "Params:  tensor([  5.3616, -17.2699])\n",
      "Grad:  tensor([-0.0010,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3660, Loss 2.927754, \n",
      "Params:  tensor([  5.3616, -17.2700])\n",
      "Grad:  tensor([-0.0010,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3661, Loss 2.927752, \n",
      "Params:  tensor([  5.3616, -17.2701])\n",
      "Grad:  tensor([-0.0010,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3662, Loss 2.927751, \n",
      "Params:  tensor([  5.3616, -17.2701])\n",
      "Grad:  tensor([-0.0010,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3663, Loss 2.927752, \n",
      "Params:  tensor([  5.3616, -17.2702])\n",
      "Grad:  tensor([-0.0010,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3664, Loss 2.927750, \n",
      "Params:  tensor([  5.3616, -17.2702])\n",
      "Grad:  tensor([-0.0011,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3665, Loss 2.927749, \n",
      "Params:  tensor([  5.3616, -17.2703])\n",
      "Grad:  tensor([-0.0010,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3666, Loss 2.927751, \n",
      "Params:  tensor([  5.3616, -17.2703])\n",
      "Grad:  tensor([-0.0010,  0.0059])\n",
      "------\n",
      "------\n",
      "Epoch 3667, Loss 2.927750, \n",
      "Params:  tensor([  5.3616, -17.2704])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3668, Loss 2.927750, \n",
      "Params:  tensor([  5.3617, -17.2705])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3669, Loss 2.927747, \n",
      "Params:  tensor([  5.3617, -17.2705])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3670, Loss 2.927749, \n",
      "Params:  tensor([  5.3617, -17.2706])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3671, Loss 2.927747, \n",
      "Params:  tensor([  5.3617, -17.2706])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3672, Loss 2.927748, \n",
      "Params:  tensor([  5.3617, -17.2707])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3673, Loss 2.927748, \n",
      "Params:  tensor([  5.3617, -17.2708])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3674, Loss 2.927747, \n",
      "Params:  tensor([  5.3617, -17.2708])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3675, Loss 2.927747, \n",
      "Params:  tensor([  5.3617, -17.2709])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3676, Loss 2.927748, \n",
      "Params:  tensor([  5.3617, -17.2709])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3677, Loss 2.927747, \n",
      "Params:  tensor([  5.3617, -17.2710])\n",
      "Grad:  tensor([-0.0010,  0.0058])\n",
      "------\n",
      "------\n",
      "Epoch 3678, Loss 2.927747, \n",
      "Params:  tensor([  5.3618, -17.2710])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3679, Loss 2.927745, \n",
      "Params:  tensor([  5.3618, -17.2711])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3680, Loss 2.927745, \n",
      "Params:  tensor([  5.3618, -17.2712])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3681, Loss 2.927746, \n",
      "Params:  tensor([  5.3618, -17.2712])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3682, Loss 2.927744, \n",
      "Params:  tensor([  5.3618, -17.2713])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3683, Loss 2.927743, \n",
      "Params:  tensor([  5.3618, -17.2713])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3684, Loss 2.927743, \n",
      "Params:  tensor([  5.3618, -17.2714])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3685, Loss 2.927743, \n",
      "Params:  tensor([  5.3618, -17.2714])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3686, Loss 2.927743, \n",
      "Params:  tensor([  5.3618, -17.2715])\n",
      "Grad:  tensor([-0.0010,  0.0057])\n",
      "------\n",
      "------\n",
      "Epoch 3687, Loss 2.927743, \n",
      "Params:  tensor([  5.3618, -17.2716])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3688, Loss 2.927744, \n",
      "Params:  tensor([  5.3619, -17.2716])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3689, Loss 2.927742, \n",
      "Params:  tensor([  5.3619, -17.2717])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3690, Loss 2.927742, \n",
      "Params:  tensor([  5.3619, -17.2717])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3691, Loss 2.927742, \n",
      "Params:  tensor([  5.3619, -17.2718])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3692, Loss 2.927742, \n",
      "Params:  tensor([  5.3619, -17.2718])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3693, Loss 2.927741, \n",
      "Params:  tensor([  5.3619, -17.2719])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3694, Loss 2.927741, \n",
      "Params:  tensor([  5.3619, -17.2719])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3695, Loss 2.927741, \n",
      "Params:  tensor([  5.3619, -17.2720])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3696, Loss 2.927742, \n",
      "Params:  tensor([  5.3619, -17.2721])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3697, Loss 2.927741, \n",
      "Params:  tensor([  5.3619, -17.2721])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3698, Loss 2.927741, \n",
      "Params:  tensor([  5.3620, -17.2722])\n",
      "Grad:  tensor([-0.0010,  0.0056])\n",
      "------\n",
      "------\n",
      "Epoch 3699, Loss 2.927740, \n",
      "Params:  tensor([  5.3620, -17.2722])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3700, Loss 2.927739, \n",
      "Params:  tensor([  5.3620, -17.2723])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3701, Loss 2.927738, \n",
      "Params:  tensor([  5.3620, -17.2723])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3702, Loss 2.927738, \n",
      "Params:  tensor([  5.3620, -17.2724])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3703, Loss 2.927737, \n",
      "Params:  tensor([  5.3620, -17.2724])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3704, Loss 2.927737, \n",
      "Params:  tensor([  5.3620, -17.2725])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3705, Loss 2.927738, \n",
      "Params:  tensor([  5.3620, -17.2726])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3706, Loss 2.927737, \n",
      "Params:  tensor([  5.3620, -17.2726])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3707, Loss 2.927736, \n",
      "Params:  tensor([  5.3620, -17.2727])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3708, Loss 2.927737, \n",
      "Params:  tensor([  5.3621, -17.2727])\n",
      "Grad:  tensor([-0.0010,  0.0055])\n",
      "------\n",
      "------\n",
      "Epoch 3709, Loss 2.927737, \n",
      "Params:  tensor([  5.3621, -17.2728])\n",
      "Grad:  tensor([-0.0010,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3710, Loss 2.927736, \n",
      "Params:  tensor([  5.3621, -17.2728])\n",
      "Grad:  tensor([-0.0010,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3711, Loss 2.927734, \n",
      "Params:  tensor([  5.3621, -17.2729])\n",
      "Grad:  tensor([-0.0010,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3712, Loss 2.927735, \n",
      "Params:  tensor([  5.3621, -17.2729])\n",
      "Grad:  tensor([-0.0010,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3713, Loss 2.927735, \n",
      "Params:  tensor([  5.3621, -17.2730])\n",
      "Grad:  tensor([-0.0009,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3714, Loss 2.927734, \n",
      "Params:  tensor([  5.3621, -17.2730])\n",
      "Grad:  tensor([-0.0009,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3715, Loss 2.927734, \n",
      "Params:  tensor([  5.3621, -17.2731])\n",
      "Grad:  tensor([-0.0009,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3716, Loss 2.927733, \n",
      "Params:  tensor([  5.3621, -17.2732])\n",
      "Grad:  tensor([-0.0009,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3717, Loss 2.927734, \n",
      "Params:  tensor([  5.3621, -17.2732])\n",
      "Grad:  tensor([-0.0009,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3718, Loss 2.927733, \n",
      "Params:  tensor([  5.3622, -17.2733])\n",
      "Grad:  tensor([-0.0009,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3719, Loss 2.927733, \n",
      "Params:  tensor([  5.3622, -17.2733])\n",
      "Grad:  tensor([-0.0009,  0.0054])\n",
      "------\n",
      "------\n",
      "Epoch 3720, Loss 2.927733, \n",
      "Params:  tensor([  5.3622, -17.2734])\n",
      "Grad:  tensor([-0.0010,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3721, Loss 2.927732, \n",
      "Params:  tensor([  5.3622, -17.2734])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3722, Loss 2.927731, \n",
      "Params:  tensor([  5.3622, -17.2735])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3723, Loss 2.927731, \n",
      "Params:  tensor([  5.3622, -17.2735])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3724, Loss 2.927733, \n",
      "Params:  tensor([  5.3622, -17.2736])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3725, Loss 2.927730, \n",
      "Params:  tensor([  5.3622, -17.2736])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3726, Loss 2.927730, \n",
      "Params:  tensor([  5.3622, -17.2737])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3727, Loss 2.927732, \n",
      "Params:  tensor([  5.3622, -17.2737])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3728, Loss 2.927730, \n",
      "Params:  tensor([  5.3622, -17.2738])\n",
      "Grad:  tensor([-0.0010,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3729, Loss 2.927732, \n",
      "Params:  tensor([  5.3623, -17.2738])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3730, Loss 2.927731, \n",
      "Params:  tensor([  5.3623, -17.2739])\n",
      "Grad:  tensor([-0.0009,  0.0053])\n",
      "------\n",
      "------\n",
      "Epoch 3731, Loss 2.927730, \n",
      "Params:  tensor([  5.3623, -17.2740])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3732, Loss 2.927728, \n",
      "Params:  tensor([  5.3623, -17.2740])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3733, Loss 2.927729, \n",
      "Params:  tensor([  5.3623, -17.2741])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3734, Loss 2.927729, \n",
      "Params:  tensor([  5.3623, -17.2741])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3735, Loss 2.927728, \n",
      "Params:  tensor([  5.3623, -17.2742])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3736, Loss 2.927728, \n",
      "Params:  tensor([  5.3623, -17.2742])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3737, Loss 2.927728, \n",
      "Params:  tensor([  5.3623, -17.2743])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3738, Loss 2.927728, \n",
      "Params:  tensor([  5.3623, -17.2743])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3739, Loss 2.927727, \n",
      "Params:  tensor([  5.3623, -17.2744])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3740, Loss 2.927728, \n",
      "Params:  tensor([  5.3624, -17.2744])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3741, Loss 2.927728, \n",
      "Params:  tensor([  5.3624, -17.2745])\n",
      "Grad:  tensor([-0.0009,  0.0052])\n",
      "------\n",
      "------\n",
      "Epoch 3742, Loss 2.927727, \n",
      "Params:  tensor([  5.3624, -17.2745])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3743, Loss 2.927727, \n",
      "Params:  tensor([  5.3624, -17.2746])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3744, Loss 2.927726, \n",
      "Params:  tensor([  5.3624, -17.2746])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3745, Loss 2.927726, \n",
      "Params:  tensor([  5.3624, -17.2747])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3746, Loss 2.927725, \n",
      "Params:  tensor([  5.3624, -17.2747])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3747, Loss 2.927725, \n",
      "Params:  tensor([  5.3624, -17.2748])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3748, Loss 2.927725, \n",
      "Params:  tensor([  5.3624, -17.2748])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3749, Loss 2.927723, \n",
      "Params:  tensor([  5.3624, -17.2749])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3750, Loss 2.927724, \n",
      "Params:  tensor([  5.3624, -17.2749])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3751, Loss 2.927724, \n",
      "Params:  tensor([  5.3625, -17.2750])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3752, Loss 2.927725, \n",
      "Params:  tensor([  5.3625, -17.2750])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3753, Loss 2.927724, \n",
      "Params:  tensor([  5.3625, -17.2751])\n",
      "Grad:  tensor([-0.0009,  0.0051])\n",
      "------\n",
      "------\n",
      "Epoch 3754, Loss 2.927724, \n",
      "Params:  tensor([  5.3625, -17.2751])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3755, Loss 2.927723, \n",
      "Params:  tensor([  5.3625, -17.2752])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3756, Loss 2.927723, \n",
      "Params:  tensor([  5.3625, -17.2752])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3757, Loss 2.927723, \n",
      "Params:  tensor([  5.3625, -17.2753])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3758, Loss 2.927722, \n",
      "Params:  tensor([  5.3625, -17.2753])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3759, Loss 2.927723, \n",
      "Params:  tensor([  5.3625, -17.2754])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3760, Loss 2.927722, \n",
      "Params:  tensor([  5.3625, -17.2754])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3761, Loss 2.927723, \n",
      "Params:  tensor([  5.3625, -17.2755])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3762, Loss 2.927721, \n",
      "Params:  tensor([  5.3626, -17.2755])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3763, Loss 2.927722, \n",
      "Params:  tensor([  5.3626, -17.2756])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3764, Loss 2.927720, \n",
      "Params:  tensor([  5.3626, -17.2756])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3765, Loss 2.927720, \n",
      "Params:  tensor([  5.3626, -17.2757])\n",
      "Grad:  tensor([-0.0009,  0.0050])\n",
      "------\n",
      "------\n",
      "Epoch 3766, Loss 2.927719, \n",
      "Params:  tensor([  5.3626, -17.2757])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3767, Loss 2.927721, \n",
      "Params:  tensor([  5.3626, -17.2758])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3768, Loss 2.927719, \n",
      "Params:  tensor([  5.3626, -17.2758])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3769, Loss 2.927719, \n",
      "Params:  tensor([  5.3626, -17.2759])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3770, Loss 2.927719, \n",
      "Params:  tensor([  5.3626, -17.2759])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3771, Loss 2.927719, \n",
      "Params:  tensor([  5.3626, -17.2760])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3772, Loss 2.927719, \n",
      "Params:  tensor([  5.3626, -17.2760])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3773, Loss 2.927720, \n",
      "Params:  tensor([  5.3626, -17.2761])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3774, Loss 2.927718, \n",
      "Params:  tensor([  5.3627, -17.2761])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3775, Loss 2.927718, \n",
      "Params:  tensor([  5.3627, -17.2762])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3776, Loss 2.927717, \n",
      "Params:  tensor([  5.3627, -17.2762])\n",
      "Grad:  tensor([-0.0009,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3777, Loss 2.927718, \n",
      "Params:  tensor([  5.3627, -17.2763])\n",
      "Grad:  tensor([-0.0008,  0.0049])\n",
      "------\n",
      "------\n",
      "Epoch 3778, Loss 2.927717, \n",
      "Params:  tensor([  5.3627, -17.2763])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3779, Loss 2.927717, \n",
      "Params:  tensor([  5.3627, -17.2764])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3780, Loss 2.927716, \n",
      "Params:  tensor([  5.3627, -17.2764])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3781, Loss 2.927716, \n",
      "Params:  tensor([  5.3627, -17.2765])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3782, Loss 2.927717, \n",
      "Params:  tensor([  5.3627, -17.2765])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3783, Loss 2.927717, \n",
      "Params:  tensor([  5.3627, -17.2766])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3784, Loss 2.927716, \n",
      "Params:  tensor([  5.3627, -17.2766])\n",
      "Grad:  tensor([-0.0009,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3785, Loss 2.927715, \n",
      "Params:  tensor([  5.3627, -17.2767])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3786, Loss 2.927715, \n",
      "Params:  tensor([  5.3628, -17.2767])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3787, Loss 2.927715, \n",
      "Params:  tensor([  5.3628, -17.2767])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3788, Loss 2.927715, \n",
      "Params:  tensor([  5.3628, -17.2768])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3789, Loss 2.927715, \n",
      "Params:  tensor([  5.3628, -17.2768])\n",
      "Grad:  tensor([-0.0008,  0.0048])\n",
      "------\n",
      "------\n",
      "Epoch 3790, Loss 2.927715, \n",
      "Params:  tensor([  5.3628, -17.2769])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3791, Loss 2.927714, \n",
      "Params:  tensor([  5.3628, -17.2769])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3792, Loss 2.927714, \n",
      "Params:  tensor([  5.3628, -17.2770])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3793, Loss 2.927714, \n",
      "Params:  tensor([  5.3628, -17.2770])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3794, Loss 2.927714, \n",
      "Params:  tensor([  5.3628, -17.2771])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3795, Loss 2.927713, \n",
      "Params:  tensor([  5.3628, -17.2771])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3796, Loss 2.927714, \n",
      "Params:  tensor([  5.3628, -17.2772])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3797, Loss 2.927713, \n",
      "Params:  tensor([  5.3629, -17.2772])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3798, Loss 2.927712, \n",
      "Params:  tensor([  5.3629, -17.2773])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3799, Loss 2.927712, \n",
      "Params:  tensor([  5.3629, -17.2773])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3800, Loss 2.927713, \n",
      "Params:  tensor([  5.3629, -17.2774])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3801, Loss 2.927711, \n",
      "Params:  tensor([  5.3629, -17.2774])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3802, Loss 2.927712, \n",
      "Params:  tensor([  5.3629, -17.2775])\n",
      "Grad:  tensor([-0.0008,  0.0047])\n",
      "------\n",
      "------\n",
      "Epoch 3803, Loss 2.927712, \n",
      "Params:  tensor([  5.3629, -17.2775])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3804, Loss 2.927711, \n",
      "Params:  tensor([  5.3629, -17.2775])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3805, Loss 2.927712, \n",
      "Params:  tensor([  5.3629, -17.2776])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3806, Loss 2.927711, \n",
      "Params:  tensor([  5.3629, -17.2776])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3807, Loss 2.927711, \n",
      "Params:  tensor([  5.3629, -17.2777])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3808, Loss 2.927711, \n",
      "Params:  tensor([  5.3629, -17.2777])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3809, Loss 2.927709, \n",
      "Params:  tensor([  5.3629, -17.2778])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3810, Loss 2.927710, \n",
      "Params:  tensor([  5.3630, -17.2778])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3811, Loss 2.927710, \n",
      "Params:  tensor([  5.3630, -17.2779])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3812, Loss 2.927708, \n",
      "Params:  tensor([  5.3630, -17.2779])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3813, Loss 2.927708, \n",
      "Params:  tensor([  5.3630, -17.2780])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3814, Loss 2.927709, \n",
      "Params:  tensor([  5.3630, -17.2780])\n",
      "Grad:  tensor([-0.0008,  0.0046])\n",
      "------\n",
      "------\n",
      "Epoch 3815, Loss 2.927709, \n",
      "Params:  tensor([  5.3630, -17.2781])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3816, Loss 2.927710, \n",
      "Params:  tensor([  5.3630, -17.2781])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3817, Loss 2.927708, \n",
      "Params:  tensor([  5.3630, -17.2781])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3818, Loss 2.927708, \n",
      "Params:  tensor([  5.3630, -17.2782])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3819, Loss 2.927706, \n",
      "Params:  tensor([  5.3630, -17.2782])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3820, Loss 2.927707, \n",
      "Params:  tensor([  5.3630, -17.2783])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3821, Loss 2.927708, \n",
      "Params:  tensor([  5.3630, -17.2783])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3822, Loss 2.927707, \n",
      "Params:  tensor([  5.3631, -17.2784])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3823, Loss 2.927707, \n",
      "Params:  tensor([  5.3631, -17.2784])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3824, Loss 2.927707, \n",
      "Params:  tensor([  5.3631, -17.2785])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3825, Loss 2.927708, \n",
      "Params:  tensor([  5.3631, -17.2785])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3826, Loss 2.927707, \n",
      "Params:  tensor([  5.3631, -17.2786])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3827, Loss 2.927706, \n",
      "Params:  tensor([  5.3631, -17.2786])\n",
      "Grad:  tensor([-0.0008,  0.0045])\n",
      "------\n",
      "------\n",
      "Epoch 3828, Loss 2.927707, \n",
      "Params:  tensor([  5.3631, -17.2786])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3829, Loss 2.927705, \n",
      "Params:  tensor([  5.3631, -17.2787])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3830, Loss 2.927706, \n",
      "Params:  tensor([  5.3631, -17.2787])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3831, Loss 2.927706, \n",
      "Params:  tensor([  5.3631, -17.2788])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3832, Loss 2.927705, \n",
      "Params:  tensor([  5.3631, -17.2788])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3833, Loss 2.927705, \n",
      "Params:  tensor([  5.3631, -17.2789])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3834, Loss 2.927705, \n",
      "Params:  tensor([  5.3631, -17.2789])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3835, Loss 2.927705, \n",
      "Params:  tensor([  5.3632, -17.2789])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3836, Loss 2.927705, \n",
      "Params:  tensor([  5.3632, -17.2790])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3837, Loss 2.927705, \n",
      "Params:  tensor([  5.3632, -17.2790])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3838, Loss 2.927704, \n",
      "Params:  tensor([  5.3632, -17.2791])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3839, Loss 2.927704, \n",
      "Params:  tensor([  5.3632, -17.2791])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3840, Loss 2.927704, \n",
      "Params:  tensor([  5.3632, -17.2792])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3841, Loss 2.927703, \n",
      "Params:  tensor([  5.3632, -17.2792])\n",
      "Grad:  tensor([-0.0008,  0.0044])\n",
      "------\n",
      "------\n",
      "Epoch 3842, Loss 2.927702, \n",
      "Params:  tensor([  5.3632, -17.2793])\n",
      "Grad:  tensor([-0.0008,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3843, Loss 2.927703, \n",
      "Params:  tensor([  5.3632, -17.2793])\n",
      "Grad:  tensor([-0.0008,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3844, Loss 2.927703, \n",
      "Params:  tensor([  5.3632, -17.2793])\n",
      "Grad:  tensor([-0.0008,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3845, Loss 2.927704, \n",
      "Params:  tensor([  5.3632, -17.2794])\n",
      "Grad:  tensor([-0.0008,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3846, Loss 2.927702, \n",
      "Params:  tensor([  5.3632, -17.2794])\n",
      "Grad:  tensor([-0.0008,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3847, Loss 2.927701, \n",
      "Params:  tensor([  5.3632, -17.2795])\n",
      "Grad:  tensor([-0.0008,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3848, Loss 2.927703, \n",
      "Params:  tensor([  5.3633, -17.2795])\n",
      "Grad:  tensor([-0.0008,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3849, Loss 2.927702, \n",
      "Params:  tensor([  5.3633, -17.2796])\n",
      "Grad:  tensor([-0.0008,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3850, Loss 2.927701, \n",
      "Params:  tensor([  5.3633, -17.2796])\n",
      "Grad:  tensor([-0.0007,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3851, Loss 2.927701, \n",
      "Params:  tensor([  5.3633, -17.2796])\n",
      "Grad:  tensor([-0.0007,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3852, Loss 2.927703, \n",
      "Params:  tensor([  5.3633, -17.2797])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.0007,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3853, Loss 2.927700, \n",
      "Params:  tensor([  5.3633, -17.2797])\n",
      "Grad:  tensor([-0.0007,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3854, Loss 2.927701, \n",
      "Params:  tensor([  5.3633, -17.2798])\n",
      "Grad:  tensor([-0.0007,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3855, Loss 2.927701, \n",
      "Params:  tensor([  5.3633, -17.2798])\n",
      "Grad:  tensor([-0.0007,  0.0043])\n",
      "------\n",
      "------\n",
      "Epoch 3856, Loss 2.927700, \n",
      "Params:  tensor([  5.3633, -17.2799])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3857, Loss 2.927700, \n",
      "Params:  tensor([  5.3633, -17.2799])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3858, Loss 2.927700, \n",
      "Params:  tensor([  5.3633, -17.2799])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3859, Loss 2.927701, \n",
      "Params:  tensor([  5.3633, -17.2800])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3860, Loss 2.927699, \n",
      "Params:  tensor([  5.3633, -17.2800])\n",
      "Grad:  tensor([-0.0008,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3861, Loss 2.927699, \n",
      "Params:  tensor([  5.3634, -17.2801])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3862, Loss 2.927700, \n",
      "Params:  tensor([  5.3634, -17.2801])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3863, Loss 2.927699, \n",
      "Params:  tensor([  5.3634, -17.2801])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3864, Loss 2.927698, \n",
      "Params:  tensor([  5.3634, -17.2802])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3865, Loss 2.927699, \n",
      "Params:  tensor([  5.3634, -17.2802])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3866, Loss 2.927697, \n",
      "Params:  tensor([  5.3634, -17.2803])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3867, Loss 2.927700, \n",
      "Params:  tensor([  5.3634, -17.2803])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3868, Loss 2.927699, \n",
      "Params:  tensor([  5.3634, -17.2804])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3869, Loss 2.927698, \n",
      "Params:  tensor([  5.3634, -17.2804])\n",
      "Grad:  tensor([-0.0007,  0.0042])\n",
      "------\n",
      "------\n",
      "Epoch 3870, Loss 2.927697, \n",
      "Params:  tensor([  5.3634, -17.2804])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3871, Loss 2.927698, \n",
      "Params:  tensor([  5.3634, -17.2805])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3872, Loss 2.927696, \n",
      "Params:  tensor([  5.3634, -17.2805])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3873, Loss 2.927699, \n",
      "Params:  tensor([  5.3634, -17.2806])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3874, Loss 2.927698, \n",
      "Params:  tensor([  5.3634, -17.2806])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3875, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2806])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3876, Loss 2.927698, \n",
      "Params:  tensor([  5.3635, -17.2807])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3877, Loss 2.927697, \n",
      "Params:  tensor([  5.3635, -17.2807])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3878, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2808])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3879, Loss 2.927697, \n",
      "Params:  tensor([  5.3635, -17.2808])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3880, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2808])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3881, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2809])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3882, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2809])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3883, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2810])\n",
      "Grad:  tensor([-0.0007,  0.0041])\n",
      "------\n",
      "------\n",
      "Epoch 3884, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2810])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3885, Loss 2.927695, \n",
      "Params:  tensor([  5.3635, -17.2810])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3886, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2811])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3887, Loss 2.927696, \n",
      "Params:  tensor([  5.3635, -17.2811])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3888, Loss 2.927695, \n",
      "Params:  tensor([  5.3635, -17.2812])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3889, Loss 2.927695, \n",
      "Params:  tensor([  5.3636, -17.2812])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3890, Loss 2.927694, \n",
      "Params:  tensor([  5.3636, -17.2812])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3891, Loss 2.927693, \n",
      "Params:  tensor([  5.3636, -17.2813])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3892, Loss 2.927693, \n",
      "Params:  tensor([  5.3636, -17.2813])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3893, Loss 2.927695, \n",
      "Params:  tensor([  5.3636, -17.2814])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3894, Loss 2.927695, \n",
      "Params:  tensor([  5.3636, -17.2814])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3895, Loss 2.927694, \n",
      "Params:  tensor([  5.3636, -17.2815])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3896, Loss 2.927696, \n",
      "Params:  tensor([  5.3636, -17.2815])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3897, Loss 2.927693, \n",
      "Params:  tensor([  5.3636, -17.2815])\n",
      "Grad:  tensor([-0.0007,  0.0040])\n",
      "------\n",
      "------\n",
      "Epoch 3898, Loss 2.927693, \n",
      "Params:  tensor([  5.3636, -17.2816])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3899, Loss 2.927694, \n",
      "Params:  tensor([  5.3636, -17.2816])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3900, Loss 2.927693, \n",
      "Params:  tensor([  5.3636, -17.2817])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3901, Loss 2.927692, \n",
      "Params:  tensor([  5.3636, -17.2817])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3902, Loss 2.927694, \n",
      "Params:  tensor([  5.3636, -17.2817])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3903, Loss 2.927692, \n",
      "Params:  tensor([  5.3637, -17.2818])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3904, Loss 2.927693, \n",
      "Params:  tensor([  5.3637, -17.2818])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3905, Loss 2.927691, \n",
      "Params:  tensor([  5.3637, -17.2818])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3906, Loss 2.927692, \n",
      "Params:  tensor([  5.3637, -17.2819])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3907, Loss 2.927692, \n",
      "Params:  tensor([  5.3637, -17.2819])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3908, Loss 2.927692, \n",
      "Params:  tensor([  5.3637, -17.2820])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3909, Loss 2.927691, \n",
      "Params:  tensor([  5.3637, -17.2820])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3910, Loss 2.927692, \n",
      "Params:  tensor([  5.3637, -17.2820])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3911, Loss 2.927690, \n",
      "Params:  tensor([  5.3637, -17.2821])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3912, Loss 2.927691, \n",
      "Params:  tensor([  5.3637, -17.2821])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3913, Loss 2.927691, \n",
      "Params:  tensor([  5.3637, -17.2822])\n",
      "Grad:  tensor([-0.0007,  0.0039])\n",
      "------\n",
      "------\n",
      "Epoch 3914, Loss 2.927691, \n",
      "Params:  tensor([  5.3637, -17.2822])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3915, Loss 2.927690, \n",
      "Params:  tensor([  5.3637, -17.2822])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3916, Loss 2.927691, \n",
      "Params:  tensor([  5.3637, -17.2823])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3917, Loss 2.927691, \n",
      "Params:  tensor([  5.3637, -17.2823])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3918, Loss 2.927689, \n",
      "Params:  tensor([  5.3638, -17.2823])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3919, Loss 2.927690, \n",
      "Params:  tensor([  5.3638, -17.2824])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3920, Loss 2.927690, \n",
      "Params:  tensor([  5.3638, -17.2824])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3921, Loss 2.927690, \n",
      "Params:  tensor([  5.3638, -17.2825])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3922, Loss 2.927690, \n",
      "Params:  tensor([  5.3638, -17.2825])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3923, Loss 2.927689, \n",
      "Params:  tensor([  5.3638, -17.2825])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3924, Loss 2.927689, \n",
      "Params:  tensor([  5.3638, -17.2826])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3925, Loss 2.927689, \n",
      "Params:  tensor([  5.3638, -17.2826])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3926, Loss 2.927688, \n",
      "Params:  tensor([  5.3638, -17.2826])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3927, Loss 2.927689, \n",
      "Params:  tensor([  5.3638, -17.2827])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3928, Loss 2.927689, \n",
      "Params:  tensor([  5.3638, -17.2827])\n",
      "Grad:  tensor([-0.0007,  0.0038])\n",
      "------\n",
      "------\n",
      "Epoch 3929, Loss 2.927688, \n",
      "Params:  tensor([  5.3638, -17.2828])\n",
      "Grad:  tensor([-0.0007,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3930, Loss 2.927688, \n",
      "Params:  tensor([  5.3638, -17.2828])\n",
      "Grad:  tensor([-0.0007,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3931, Loss 2.927688, \n",
      "Params:  tensor([  5.3638, -17.2828])\n",
      "Grad:  tensor([-0.0007,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3932, Loss 2.927688, \n",
      "Params:  tensor([  5.3638, -17.2829])\n",
      "Grad:  tensor([-0.0007,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3933, Loss 2.927687, \n",
      "Params:  tensor([  5.3639, -17.2829])\n",
      "Grad:  tensor([-0.0007,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3934, Loss 2.927689, \n",
      "Params:  tensor([  5.3639, -17.2829])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3935, Loss 2.927688, \n",
      "Params:  tensor([  5.3639, -17.2830])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3936, Loss 2.927687, \n",
      "Params:  tensor([  5.3639, -17.2830])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3937, Loss 2.927687, \n",
      "Params:  tensor([  5.3639, -17.2831])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3938, Loss 2.927687, \n",
      "Params:  tensor([  5.3639, -17.2831])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3939, Loss 2.927686, \n",
      "Params:  tensor([  5.3639, -17.2831])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3940, Loss 2.927686, \n",
      "Params:  tensor([  5.3639, -17.2832])\n",
      "Grad:  tensor([-0.0007,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3941, Loss 2.927687, \n",
      "Params:  tensor([  5.3639, -17.2832])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3942, Loss 2.927686, \n",
      "Params:  tensor([  5.3639, -17.2832])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3943, Loss 2.927686, \n",
      "Params:  tensor([  5.3639, -17.2833])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3944, Loss 2.927686, \n",
      "Params:  tensor([  5.3639, -17.2833])\n",
      "Grad:  tensor([-0.0006,  0.0037])\n",
      "------\n",
      "------\n",
      "Epoch 3945, Loss 2.927686, \n",
      "Params:  tensor([  5.3639, -17.2833])\n",
      "Grad:  tensor([-0.0007,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3946, Loss 2.927685, \n",
      "Params:  tensor([  5.3639, -17.2834])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3947, Loss 2.927685, \n",
      "Params:  tensor([  5.3639, -17.2834])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3948, Loss 2.927686, \n",
      "Params:  tensor([  5.3640, -17.2835])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3949, Loss 2.927685, \n",
      "Params:  tensor([  5.3640, -17.2835])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3950, Loss 2.927686, \n",
      "Params:  tensor([  5.3640, -17.2835])\n",
      "Grad:  tensor([-0.0007,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3951, Loss 2.927686, \n",
      "Params:  tensor([  5.3640, -17.2836])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3952, Loss 2.927687, \n",
      "Params:  tensor([  5.3640, -17.2836])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3953, Loss 2.927685, \n",
      "Params:  tensor([  5.3640, -17.2836])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3954, Loss 2.927686, \n",
      "Params:  tensor([  5.3640, -17.2837])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3955, Loss 2.927686, \n",
      "Params:  tensor([  5.3640, -17.2837])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3956, Loss 2.927685, \n",
      "Params:  tensor([  5.3640, -17.2837])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3957, Loss 2.927683, \n",
      "Params:  tensor([  5.3640, -17.2838])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3958, Loss 2.927684, \n",
      "Params:  tensor([  5.3640, -17.2838])\n",
      "Grad:  tensor([-0.0007,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3959, Loss 2.927685, \n",
      "Params:  tensor([  5.3640, -17.2839])\n",
      "Grad:  tensor([-0.0006,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3960, Loss 2.927684, \n",
      "Params:  tensor([  5.3640, -17.2839])\n",
      "Grad:  tensor([-0.0007,  0.0036])\n",
      "------\n",
      "------\n",
      "Epoch 3961, Loss 2.927684, \n",
      "Params:  tensor([  5.3640, -17.2839])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3962, Loss 2.927684, \n",
      "Params:  tensor([  5.3640, -17.2840])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3963, Loss 2.927685, \n",
      "Params:  tensor([  5.3640, -17.2840])\n",
      "Grad:  tensor([-0.0007,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3964, Loss 2.927683, \n",
      "Params:  tensor([  5.3641, -17.2840])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3965, Loss 2.927685, \n",
      "Params:  tensor([  5.3641, -17.2841])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3966, Loss 2.927685, \n",
      "Params:  tensor([  5.3641, -17.2841])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3967, Loss 2.927684, \n",
      "Params:  tensor([  5.3641, -17.2841])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3968, Loss 2.927683, \n",
      "Params:  tensor([  5.3641, -17.2842])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3969, Loss 2.927683, \n",
      "Params:  tensor([  5.3641, -17.2842])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3970, Loss 2.927682, \n",
      "Params:  tensor([  5.3641, -17.2842])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3971, Loss 2.927683, \n",
      "Params:  tensor([  5.3641, -17.2843])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3972, Loss 2.927684, \n",
      "Params:  tensor([  5.3641, -17.2843])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3973, Loss 2.927682, \n",
      "Params:  tensor([  5.3641, -17.2843])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3974, Loss 2.927683, \n",
      "Params:  tensor([  5.3641, -17.2844])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3975, Loss 2.927683, \n",
      "Params:  tensor([  5.3641, -17.2844])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3976, Loss 2.927683, \n",
      "Params:  tensor([  5.3641, -17.2844])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3977, Loss 2.927682, \n",
      "Params:  tensor([  5.3641, -17.2845])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3978, Loss 2.927682, \n",
      "Params:  tensor([  5.3641, -17.2845])\n",
      "Grad:  tensor([-0.0006,  0.0035])\n",
      "------\n",
      "------\n",
      "Epoch 3979, Loss 2.927682, \n",
      "Params:  tensor([  5.3641, -17.2845])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3980, Loss 2.927681, \n",
      "Params:  tensor([  5.3642, -17.2846])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3981, Loss 2.927682, \n",
      "Params:  tensor([  5.3642, -17.2846])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3982, Loss 2.927682, \n",
      "Params:  tensor([  5.3642, -17.2847])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3983, Loss 2.927682, \n",
      "Params:  tensor([  5.3642, -17.2847])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3984, Loss 2.927682, \n",
      "Params:  tensor([  5.3642, -17.2847])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3985, Loss 2.927682, \n",
      "Params:  tensor([  5.3642, -17.2848])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3986, Loss 2.927682, \n",
      "Params:  tensor([  5.3642, -17.2848])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3987, Loss 2.927680, \n",
      "Params:  tensor([  5.3642, -17.2848])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3988, Loss 2.927682, \n",
      "Params:  tensor([  5.3642, -17.2849])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3989, Loss 2.927681, \n",
      "Params:  tensor([  5.3642, -17.2849])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3990, Loss 2.927681, \n",
      "Params:  tensor([  5.3642, -17.2849])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3991, Loss 2.927680, \n",
      "Params:  tensor([  5.3642, -17.2850])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3992, Loss 2.927681, \n",
      "Params:  tensor([  5.3642, -17.2850])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3993, Loss 2.927680, \n",
      "Params:  tensor([  5.3642, -17.2850])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3994, Loss 2.927680, \n",
      "Params:  tensor([  5.3642, -17.2851])\n",
      "Grad:  tensor([-0.0006,  0.0034])\n",
      "------\n",
      "------\n",
      "Epoch 3995, Loss 2.927681, \n",
      "Params:  tensor([  5.3642, -17.2851])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 3996, Loss 2.927681, \n",
      "Params:  tensor([  5.3642, -17.2851])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 3997, Loss 2.927679, \n",
      "Params:  tensor([  5.3643, -17.2852])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 3998, Loss 2.927680, \n",
      "Params:  tensor([  5.3643, -17.2852])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 3999, Loss 2.927679, \n",
      "Params:  tensor([  5.3643, -17.2852])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4000, Loss 2.927680, \n",
      "Params:  tensor([  5.3643, -17.2853])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4001, Loss 2.927680, \n",
      "Params:  tensor([  5.3643, -17.2853])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4002, Loss 2.927681, \n",
      "Params:  tensor([  5.3643, -17.2853])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4003, Loss 2.927679, \n",
      "Params:  tensor([  5.3643, -17.2854])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4004, Loss 2.927679, \n",
      "Params:  tensor([  5.3643, -17.2854])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4005, Loss 2.927680, \n",
      "Params:  tensor([  5.3643, -17.2854])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4006, Loss 2.927680, \n",
      "Params:  tensor([  5.3643, -17.2855])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4007, Loss 2.927677, \n",
      "Params:  tensor([  5.3643, -17.2855])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4008, Loss 2.927678, \n",
      "Params:  tensor([  5.3643, -17.2855])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4009, Loss 2.927679, \n",
      "Params:  tensor([  5.3643, -17.2856])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4010, Loss 2.927678, \n",
      "Params:  tensor([  5.3643, -17.2856])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4011, Loss 2.927679, \n",
      "Params:  tensor([  5.3643, -17.2856])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4012, Loss 2.927679, \n",
      "Params:  tensor([  5.3643, -17.2857])\n",
      "Grad:  tensor([-0.0006,  0.0033])\n",
      "------\n",
      "------\n",
      "Epoch 4013, Loss 2.927679, \n",
      "Params:  tensor([  5.3643, -17.2857])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4014, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2857])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4015, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2857])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4016, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2858])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4017, Loss 2.927679, \n",
      "Params:  tensor([  5.3644, -17.2858])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4018, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2858])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4019, Loss 2.927678, \n",
      "Params:  tensor([  5.3644, -17.2859])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4020, Loss 2.927678, \n",
      "Params:  tensor([  5.3644, -17.2859])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4021, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2859])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4022, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2860])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4023, Loss 2.927678, \n",
      "Params:  tensor([  5.3644, -17.2860])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4024, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2860])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4025, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2861])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4026, Loss 2.927676, \n",
      "Params:  tensor([  5.3644, -17.2861])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4027, Loss 2.927676, \n",
      "Params:  tensor([  5.3644, -17.2861])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4028, Loss 2.927675, \n",
      "Params:  tensor([  5.3644, -17.2862])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4029, Loss 2.927677, \n",
      "Params:  tensor([  5.3644, -17.2862])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4030, Loss 2.927674, \n",
      "Params:  tensor([  5.3644, -17.2862])\n",
      "Grad:  tensor([-0.0006,  0.0032])\n",
      "------\n",
      "------\n",
      "Epoch 4031, Loss 2.927676, \n",
      "Params:  tensor([  5.3644, -17.2863])\n",
      "Grad:  tensor([-0.0006,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4032, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2863])\n",
      "Grad:  tensor([-0.0006,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4033, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2863])\n",
      "Grad:  tensor([-0.0006,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4034, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2864])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4035, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2864])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4036, Loss 2.927674, \n",
      "Params:  tensor([  5.3645, -17.2864])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4037, Loss 2.927674, \n",
      "Params:  tensor([  5.3645, -17.2865])\n",
      "Grad:  tensor([-0.0006,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4038, Loss 2.927677, \n",
      "Params:  tensor([  5.3645, -17.2865])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4039, Loss 2.927674, \n",
      "Params:  tensor([  5.3645, -17.2865])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4040, Loss 2.927676, \n",
      "Params:  tensor([  5.3645, -17.2865])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4041, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2866])\n",
      "Grad:  tensor([-0.0006,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4042, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2866])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4043, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2866])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4044, Loss 2.927674, \n",
      "Params:  tensor([  5.3645, -17.2867])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4045, Loss 2.927674, \n",
      "Params:  tensor([  5.3645, -17.2867])\n",
      "Grad:  tensor([-0.0006,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4046, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2867])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4047, Loss 2.927674, \n",
      "Params:  tensor([  5.3645, -17.2868])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4048, Loss 2.927674, \n",
      "Params:  tensor([  5.3645, -17.2868])\n",
      "Grad:  tensor([-0.0006,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4049, Loss 2.927675, \n",
      "Params:  tensor([  5.3645, -17.2868])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4050, Loss 2.927672, \n",
      "Params:  tensor([  5.3646, -17.2868])\n",
      "Grad:  tensor([-0.0005,  0.0031])\n",
      "------\n",
      "------\n",
      "Epoch 4051, Loss 2.927675, \n",
      "Params:  tensor([  5.3646, -17.2869])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4052, Loss 2.927675, \n",
      "Params:  tensor([  5.3646, -17.2869])\n",
      "Grad:  tensor([-0.0006,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4053, Loss 2.927673, \n",
      "Params:  tensor([  5.3646, -17.2869])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4054, Loss 2.927673, \n",
      "Params:  tensor([  5.3646, -17.2870])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4055, Loss 2.927674, \n",
      "Params:  tensor([  5.3646, -17.2870])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4056, Loss 2.927673, \n",
      "Params:  tensor([  5.3646, -17.2870])\n",
      "Grad:  tensor([-0.0006,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4057, Loss 2.927674, \n",
      "Params:  tensor([  5.3646, -17.2871])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4058, Loss 2.927672, \n",
      "Params:  tensor([  5.3646, -17.2871])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4059, Loss 2.927674, \n",
      "Params:  tensor([  5.3646, -17.2871])\n",
      "Grad:  tensor([-0.0006,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4060, Loss 2.927675, \n",
      "Params:  tensor([  5.3646, -17.2872])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4061, Loss 2.927672, \n",
      "Params:  tensor([  5.3646, -17.2872])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4062, Loss 2.927673, \n",
      "Params:  tensor([  5.3646, -17.2872])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4063, Loss 2.927675, \n",
      "Params:  tensor([  5.3646, -17.2872])\n",
      "Grad:  tensor([-0.0006,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4064, Loss 2.927673, \n",
      "Params:  tensor([  5.3646, -17.2873])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4065, Loss 2.927674, \n",
      "Params:  tensor([  5.3646, -17.2873])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4066, Loss 2.927672, \n",
      "Params:  tensor([  5.3646, -17.2873])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4067, Loss 2.927673, \n",
      "Params:  tensor([  5.3646, -17.2874])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4068, Loss 2.927673, \n",
      "Params:  tensor([  5.3646, -17.2874])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4069, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2874])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4070, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2875])\n",
      "Grad:  tensor([-0.0005,  0.0030])\n",
      "------\n",
      "------\n",
      "Epoch 4071, Loss 2.927673, \n",
      "Params:  tensor([  5.3647, -17.2875])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4072, Loss 2.927673, \n",
      "Params:  tensor([  5.3647, -17.2875])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4073, Loss 2.927671, \n",
      "Params:  tensor([  5.3647, -17.2875])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4074, Loss 2.927673, \n",
      "Params:  tensor([  5.3647, -17.2876])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4075, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2876])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4076, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2876])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4077, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2877])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4078, Loss 2.927670, \n",
      "Params:  tensor([  5.3647, -17.2877])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4079, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2877])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4080, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2877])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4081, Loss 2.927671, \n",
      "Params:  tensor([  5.3647, -17.2878])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4082, Loss 2.927670, \n",
      "Params:  tensor([  5.3647, -17.2878])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4083, Loss 2.927673, \n",
      "Params:  tensor([  5.3647, -17.2878])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4084, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2879])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4085, Loss 2.927670, \n",
      "Params:  tensor([  5.3647, -17.2879])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4086, Loss 2.927670, \n",
      "Params:  tensor([  5.3647, -17.2879])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4087, Loss 2.927670, \n",
      "Params:  tensor([  5.3647, -17.2879])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4088, Loss 2.927672, \n",
      "Params:  tensor([  5.3647, -17.2880])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4089, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2880])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4090, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2880])\n",
      "Grad:  tensor([-0.0005,  0.0029])\n",
      "------\n",
      "------\n",
      "Epoch 4091, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2881])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4092, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2881])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4093, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2881])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4094, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2881])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4095, Loss 2.927669, \n",
      "Params:  tensor([  5.3648, -17.2882])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4096, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2882])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4097, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2882])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4098, Loss 2.927671, \n",
      "Params:  tensor([  5.3648, -17.2883])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4099, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2883])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4100, Loss 2.927671, \n",
      "Params:  tensor([  5.3648, -17.2883])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4101, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2883])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4102, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2884])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4103, Loss 2.927671, \n",
      "Params:  tensor([  5.3648, -17.2884])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4104, Loss 2.927669, \n",
      "Params:  tensor([  5.3648, -17.2884])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4105, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2885])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4106, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2885])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4107, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2885])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4108, Loss 2.927670, \n",
      "Params:  tensor([  5.3648, -17.2885])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4109, Loss 2.927668, \n",
      "Params:  tensor([  5.3649, -17.2886])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4110, Loss 2.927669, \n",
      "Params:  tensor([  5.3649, -17.2886])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4111, Loss 2.927669, \n",
      "Params:  tensor([  5.3649, -17.2886])\n",
      "Grad:  tensor([-0.0005,  0.0028])\n",
      "------\n",
      "------\n",
      "Epoch 4112, Loss 2.927670, \n",
      "Params:  tensor([  5.3649, -17.2886])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4113, Loss 2.927670, \n",
      "Params:  tensor([  5.3649, -17.2887])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4114, Loss 2.927670, \n",
      "Params:  tensor([  5.3649, -17.2887])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4115, Loss 2.927670, \n",
      "Params:  tensor([  5.3649, -17.2887])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4116, Loss 2.927669, \n",
      "Params:  tensor([  5.3649, -17.2887])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4117, Loss 2.927669, \n",
      "Params:  tensor([  5.3649, -17.2888])\n",
      "Grad:  tensor([-0.0004,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4118, Loss 2.927670, \n",
      "Params:  tensor([  5.3649, -17.2888])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4119, Loss 2.927669, \n",
      "Params:  tensor([  5.3649, -17.2888])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4120, Loss 2.927670, \n",
      "Params:  tensor([  5.3649, -17.2889])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4121, Loss 2.927668, \n",
      "Params:  tensor([  5.3649, -17.2889])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4122, Loss 2.927668, \n",
      "Params:  tensor([  5.3649, -17.2889])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4123, Loss 2.927669, \n",
      "Params:  tensor([  5.3649, -17.2889])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4124, Loss 2.927668, \n",
      "Params:  tensor([  5.3649, -17.2890])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4125, Loss 2.927670, \n",
      "Params:  tensor([  5.3649, -17.2890])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4126, Loss 2.927666, \n",
      "Params:  tensor([  5.3649, -17.2890])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4127, Loss 2.927669, \n",
      "Params:  tensor([  5.3649, -17.2890])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4128, Loss 2.927668, \n",
      "Params:  tensor([  5.3649, -17.2891])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4129, Loss 2.927669, \n",
      "Params:  tensor([  5.3649, -17.2891])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4130, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2891])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4131, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2892])\n",
      "Grad:  tensor([-0.0004,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4132, Loss 2.927668, \n",
      "Params:  tensor([  5.3650, -17.2892])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4133, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2892])\n",
      "Grad:  tensor([-0.0005,  0.0027])\n",
      "------\n",
      "------\n",
      "Epoch 4134, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2892])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4135, Loss 2.927666, \n",
      "Params:  tensor([  5.3650, -17.2893])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4136, Loss 2.927666, \n",
      "Params:  tensor([  5.3650, -17.2893])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4137, Loss 2.927669, \n",
      "Params:  tensor([  5.3650, -17.2893])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4138, Loss 2.927666, \n",
      "Params:  tensor([  5.3650, -17.2893])\n",
      "Grad:  tensor([-0.0004,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4139, Loss 2.927668, \n",
      "Params:  tensor([  5.3650, -17.2894])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4140, Loss 2.927666, \n",
      "Params:  tensor([  5.3650, -17.2894])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4141, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2894])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4142, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2894])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4143, Loss 2.927666, \n",
      "Params:  tensor([  5.3650, -17.2895])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4144, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2895])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4145, Loss 2.927666, \n",
      "Params:  tensor([  5.3650, -17.2895])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4146, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2896])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4147, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2896])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4148, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2896])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4149, Loss 2.927667, \n",
      "Params:  tensor([  5.3650, -17.2896])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4150, Loss 2.927665, \n",
      "Params:  tensor([  5.3650, -17.2897])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4151, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2897])\n",
      "Grad:  tensor([-0.0004,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4152, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2897])\n",
      "Grad:  tensor([-0.0004,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4153, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2897])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4154, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2898])\n",
      "Grad:  tensor([-0.0005,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4155, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2898])\n",
      "Grad:  tensor([-0.0004,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4156, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2898])\n",
      "Grad:  tensor([-0.0004,  0.0026])\n",
      "------\n",
      "------\n",
      "Epoch 4157, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2898])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4158, Loss 2.927665, \n",
      "Params:  tensor([  5.3651, -17.2899])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4159, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2899])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4160, Loss 2.927665, \n",
      "Params:  tensor([  5.3651, -17.2899])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4161, Loss 2.927664, \n",
      "Params:  tensor([  5.3651, -17.2899])\n",
      "Grad:  tensor([-0.0005,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4162, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2900])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4163, Loss 2.927665, \n",
      "Params:  tensor([  5.3651, -17.2900])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4164, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2900])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4165, Loss 2.927664, \n",
      "Params:  tensor([  5.3651, -17.2900])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4166, Loss 2.927665, \n",
      "Params:  tensor([  5.3651, -17.2901])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4167, Loss 2.927665, \n",
      "Params:  tensor([  5.3651, -17.2901])\n",
      "Grad:  tensor([-0.0005,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4168, Loss 2.927665, \n",
      "Params:  tensor([  5.3651, -17.2901])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4169, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2901])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4170, Loss 2.927664, \n",
      "Params:  tensor([  5.3651, -17.2902])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4171, Loss 2.927665, \n",
      "Params:  tensor([  5.3651, -17.2902])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4172, Loss 2.927666, \n",
      "Params:  tensor([  5.3651, -17.2902])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4173, Loss 2.927663, \n",
      "Params:  tensor([  5.3651, -17.2902])\n",
      "Grad:  tensor([-0.0005,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4174, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2903])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4175, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2903])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4176, Loss 2.927665, \n",
      "Params:  tensor([  5.3652, -17.2903])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4177, Loss 2.927663, \n",
      "Params:  tensor([  5.3652, -17.2903])\n",
      "Grad:  tensor([-0.0004,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4178, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2903])\n",
      "Grad:  tensor([-0.0005,  0.0025])\n",
      "------\n",
      "------\n",
      "Epoch 4179, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2904])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4180, Loss 2.927663, \n",
      "Params:  tensor([  5.3652, -17.2904])\n",
      "Grad:  tensor([-0.0005,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4181, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2904])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4182, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2904])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4183, Loss 2.927663, \n",
      "Params:  tensor([  5.3652, -17.2905])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4184, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2905])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4185, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2905])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4186, Loss 2.927662, \n",
      "Params:  tensor([  5.3652, -17.2905])\n",
      "Grad:  tensor([-0.0005,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4187, Loss 2.927665, \n",
      "Params:  tensor([  5.3652, -17.2906])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4188, Loss 2.927663, \n",
      "Params:  tensor([  5.3652, -17.2906])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4189, Loss 2.927662, \n",
      "Params:  tensor([  5.3652, -17.2906])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4190, Loss 2.927663, \n",
      "Params:  tensor([  5.3652, -17.2906])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4191, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2907])\n",
      "Grad:  tensor([-0.0005,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4192, Loss 2.927664, \n",
      "Params:  tensor([  5.3652, -17.2907])\n",
      "Grad:  tensor([-0.0005,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4193, Loss 2.927662, \n",
      "Params:  tensor([  5.3652, -17.2907])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4194, Loss 2.927663, \n",
      "Params:  tensor([  5.3652, -17.2907])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4195, Loss 2.927663, \n",
      "Params:  tensor([  5.3652, -17.2908])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4196, Loss 2.927665, \n",
      "Params:  tensor([  5.3652, -17.2908])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4197, Loss 2.927664, \n",
      "Params:  tensor([  5.3653, -17.2908])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4198, Loss 2.927663, \n",
      "Params:  tensor([  5.3653, -17.2908])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4199, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2909])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4200, Loss 2.927664, \n",
      "Params:  tensor([  5.3653, -17.2909])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4201, Loss 2.927663, \n",
      "Params:  tensor([  5.3653, -17.2909])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4202, Loss 2.927661, \n",
      "Params:  tensor([  5.3653, -17.2909])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4203, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2910])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4204, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2910])\n",
      "Grad:  tensor([-0.0004,  0.0024])\n",
      "------\n",
      "------\n",
      "Epoch 4205, Loss 2.927663, \n",
      "Params:  tensor([  5.3653, -17.2910])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4206, Loss 2.927663, \n",
      "Params:  tensor([  5.3653, -17.2910])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4207, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2910])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4208, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2911])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4209, Loss 2.927663, \n",
      "Params:  tensor([  5.3653, -17.2911])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4210, Loss 2.927664, \n",
      "Params:  tensor([  5.3653, -17.2911])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4211, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2911])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4212, Loss 2.927660, \n",
      "Params:  tensor([  5.3653, -17.2912])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4213, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2912])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4214, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2912])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4215, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2912])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4216, Loss 2.927661, \n",
      "Params:  tensor([  5.3653, -17.2913])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4217, Loss 2.927660, \n",
      "Params:  tensor([  5.3653, -17.2913])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4218, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2913])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4219, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2913])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4220, Loss 2.927663, \n",
      "Params:  tensor([  5.3653, -17.2913])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4221, Loss 2.927662, \n",
      "Params:  tensor([  5.3653, -17.2914])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4222, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2914])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4223, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2914])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4224, Loss 2.927663, \n",
      "Params:  tensor([  5.3654, -17.2914])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4225, Loss 2.927660, \n",
      "Params:  tensor([  5.3654, -17.2915])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4226, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2915])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4227, Loss 2.927660, \n",
      "Params:  tensor([  5.3654, -17.2915])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4228, Loss 2.927661, \n",
      "Params:  tensor([  5.3654, -17.2915])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4229, Loss 2.927661, \n",
      "Params:  tensor([  5.3654, -17.2915])\n",
      "Grad:  tensor([-0.0004,  0.0023])\n",
      "------\n",
      "------\n",
      "Epoch 4230, Loss 2.927660, \n",
      "Params:  tensor([  5.3654, -17.2916])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4231, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2916])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4232, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2916])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4233, Loss 2.927660, \n",
      "Params:  tensor([  5.3654, -17.2916])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4234, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2917])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4235, Loss 2.927661, \n",
      "Params:  tensor([  5.3654, -17.2917])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4236, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2917])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4237, Loss 2.927661, \n",
      "Params:  tensor([  5.3654, -17.2917])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4238, Loss 2.927660, \n",
      "Params:  tensor([  5.3654, -17.2918])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4239, Loss 2.927661, \n",
      "Params:  tensor([  5.3654, -17.2918])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4240, Loss 2.927660, \n",
      "Params:  tensor([  5.3654, -17.2918])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4241, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2918])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4242, Loss 2.927660, \n",
      "Params:  tensor([  5.3654, -17.2918])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4243, Loss 2.927659, \n",
      "Params:  tensor([  5.3654, -17.2919])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4244, Loss 2.927661, \n",
      "Params:  tensor([  5.3654, -17.2919])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4245, Loss 2.927661, \n",
      "Params:  tensor([  5.3654, -17.2919])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4246, Loss 2.927662, \n",
      "Params:  tensor([  5.3654, -17.2919])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4247, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2920])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4248, Loss 2.927659, \n",
      "Params:  tensor([  5.3655, -17.2920])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4249, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2920])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4250, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2920])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4251, Loss 2.927662, \n",
      "Params:  tensor([  5.3655, -17.2920])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4252, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2921])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4253, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2921])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4254, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2921])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4255, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2921])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4256, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2921])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4257, Loss 2.927661, \n",
      "Params:  tensor([  5.3655, -17.2922])\n",
      "Grad:  tensor([-0.0004,  0.0022])\n",
      "------\n",
      "------\n",
      "Epoch 4258, Loss 2.927659, \n",
      "Params:  tensor([  5.3655, -17.2922])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4259, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2922])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4260, Loss 2.927659, \n",
      "Params:  tensor([  5.3655, -17.2922])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4261, Loss 2.927659, \n",
      "Params:  tensor([  5.3655, -17.2922])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4262, Loss 2.927662, \n",
      "Params:  tensor([  5.3655, -17.2923])\n",
      "Grad:  tensor([-0.0003,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4263, Loss 2.927658, \n",
      "Params:  tensor([  5.3655, -17.2923])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4264, Loss 2.927659, \n",
      "Params:  tensor([  5.3655, -17.2923])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4265, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2923])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4266, Loss 2.927659, \n",
      "Params:  tensor([  5.3655, -17.2924])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4267, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2924])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4268, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2924])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4269, Loss 2.927659, \n",
      "Params:  tensor([  5.3655, -17.2924])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4270, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2924])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4271, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2925])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4272, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2925])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4273, Loss 2.927660, \n",
      "Params:  tensor([  5.3655, -17.2925])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4274, Loss 2.927658, \n",
      "Params:  tensor([  5.3656, -17.2925])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4275, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2925])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4276, Loss 2.927660, \n",
      "Params:  tensor([  5.3656, -17.2926])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4277, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2926])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4278, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2926])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4279, Loss 2.927658, \n",
      "Params:  tensor([  5.3656, -17.2926])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4280, Loss 2.927658, \n",
      "Params:  tensor([  5.3656, -17.2926])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4281, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2927])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4282, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2927])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4283, Loss 2.927660, \n",
      "Params:  tensor([  5.3656, -17.2927])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4284, Loss 2.927660, \n",
      "Params:  tensor([  5.3656, -17.2927])\n",
      "Grad:  tensor([-0.0004,  0.0021])\n",
      "------\n",
      "------\n",
      "Epoch 4285, Loss 2.927658, \n",
      "Params:  tensor([  5.3656, -17.2927])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4286, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2928])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4287, Loss 2.927658, \n",
      "Params:  tensor([  5.3656, -17.2928])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4288, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2928])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4289, Loss 2.927658, \n",
      "Params:  tensor([  5.3656, -17.2928])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4290, Loss 2.927657, \n",
      "Params:  tensor([  5.3656, -17.2929])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4291, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2929])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4292, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2929])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4293, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2929])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4294, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2929])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4295, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2930])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4296, Loss 2.927657, \n",
      "Params:  tensor([  5.3656, -17.2930])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4297, Loss 2.927657, \n",
      "Params:  tensor([  5.3656, -17.2930])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4298, Loss 2.927657, \n",
      "Params:  tensor([  5.3656, -17.2930])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4299, Loss 2.927658, \n",
      "Params:  tensor([  5.3656, -17.2930])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4300, Loss 2.927659, \n",
      "Params:  tensor([  5.3656, -17.2931])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4301, Loss 2.927660, \n",
      "Params:  tensor([  5.3657, -17.2931])\n",
      "Grad:  tensor([-0.0004,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4302, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2931])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4303, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2931])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4304, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2931])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4305, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2932])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4306, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2932])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4307, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2932])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4308, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2932])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4309, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2932])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4310, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2932])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4311, Loss 2.927660, \n",
      "Params:  tensor([  5.3657, -17.2933])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4312, Loss 2.927659, \n",
      "Params:  tensor([  5.3657, -17.2933])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4313, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2933])\n",
      "Grad:  tensor([-0.0003,  0.0020])\n",
      "------\n",
      "------\n",
      "Epoch 4314, Loss 2.927656, \n",
      "Params:  tensor([  5.3657, -17.2933])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4315, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2933])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4316, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2934])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4317, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2934])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4318, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2934])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4319, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2934])\n",
      "Grad:  tensor([-0.0004,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4320, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2934])\n",
      "Grad:  tensor([-0.0004,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4321, Loss 2.927656, \n",
      "Params:  tensor([  5.3657, -17.2935])\n",
      "Grad:  tensor([-0.0004,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4322, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2935])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4323, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2935])\n",
      "Grad:  tensor([-0.0004,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4324, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2935])\n",
      "Grad:  tensor([-0.0004,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4325, Loss 2.927658, \n",
      "Params:  tensor([  5.3657, -17.2935])\n",
      "Grad:  tensor([-0.0004,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4326, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2936])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4327, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2936])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4328, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2936])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4329, Loss 2.927656, \n",
      "Params:  tensor([  5.3657, -17.2936])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4330, Loss 2.927657, \n",
      "Params:  tensor([  5.3657, -17.2936])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4331, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2936])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4332, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2937])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4333, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2937])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4334, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2937])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4335, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2937])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4336, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2937])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4337, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2938])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4338, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2938])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4339, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2938])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4340, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2938])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4341, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2938])\n",
      "Grad:  tensor([-0.0003,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4342, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2939])\n",
      "Grad:  tensor([-0.0004,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4343, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2939])\n",
      "Grad:  tensor([-0.0004,  0.0019])\n",
      "------\n",
      "------\n",
      "Epoch 4344, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2939])\n",
      "Grad:  tensor([-0.0004,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4345, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2939])\n",
      "Grad:  tensor([-0.0004,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4346, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2939])\n",
      "Grad:  tensor([-0.0004,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4347, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2940])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4348, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2940])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4349, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2940])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4350, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2940])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4351, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2940])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4352, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2941])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4353, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2941])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4354, Loss 2.927655, \n",
      "Params:  tensor([  5.3658, -17.2941])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4355, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2941])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4356, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2941])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4357, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2941])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4358, Loss 2.927657, \n",
      "Params:  tensor([  5.3658, -17.2942])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4359, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2942])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4360, Loss 2.927656, \n",
      "Params:  tensor([  5.3658, -17.2942])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4361, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2942])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4362, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2942])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4363, Loss 2.927657, \n",
      "Params:  tensor([  5.3659, -17.2942])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4364, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2943])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4365, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2943])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4366, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2943])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4367, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2943])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4368, Loss 2.927654, \n",
      "Params:  tensor([  5.3659, -17.2943])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4369, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2943])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4370, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2944])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4371, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2944])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4372, Loss 2.927657, \n",
      "Params:  tensor([  5.3659, -17.2944])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4373, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2944])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4374, Loss 2.927657, \n",
      "Params:  tensor([  5.3659, -17.2944])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4375, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2945])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4376, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2945])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4377, Loss 2.927654, \n",
      "Params:  tensor([  5.3659, -17.2945])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4378, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2945])\n",
      "Grad:  tensor([-0.0003,  0.0018])\n",
      "------\n",
      "------\n",
      "Epoch 4379, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2945])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4380, Loss 2.927654, \n",
      "Params:  tensor([  5.3659, -17.2945])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4381, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2946])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4382, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2946])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4383, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2946])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4384, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2946])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4385, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2946])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4386, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2946])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4387, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2947])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4388, Loss 2.927653, \n",
      "Params:  tensor([  5.3659, -17.2947])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4389, Loss 2.927654, \n",
      "Params:  tensor([  5.3659, -17.2947])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4390, Loss 2.927654, \n",
      "Params:  tensor([  5.3659, -17.2947])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4391, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2947])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4392, Loss 2.927656, \n",
      "Params:  tensor([  5.3659, -17.2947])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4393, Loss 2.927655, \n",
      "Params:  tensor([  5.3659, -17.2948])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4394, Loss 2.927656, \n",
      "Params:  tensor([  5.3660, -17.2948])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4395, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2948])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4396, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2948])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4397, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2948])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4398, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2948])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4399, Loss 2.927656, \n",
      "Params:  tensor([  5.3660, -17.2949])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4400, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2949])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4401, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2949])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4402, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2949])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4403, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2949])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4404, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2949])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4405, Loss 2.927656, \n",
      "Params:  tensor([  5.3660, -17.2950])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4406, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2950])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4407, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2950])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4408, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2950])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4409, Loss 2.927653, \n",
      "Params:  tensor([  5.3660, -17.2950])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4410, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2951])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4411, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2951])\n",
      "Grad:  tensor([-0.0003,  0.0017])\n",
      "------\n",
      "------\n",
      "Epoch 4412, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2951])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4413, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2951])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4414, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2951])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4415, Loss 2.927653, \n",
      "Params:  tensor([  5.3660, -17.2951])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4416, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2952])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4417, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2952])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4418, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2952])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4419, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2952])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4420, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2952])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4421, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2952])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4422, Loss 2.927653, \n",
      "Params:  tensor([  5.3660, -17.2953])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4423, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2953])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4424, Loss 2.927653, \n",
      "Params:  tensor([  5.3660, -17.2953])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4425, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2953])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4426, Loss 2.927655, \n",
      "Params:  tensor([  5.3660, -17.2953])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4427, Loss 2.927654, \n",
      "Params:  tensor([  5.3660, -17.2953])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4428, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2953])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4429, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2954])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4430, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2954])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4431, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2954])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4432, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2954])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4433, Loss 2.927655, \n",
      "Params:  tensor([  5.3661, -17.2954])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4434, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2954])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4435, Loss 2.927655, \n",
      "Params:  tensor([  5.3661, -17.2955])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4436, Loss 2.927652, \n",
      "Params:  tensor([  5.3661, -17.2955])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4437, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2955])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4438, Loss 2.927654, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  tensor([  5.3661, -17.2955])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4439, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2955])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4440, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2955])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4441, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2955])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4442, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2956])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4443, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2956])\n",
      "Grad:  tensor([-0.0002,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4444, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2956])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4445, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2956])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4446, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2956])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4447, Loss 2.927652, \n",
      "Params:  tensor([  5.3661, -17.2956])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4448, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2957])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4449, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2957])\n",
      "Grad:  tensor([-0.0003,  0.0016])\n",
      "------\n",
      "------\n",
      "Epoch 4450, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2957])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4451, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2957])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4452, Loss 2.927651, \n",
      "Params:  tensor([  5.3661, -17.2957])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4453, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2957])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4454, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2957])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4455, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2958])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4456, Loss 2.927655, \n",
      "Params:  tensor([  5.3661, -17.2958])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4457, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2958])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4458, Loss 2.927652, \n",
      "Params:  tensor([  5.3661, -17.2958])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4459, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2958])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4460, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2958])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4461, Loss 2.927654, \n",
      "Params:  tensor([  5.3661, -17.2959])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4462, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2959])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4463, Loss 2.927652, \n",
      "Params:  tensor([  5.3661, -17.2959])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4464, Loss 2.927653, \n",
      "Params:  tensor([  5.3661, -17.2959])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4465, Loss 2.927654, \n",
      "Params:  tensor([  5.3662, -17.2959])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4466, Loss 2.927654, \n",
      "Params:  tensor([  5.3662, -17.2959])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4467, Loss 2.927654, \n",
      "Params:  tensor([  5.3662, -17.2959])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4468, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2960])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4469, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2960])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4470, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2960])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4471, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2960])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4472, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2960])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4473, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2960])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4474, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2960])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4475, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2961])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4476, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2961])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4477, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2961])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4478, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2961])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4479, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2961])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4480, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2961])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4481, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2962])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4482, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2962])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4483, Loss 2.927654, \n",
      "Params:  tensor([  5.3662, -17.2962])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4484, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2962])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4485, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2962])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4486, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2962])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4487, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2962])\n",
      "Grad:  tensor([-0.0003,  0.0015])\n",
      "------\n",
      "------\n",
      "Epoch 4488, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2963])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4489, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2963])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4490, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2963])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4491, Loss 2.927651, \n",
      "Params:  tensor([  5.3662, -17.2963])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4492, Loss 2.927651, \n",
      "Params:  tensor([  5.3662, -17.2963])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4493, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2963])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4494, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2964])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4495, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2964])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4496, Loss 2.927651, \n",
      "Params:  tensor([  5.3662, -17.2964])\n",
      "Grad:  tensor([-0.0003,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4497, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2964])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4498, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2964])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4499, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2964])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4500, Loss 2.927651, \n",
      "Params:  tensor([  5.3662, -17.2964])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4501, Loss 2.927652, \n",
      "Params:  tensor([  5.3662, -17.2964])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4502, Loss 2.927653, \n",
      "Params:  tensor([  5.3662, -17.2965])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4503, Loss 2.927653, \n",
      "Params:  tensor([  5.3663, -17.2965])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4504, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2965])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4505, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2965])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4506, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2965])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4507, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2965])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4508, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2965])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4509, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2966])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4510, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2966])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4511, Loss 2.927650, \n",
      "Params:  tensor([  5.3663, -17.2966])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4512, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2966])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4513, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2966])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4514, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2966])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4515, Loss 2.927650, \n",
      "Params:  tensor([  5.3663, -17.2966])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4516, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2966])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4517, Loss 2.927653, \n",
      "Params:  tensor([  5.3663, -17.2967])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4518, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2967])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4519, Loss 2.927650, \n",
      "Params:  tensor([  5.3663, -17.2967])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4520, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2967])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4521, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2967])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4522, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2967])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4523, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2967])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4524, Loss 2.927653, \n",
      "Params:  tensor([  5.3663, -17.2968])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4525, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2968])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4526, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2968])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4527, Loss 2.927653, \n",
      "Params:  tensor([  5.3663, -17.2968])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4528, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2968])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4529, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2968])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4530, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2968])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4531, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2969])\n",
      "Grad:  tensor([-0.0002,  0.0014])\n",
      "------\n",
      "------\n",
      "Epoch 4532, Loss 2.927650, \n",
      "Params:  tensor([  5.3663, -17.2969])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4533, Loss 2.927652, \n",
      "Params:  tensor([  5.3663, -17.2969])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4534, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2969])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4535, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2969])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4536, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2969])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4537, Loss 2.927653, \n",
      "Params:  tensor([  5.3663, -17.2969])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4538, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2969])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4539, Loss 2.927650, \n",
      "Params:  tensor([  5.3663, -17.2970])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4540, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2970])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4541, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2970])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4542, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2970])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4543, Loss 2.927651, \n",
      "Params:  tensor([  5.3663, -17.2970])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4544, Loss 2.927650, \n",
      "Params:  tensor([  5.3663, -17.2970])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4545, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2970])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4546, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2971])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4547, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2971])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4548, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2971])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4549, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2971])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4550, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2971])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4551, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2971])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4552, Loss 2.927653, \n",
      "Params:  tensor([  5.3664, -17.2971])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4553, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2971])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4554, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2972])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4555, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2972])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4556, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2972])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4557, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2972])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4558, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2972])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4559, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2972])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4560, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2972])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4561, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2973])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4562, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2973])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4563, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2973])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4564, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2973])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4565, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2973])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4566, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2973])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4567, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2973])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4568, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2973])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4569, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2974])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4570, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2974])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4571, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2974])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4572, Loss 2.927649, \n",
      "Params:  tensor([  5.3664, -17.2974])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4573, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2974])\n",
      "Grad:  tensor([-0.0002,  0.0013])\n",
      "------\n",
      "------\n",
      "Epoch 4574, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2974])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4575, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2974])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4576, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2975])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4577, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2975])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4578, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2975])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4579, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2975])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4580, Loss 2.927652, \n",
      "Params:  tensor([  5.3664, -17.2975])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4581, Loss 2.927649, \n",
      "Params:  tensor([  5.3664, -17.2975])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4582, Loss 2.927653, \n",
      "Params:  tensor([  5.3664, -17.2975])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4583, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2975])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4584, Loss 2.927649, \n",
      "Params:  tensor([  5.3664, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4585, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4586, Loss 2.927650, \n",
      "Params:  tensor([  5.3664, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4587, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4588, Loss 2.927651, \n",
      "Params:  tensor([  5.3664, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4589, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4590, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4591, Loss 2.927652, \n",
      "Params:  tensor([  5.3665, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4592, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2976])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4593, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4594, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4595, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4596, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4597, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4598, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4599, Loss 2.927652, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4600, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4601, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2977])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4602, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4603, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4604, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4605, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4606, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4607, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4608, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4609, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4610, Loss 2.927649, \n",
      "Params:  tensor([  5.3665, -17.2978])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4611, Loss 2.927649, \n",
      "Params:  tensor([  5.3665, -17.2979])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4612, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2979])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4613, Loss 2.927649, \n",
      "Params:  tensor([  5.3665, -17.2979])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4614, Loss 2.927649, \n",
      "Params:  tensor([  5.3665, -17.2979])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4615, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2979])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4616, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2979])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4617, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2979])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4618, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2979])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4619, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4620, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4621, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4622, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4623, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4624, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4625, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4626, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0012])\n",
      "------\n",
      "------\n",
      "Epoch 4627, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2980])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4628, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4629, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4630, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4631, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4632, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4633, Loss 2.927651, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4634, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4635, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4636, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2981])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4637, Loss 2.927649, \n",
      "Params:  tensor([  5.3665, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4638, Loss 2.927650, \n",
      "Params:  tensor([  5.3665, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4639, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4640, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4641, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4642, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4643, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4644, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4645, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2982])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4646, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2983])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4647, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2983])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4648, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2983])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4649, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2983])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4650, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2983])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4651, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2983])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4652, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2983])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4653, Loss 2.927651, \n",
      "Params:  tensor([  5.3666, -17.2983])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4654, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4655, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4656, Loss 2.927651, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4657, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4658, Loss 2.927651, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4659, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4660, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4661, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4662, Loss 2.927648, \n",
      "Params:  tensor([  5.3666, -17.2984])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4663, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4664, Loss 2.927648, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4665, Loss 2.927648, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4666, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4667, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4668, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4669, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4670, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4671, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2985])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4672, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4673, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4674, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4675, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0011])\n",
      "------\n",
      "------\n",
      "Epoch 4676, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4677, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4678, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4679, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4680, Loss 2.927648, \n",
      "Params:  tensor([  5.3666, -17.2986])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4681, Loss 2.927648, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4682, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4683, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4684, Loss 2.927648, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4685, Loss 2.927650, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4686, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4687, Loss 2.927651, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4688, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4689, Loss 2.927649, \n",
      "Params:  tensor([  5.3666, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4690, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4691, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2987])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4692, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4693, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4694, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4695, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4696, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4697, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4698, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4699, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4700, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4701, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2988])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4702, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4703, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4704, Loss 2.927647, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4705, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4706, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4707, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4708, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4709, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4710, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4711, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4712, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2989])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4713, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4714, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4715, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4716, Loss 2.927647, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4717, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4718, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4719, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4720, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4721, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4722, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2990])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4723, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4724, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4725, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4726, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4727, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4728, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4729, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4730, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4731, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4732, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4733, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2991])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4734, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4735, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4736, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4737, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4738, Loss 2.927650, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0010])\n",
      "------\n",
      "------\n",
      "Epoch 4739, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4740, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4741, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4742, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4743, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2992])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4744, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4745, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4746, Loss 2.927648, \n",
      "Params:  tensor([  5.3667, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4747, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4748, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4749, Loss 2.927649, \n",
      "Params:  tensor([  5.3667, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4750, Loss 2.927650, \n",
      "Params:  tensor([  5.3668, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4751, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4752, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4753, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4754, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2993])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4755, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4756, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4757, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4758, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4759, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4760, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4761, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4762, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4763, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4764, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2994])\n",
      "Grad:  tensor([-0.0001,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4765, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4766, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4767, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4768, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4769, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4770, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4771, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4772, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4773, Loss 2.927650, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4774, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4775, Loss 2.927650, \n",
      "Params:  tensor([  5.3668, -17.2995])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4776, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4777, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4778, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4779, Loss 2.927650, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4780, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4781, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4782, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4783, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4784, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4785, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2996])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4786, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4787, Loss 2.927650, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4788, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4789, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4790, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4791, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4792, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4793, Loss 2.927650, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4794, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0001,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4795, Loss 2.927650, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4796, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4797, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2997])\n",
      "Grad:  tensor([-0.0002,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4798, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4799, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4800, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4801, Loss 2.927646, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4802, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4803, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0009])\n",
      "------\n",
      "------\n",
      "Epoch 4804, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4805, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4806, Loss 2.927647, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4807, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4808, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4809, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4810, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2998])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4811, Loss 2.927648, \n",
      "Params:  tensor([  5.3668, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4812, Loss 2.927649, \n",
      "Params:  tensor([  5.3668, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4813, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4814, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4815, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4816, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4817, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4818, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4819, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4820, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4821, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4822, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4823, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.2999])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4824, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4825, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4826, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4827, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4828, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4829, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4830, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4831, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4832, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4833, Loss 2.927646, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4834, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4835, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4836, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3000])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4837, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4838, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4839, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4840, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4841, Loss 2.927650, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4842, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4843, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4844, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4845, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4846, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4847, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4848, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4849, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3001])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4850, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4851, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4852, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4853, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4854, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4855, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4856, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4857, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4858, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4859, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4860, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4861, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4862, Loss 2.927645, \n",
      "Params:  tensor([  5.3669, -17.3002])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4863, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4864, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4865, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4866, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4867, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4868, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4869, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4870, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4871, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4872, Loss 2.927646, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4873, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4874, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4875, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3003])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4876, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4877, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4878, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4879, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4880, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4881, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0008])\n",
      "------\n",
      "------\n",
      "Epoch 4882, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4883, Loss 2.927648, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4884, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4885, Loss 2.927647, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4886, Loss 2.927649, \n",
      "Params:  tensor([  5.3669, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4887, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4888, Loss 2.927649, \n",
      "Params:  tensor([  5.3670, -17.3004])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4889, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4890, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4891, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4892, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4893, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4894, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4895, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4896, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4897, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4898, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4899, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4900, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4901, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3005])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4902, Loss 2.927649, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4903, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4904, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4905, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4906, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4907, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4908, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4909, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4910, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4911, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4912, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4913, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4914, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4915, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3006])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4916, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4917, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4918, Loss 2.927649, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4919, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4920, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4921, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4922, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4923, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4924, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4925, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4926, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4927, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4928, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3007])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4929, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4930, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4931, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4932, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4933, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4934, Loss 2.927649, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4935, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4936, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4937, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4938, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4939, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4940, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4941, Loss 2.927649, \n",
      "Params:  tensor([  5.3670, -17.3008])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4942, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4943, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4944, Loss 2.927649, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4945, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4946, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4947, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4948, Loss 2.927649, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4949, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4950, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-9.9361e-05,  6.6355e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4951, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-9.5367e-05,  6.6292e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4952, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-9.6858e-05,  6.6188e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4953, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4954, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4955, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4956, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4957, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4958, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3009])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4959, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3010])\n",
      "Grad:  tensor([-9.8228e-05,  6.5479e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4960, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4961, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3010])\n",
      "Grad:  tensor([-9.8288e-05,  6.5270e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4962, Loss 2.927646, \n",
      "Params:  tensor([  5.3670, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0007])\n",
      "------\n",
      "------\n",
      "Epoch 4963, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4964, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4965, Loss 2.927647, \n",
      "Params:  tensor([  5.3670, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4966, Loss 2.927648, \n",
      "Params:  tensor([  5.3670, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4967, Loss 2.927646, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-9.7990e-05,  6.4683e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4968, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-9.7573e-05,  6.4588e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4969, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4970, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4971, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4972, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4973, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4974, Loss 2.927646, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4975, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3010])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4976, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4977, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-9.5606e-05,  6.3694e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4978, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-9.8169e-05,  6.3545e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4979, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4980, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4981, Loss 2.927646, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4982, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4983, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4984, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4985, Loss 2.927646, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4986, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-9.6440e-05,  6.2808e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4987, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4988, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4989, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4990, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4991, Loss 2.927646, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4992, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4993, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3011])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4994, Loss 2.927646, \n",
      "Params:  tensor([  5.3671, -17.3012])\n",
      "Grad:  tensor([-9.2626e-05,  6.2042e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4995, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3012])\n",
      "Grad:  tensor([-9.8884e-05,  6.1837e-04])\n",
      "------\n",
      "------\n",
      "Epoch 4996, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3012])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4997, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3012])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4998, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3012])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 4999, Loss 2.927647, \n",
      "Params:  tensor([  5.3671, -17.3012])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n",
      "------\n",
      "Epoch 5000, Loss 2.927648, \n",
      "Params:  tensor([  5.3671, -17.3012])\n",
      "Grad:  tensor([-0.0001,  0.0006])\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_un, \n",
    "    t_c = t_c,\n",
    "#     print_params = False\n",
    ")\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x2400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_p = model(t_un, *params)  # <1>\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Temperature (°Fahrenheit)\")\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy()) # <2>\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.savefig(\"temp_unknown_plot.png\", format=\"png\")  # bookskip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x2400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Measurement\")\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "\n",
    "plt.savefig(\"temp_data_plot.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "            \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "            \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f ' % (epoch, float(loss)))\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 2.927645 \n",
      "Epoch 1000, Loss 2.927645 \n",
      "Epoch 1500, Loss 2.927645 \n",
      "Epoch 2000, Loss 2.927645 \n",
      "Epoch 2500, Loss 2.927645 \n",
      "Epoch 3000, Loss 2.927645 \n",
      "Epoch 3500, Loss 2.927645 \n",
      "Epoch 4000, Loss 2.927645 \n",
      "Epoch 4500, Loss 2.927645 \n",
      "Epoch 5000, Loss 2.927645 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3676, -17.3042], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 2.927646 \n",
      "Epoch 1000, Loss 2.927645 \n",
      "Epoch 1500, Loss 2.927644 \n",
      "Epoch 2000, Loss 2.927644 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5368, -17.3048], requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 2000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "t_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val = int(0.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5, 10,  7,  0,  1,  3,  2,  8,  6]), tensor([9, 4]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u,\n",
    "                  val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "#             print(f'Val_t_p: {val_t_p}, Val_t_c: {val_t_c}')\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                  f\" Validation loss {val_loss.item():.4f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 89.6366, Validation loss 38.6392\n",
      "Epoch 2, Training loss 44.9269, Validation loss 2.5454\n",
      "Epoch 3, Training loss 37.1240, Validation loss 0.5562\n",
      "Epoch 500, Training loss 6.6685, Validation loss 5.3562\n",
      "Epoch 1000, Training loss 2.9650, Validation loss 6.1116\n",
      "Epoch 1500, Training loss 2.4965, Validation loss 6.4033\n",
      "Epoch 2000, Training loss 2.4372, Validation loss 6.5099\n",
      "Epoch 2500, Training loss 2.4297, Validation loss 6.5482\n",
      "Epoch 3000, Training loss 2.4288, Validation loss 6.5619\n",
      "Epoch 3500, Training loss 2.4286, Validation loss 6.5667\n",
      "Epoch 4000, Training loss 2.4286, Validation loss 6.5685\n",
      "Epoch 4500, Training loss 2.4286, Validation loss 6.5691\n",
      "Epoch 5000, Training loss 2.4286, Validation loss 6.5693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.4878, -17.4613], requires_grad=True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un,\n",
    "    val_t_u = val_t_un,\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
